{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projecte github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Draw the plots immediately after the current cell\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "# Uncomment the following line to install pymysql\n",
    "#!pip install PyMySQL\n",
    "import pymysql\n",
    "import warnings\n",
    "\n",
    "# POLARS # <- teacher said that this one is 1000% faster than pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project configuration\n",
    "\n",
    "The file that we are going to use to set up the basic structure:\n",
    "\n",
    "GitHubScraper/JupyterNotebook/Structure/githubProjectStructure.sql\n",
    "\n",
    "from the github repository.\n",
    "\n",
    "### Step 1, import the structure\n",
    "In workbench: \n",
    "\"Server\" -> \"Data Import\", and then you should select the direction of \"githubProjectStructure.sql\" using the 3 dots and import from self-contained file like in the following image:\n",
    "\n",
    "![Javatpoint](./Images/Project_Setup/data_import.png)  \n",
    "\n",
    "Then scroll down, and create a new Schema by clicking on the \"New...\" button. Name it: \"githubProject\".\n",
    "\n",
    "![Javatpoint](./Images/Project_Setup/create_new_schema.png)  \n",
    "\n",
    "Scroll to the bottom, and in the bottom right, click on \"Start import\". If there are no errors, it will transitionate to \"Import progress\" and indicate success.\n",
    "\n",
    "Refresh the scehmas:\n",
    "\n",
    "![Javatpoint](./Images/Project_Setup/refresh.png)  \n",
    "\n",
    "### Step 2, importing the data from the csv files\n",
    "\n",
    "Download the csv files [here](https://drive.google.com/drive/folders/1NWhfFss0_M9V_clkcE9TH-Fy3oIEndWV?usp=sharing).\n",
    "\n",
    "Selecting the \"githubProject\" scheme, right click and \"Table Data Import Wizard\", with this, we are going to import each csv file to the matching table.\n",
    "\n",
    "You have to select the .csv you want to import, for example in this case, the trendVisits.csv\n",
    "\n",
    "![Javatpoint](./Images/Project_Setup/file_import.png) \n",
    "\n",
    "Then, since you already have the structure, you have to use the existing table that matches with the correct csv file:\n",
    "\n",
    "![Javatpoint](./Images/Project_Setup/use_existing.png) \n",
    "\n",
    "After that, check that the variables type are correct and import it. Do this for each csv file.\n",
    "\n",
    "You can also verify the data by doing this:\n",
    "\n",
    "![Javatpoint](./Images/Project_Setup/verify.png) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "TODO index or sections indicating what are we doing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.geeksforgeeks.org/connect-to-mysql-using-pymysql-in-python/\n",
    "# Connect to MySQL database\n",
    "def tryToConnectMySQL(db_name, db_host, db_port, db_username, db_password):\n",
    "    try:\n",
    "        conn = pymysql.connect(host=db_host,\n",
    "                               port=db_port,\n",
    "                               user=db_username,\n",
    "                               password=db_password,\n",
    "                               db=db_name)\n",
    "        if conn:\n",
    "            print(\"Connection successful\")\n",
    "        else:\n",
    "            print(\"Error\")\n",
    "        \n",
    "        return conn\n",
    "        \n",
    "    except pymysql.Error as e:        \n",
    "        warnings.warn(\"Error connecting to MySQL:\", e)\n",
    "        return None\n",
    "\n",
    "# Pass arguments from outside\n",
    "db_name = \"githubProject\"\n",
    "db_host = \"localhost\"\n",
    "db_port = 3306\n",
    "db_username = \"root\"\n",
    "db_password = input(\"Input your password\")\n",
    "\n",
    "dataBaseConnection = tryToConnectMySQL(db_name, db_host, db_port, db_username, db_password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute query and return data in pandas dataframe \n",
    "def execute_select_query(cursor, query):\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        output = cursor.fetchall()\n",
    "        # Fetch column names from cursor's description\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "        \n",
    "        # Convert output to pandas DataFrame\n",
    "        if output:\n",
    "            df = pd.DataFrame(output, columns=columns)\n",
    "            print(\"Query executed successfully!\")\n",
    "        return output, df\n",
    "            \n",
    "    except pymysql.Error as e:\n",
    "        print(\"Error executing query:\")\n",
    "        warnings.warn(str(e))\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the cursos in order to interact with the DataBase\n",
    "cursor = dataBaseConnection.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Source Projects\n",
    "Repositories with criteria that indicates being not just a public repository, but a project open to contributions.\n",
    "\n",
    "Ordered by descending contributors count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT r.owner, r.name, r.mainLanguage, MAX(stars) as total_stars, MAX(contributors) as total_contributors, MAX(openPullRequests + closedPullRequests) as total_prs, MAX(openIssues + closedIssues) as total_issues, MAX(watchers) as total_watchers, MAX(stars) as total_stars FROM Repositories r\n",
    "JOIN RepositoryVisits v\n",
    "ON r.owner = v.owner AND r.name = v.name\n",
    "GROUP BY r.owner, r.name, r.mainLanguage\n",
    "HAVING total_contributors > 5 AND total_issues > 50\n",
    "ORDER BY total_contributors DESC;\n",
    "\"\"\"\n",
    "\n",
    "output_repo, df_repo = execute_select_query(cursor, query)\n",
    "print(len(df_repo), \"repositories matching criteria\")\n",
    "df_repo.head(100)\n",
    "# TODO require lates commit to be recent\n",
    "# TODO store license as well\n",
    "\n",
    "# TODO also search for info/resource repositories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data of these repositories is then extracted to a `.json` for the website to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renames or discards languages.\n",
    "LANGUAGE_REMAP = {\n",
    "    \"TypeScript\": \"JavaScript\",\n",
    "    \"C\": \"C/C++\",\n",
    "    \"C++\": \"C/C++\",\n",
    "    \"Rust\": \"Others\",\n",
    "    \"Ruby\": \"Others\",\n",
    "    \"Go\": \"Others\",\n",
    "    \"Swift\": \"Others\",\n",
    "    \"Clojure\": \"Others\",\n",
    "    \"Haskell\": \"Others\",\n",
    "    \"Vim Script\": \"\",\n",
    "    \"CSS\": \"\",\n",
    "    \"MDX\": \"\",\n",
    "    \"Shell\": \"\",\n",
    "}\n",
    "\n",
    "used_languages = set()\n",
    "used_tags = set()\n",
    "\n",
    "oss_repos = {}\n",
    "for index, row in df_repo.iterrows():\n",
    "    key = row[\"owner\"] + \"/\" + row[\"name\"]\n",
    "    main_language = row[\"mainLanguage\"]\n",
    "    if main_language in LANGUAGE_REMAP:\n",
    "        main_language = LANGUAGE_REMAP[main_language]\n",
    "    oss_repos[key] = {\n",
    "        \"topics\": set(),\n",
    "        \"languages\": set([main_language] if main_language != \"\" else []),\n",
    "    }\n",
    "    used_languages.add(main_language)\n",
    "\n",
    "# Remaps GitHub topics to the tags that are used in the website.\n",
    "TAG_MAP = {}\n",
    "TAG_ALIASES = {\n",
    "    \"Web\": [\"react\", \"vue\", \"web\", \"reactjs\", \"css\", \"chrome-extension\", \"react-grid\", \"react-table\", \"php\", \"http\"],\n",
    "    \"Modding\": [\"mod\", \"minecraft\", \"emulation\", \"emulator\", \"forge\", \"minecraft-launcher\", \"modrinth\", \"minecraft-api\", \"minecraft-server\", \"bepinex\", \"unity3d\", \"unreal\", \"unity-mono\", \"craftbukkit\", \"valheim\", \"minecraft-mod\", \"gta5\", \"fabric\"],\n",
    "    \"Data Science\": [\"math\", \"numpy\", \"data-science\", \"graphql\", \"data-visualization\", \"jupyter-notebook\"],\n",
    "    \"Machine Learning\": [\"ml\", \"pytorch\", \"deep-learning\", \"machine-learning\", \"deep-neural-networks\", \"tensorflow\", \"neural-network\", \"tensor\", \"computer-vision\", \"reinforcement-learning\", \"hyperparameter-tuning\", \"ai\", \"artificial-intelligence\", \"llama\", \"llms\", \"llm\"],\n",
    "    \"Tool\": [\"containers\", \"zsh\", \"docker\", \"github\", \"cli\", \"searchengine\", \"postgrest\", \"devtool\", \"cloudstorage\", \"git\", \"npm\", \"database\", \"postgresql\", \"backend\", \"shell-scripting\", \"websocket\", \"collaboration\"],\n",
    "    \"App\": [\"note-taking\", \"productivity\", \"prest\", \"download\", \"latex\", \"text-editor\", \"curl\", \"ftp\", \"bot\", \"synchronization\", \"sqlite\", \"mattermost\", \"messaging\", \"conferencing\", \"remote-desktop\"],\n",
    "\n",
    "    \"Resource\": [\"learn-to-code\", \"freecodecamp\", \"curriculum\", \"certification\", \"learnopengl\", \"lists\", \"resources\", \"resource\", \"dataset\", \"public-api\", \"public-apis\"],\n",
    "}\n",
    "# Tags manually added to some repositories (which otherwise lack descriptive ones)\n",
    "MANUAL_TAGS = {\n",
    "    \"minio/minio\": [\"Machine Learning\"],\n",
    "    \"Aliucord/Aliucord\": [\"Modding\"],\n",
    "    \"cli-guidelines/cli-guidelines\": [\"Resource\"],\n",
    "    \"yjs/yjs\": [\"Tool\"],\n",
    "    \"TigerVNC/tigervnc\": [\"Tool\"],\n",
    "    \"ollama/ollama\": [\"App\"],\n",
    "}\n",
    "for tag,aliases in TAG_ALIASES.items():\n",
    "    for alias in aliases:\n",
    "        TAG_MAP[alias] = tag\n",
    "\n",
    "from collections import defaultdict\n",
    "IGNORED_TAGS = defaultdict(int)\n",
    "import json\n",
    "with open(\"../persistence.json\", \"r\") as f:\n",
    "    repos_data = json.load(f)[\"repositories\"]\n",
    "\n",
    "    for key,repo in oss_repos.items():\n",
    "        repo[\"description\"] = repos_data[key][\"description\"]\n",
    "        for tag in repos_data[key][\"tags\"]:\n",
    "            if tag in TAG_MAP:\n",
    "                repo[\"topics\"].add(TAG_MAP[tag])\n",
    "            else:\n",
    "                IGNORED_TAGS[tag] += 1\n",
    "        if key in MANUAL_TAGS:\n",
    "            for tag in MANUAL_TAGS[key]:\n",
    "                repo[\"topics\"].add(tag)\n",
    "\n",
    "        for tag in repo[\"topics\"]:\n",
    "            used_tags.add(tag)\n",
    "\n",
    "# Exclude mirrors and other projects that are not contributable projects or unsuitable\n",
    "del oss_repos[\"gitlabhq/gitlabhq\"]\n",
    "del oss_repos[\"qemu/qemu\"]\n",
    "del oss_repos[\"xasset/xasset\"] # Not english.\n",
    "del oss_repos[\"jynew/jynew\"] # Unity RPG game framework, documentation in chinese-only though.\n",
    "\n",
    "for key,repo in oss_repos.items():\n",
    "    if len(repo[\"topics\"]) == 0:\n",
    "        print(\"Repo without tags\", f\"https://github.com/{key}\")\n",
    "\n",
    "# List unused Github topics\n",
    "# top_ignored_tags = [(tag, count) for tag, count in IGNORED_TAGS.items()]\n",
    "# top_ignored_tags = sorted(top_ignored_tags, key=lambda x: x[1], reverse=True)\n",
    "# print(len(top_ignored_tags))\n",
    "# print(top_ignored_tags)\n",
    "\n",
    "# Convert sets to lists for json serialization, and add other\n",
    "# keys the site expects\n",
    "for key,repo in oss_repos.items():\n",
    "    repo[\"owner\"] = key.split(\"/\")[0]\n",
    "    repo[\"repo\"] = key.split(\"/\")[1]\n",
    "    repo[\"topics\"] = list(repo[\"topics\"])\n",
    "    repo[\"languages\"] = list(repo[\"languages\"])\n",
    "\n",
    "with open(\"repositories.json\", \"w\") as f:\n",
    "    json.dump(oss_repos, f, indent=2)\n",
    "\n",
    "print(\"Valid repositories:\", len(oss_repos))\n",
    "print(\"Languages used:\", used_languages)\n",
    "print(\"Tags used:\", used_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tables\n",
    "Shows the table for the different entities of the Data Base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM Repositories;\"\n",
    "\n",
    "output_repo, df_repo = execute_select_query(cursor, query)\n",
    "df_repo.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM RepositoryVisits;\"\n",
    "\n",
    "output_repo_visists, df_repo_visists = execute_select_query(cursor, query)\n",
    "df_repo_visists.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM RepositoryTopics;\"\n",
    "\n",
    "output_repo_topics, df_repo_topics = execute_select_query(cursor, query)\n",
    "df_repo_topics.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Owners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM Owners;\"\n",
    "# Use _ to ignore\n",
    "_, df_owners = execute_select_query(cursor, query)\n",
    "df_owners.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM OwnerVisits;\"\n",
    "\n",
    "output_owner_visits, df_owner_visits = execute_select_query(cursor, query)\n",
    "df_owner_visits.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM Commits;\"\n",
    "\n",
    "output_commits, df_commits = execute_select_query(cursor, query)\n",
    "df_commits.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM Topics;\"\n",
    "\n",
    "output_topics, df_topics = execute_select_query(cursor, query)\n",
    "df_topics.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM TopicVisits;\"\n",
    "\n",
    "_, df_topic_visits = execute_select_query(cursor, query)\n",
    "df_topic_visits.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM TrendVisits;\"\n",
    "\n",
    "_, df_trend_visits = execute_select_query(cursor, query)\n",
    "df_trend_visits.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closes the connection\n",
    "dataBaseConnection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "### Basic statistics\n",
    "* Distribution of main topics (bar or pie chart). Aka, in what proportion of our studied repos are they treated.\n",
    "* Distribution of languages (bar or pie chart). Like above.\n",
    "* Time evolution of interest in topics: repositories per topic, followers per topic.\n",
    "\n",
    "### Dimensional reduction for an overview on repositories\n",
    "What we mean here is to perform a PCA (principal components analysis) in order to able to have an insight on the structre of the whole dataset. We would then color the data points in it depending on the language used, for instance, to see if these groups have similar characteristics and lie close in the dataframe or not.\n",
    "* Create a dataframe containing all RepositoryVisits data for a certain date for all the studied repositories.\n",
    "* Perform a standarization on the data (i.e., to prevent some variables such as commits to be far more important than others such as forks).\n",
    "* Perform a PCA into 2 components on it.\n",
    "* Plot the results while clustering the points depending on different criteria:\n",
    "    + mainLanguage\n",
    "    + topic\n",
    "* Supervised machine learning using stars.\n",
    "### Open questions\n",
    "* How to use stars and trends?\n",
    "* How to use contributions by owners?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commits analisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import path\n",
    "from PIL import Image\n",
    "\"\"\"\n",
    "If \n",
    "!pip install wordcloud \n",
    "doesn't work:\n",
    "import sys\n",
    "print(sys.executable) # use the path\n",
    "\n",
    "path -m pip install wordcloud\n",
    "\"\"\"\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "# https://www.datacamp.com/tutorial/wordcloud-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_cloud(word_cloud, text, save_image = False, image_name = \"none\"):\n",
    "    # Create and generate a word cloud image:\n",
    "    word_cloud_output = word_cloud.generate(text)\n",
    "    \n",
    "    # Display the generated image:\n",
    "    plt.imshow(word_cloud_output, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    if save_image:\n",
    "        word_cloud_output.to_file(f\"./Images/wordclouds/{image_name}_word_cloud.png\")\n",
    "        \n",
    "def transform_format(val):\n",
    "    if val == 0:\n",
    "        return 255\n",
    "    else:\n",
    "        return val\n",
    "        \n",
    "def obtain_mask(mask):    \n",
    "    trans_mask = np.ndarray((mask.shape[0],mask.shape[1]), np.int32)\n",
    "    for i in range(len(mask)):\n",
    "        trans_mask[i] = list(map(transform_format, mask[i]))\n",
    "    return trans_mask        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "# stopwords.update([\"a\"]) # manually add stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove commits made by bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the commits authored by a bot.\n",
    "from IPython.display import display\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "cond_a = df_commits[df_commits[\"author\"].str.contains(\"\\[bot]|-bot\")]\n",
    "cond_c = df_commits[df_commits[\"message\"].str.contains(\"dependabot\")]\n",
    "cond_d = df_commits[df_commits[\"message\"].str.contains(\"renovatebot\")]\n",
    "\n",
    "temp = pd.concat([cond_a])\n",
    "\n",
    "unique_auth = pd.unique(temp.author)\n",
    "unique_msg = pd.unique(cond_c.message)\n",
    "\n",
    "dict = {'bots' : unique_auth}\n",
    "df_bots = pd.DataFrame(dict)\n",
    "\n",
    "# displaying the bots names\n",
    "display(df_bots)\n",
    "\n",
    "dict = {'bots_message' : unique_msg}\n",
    "df_bots_msg = pd.DataFrame(dict)\n",
    "\n",
    "display(df_bots_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the bots\n",
    "print(\"With bots\", len(df_commits))\n",
    "df_commits = df_commits[~df_commits[\"author\"].isin(df_bots[\"bots\"])]\n",
    "print(\"Without bots\", len(df_commits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_commits_small = df_commits.tail() # only last 5\n",
    "print(df_commits_small.tail().message, \"\\n\")\n",
    "small_text_sample = \" \".join(commit for commit in df_commits_small.message) # concatenate them\n",
    "print(\"Concatenated text:\", text)\n",
    "image_name = \"first_word_cloud\"\n",
    "word_cloud = WordCloud(stopwords=stopwords, max_font_size=50, max_words=100, background_color=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_hyper_small = df_commits[\"message\"][0]\n",
    "\n",
    "image_name = \"first_word_cloud\"\n",
    "word_cloud = WordCloud(stopwords=stopwords, max_font_size=50, max_words=100, background_color=\"black\")\n",
    "\n",
    "plot_word_cloud(word_cloud, text_hyper_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = \" \".join(commit for commit in df_commits.message) # concatenate them\n",
    "word_cloud = WordCloud(stopwords=stopwords, max_font_size=30, max_words=200, background_color=\"black\")\n",
    "plot_word_cloud(word_cloud, all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a mask and a color map\n",
    "        \n",
    "# <a href='https://dryicons.com/icon/square-github-icon-8312'> Icon by Dryicons </a>\n",
    "github_image = np.array(Image.open(\"./Images/wordclouds/github_square.png\"))\n",
    "\n",
    "word_cloud_mask = obtain_mask(github_image)\n",
    "# Word cloud repeats words since he needs fill the gaps using the correct size.\n",
    "# print(len(all_text.split(' ')))\n",
    "# print(len(pd.unique(all_text.split(' '))))\n",
    "\n",
    "word_cloud = WordCloud(stopwords=stopwords, colormap='rainbow', mask=word_cloud_mask, max_font_size=30, max_words=10000, background_color=\"#31003C\")\n",
    "plot_word_cloud(word_cloud, all_text, True, \"git_mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a mask        \n",
    "git_image = np.array(Image.open(\"./Images/wordclouds/git.png\"))\n",
    "\n",
    "word_cloud_mask = obtain_mask(git_image)\n",
    "\n",
    "word_cloud = WordCloud(stopwords=stopwords, mask=word_cloud_mask,colormap='hot', max_font_size=20, max_words=10000, background_color=\"#474747\")\n",
    "plot_word_cloud(word_cloud, all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here goes some grafics bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Could explore the wordcloud of specific projects too:\n",
    "#TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-python3-kernel",
   "language": "python",
   "name": "my-python3-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
