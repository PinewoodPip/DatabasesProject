{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projecte de Bases de Dades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquest notebook cont√© els queries que hem usat per obtenir les dades i l'an√†lisis d'aquestes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install dependencies if necessary\n",
    "#!pip install PyMySQL\n",
    "#!pip install scikit-learn\n",
    "#!pip install nltk\n",
    "#!pip install wordcloud\n",
    "#!pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Draw the plots immediately after the current cell\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "plt.style.use('default')\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creaci√≥ de la base de dades\n",
    "\n",
    "El backup de la base de dades final es pot trobar en `Database/github.sql`. Aquest cont√© el schema i les dades; per crear la base de dades nom√©s cal executar tot el fitxer (amb el bot√≥ ‚ö° en MySQL Workbench). La base de dades s'anomena `github`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connexi√≥\n",
    "\n",
    "Per comen√ßar a treballar amb la base de dades, cal connectar-se primer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = \"github\"\n",
    "db_host = \"localhost\"\n",
    "db_port = 3306\n",
    "db_username = \"root\"\n",
    "db_password = input(\"Enter the DB password\")\n",
    "\n",
    "dataBaseConnection = pymysql.connect(host=db_host,\n",
    "                            port=db_port,\n",
    "                            user=db_username,\n",
    "                            password=db_password,\n",
    "                            db=db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cursor to interact with the database\n",
    "cursor = dataBaseConnection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_select_query(cursor, query):\n",
    "    \"\"\"\n",
    "    Executes a query and returns the results as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    cursor.execute(query)\n",
    "    output = cursor.fetchall()\n",
    "\n",
    "    # Fetch column names from cursor's description\n",
    "    columns = [desc[0] for desc in cursor.description]\n",
    "    \n",
    "    # Convert output to pandas DataFrame\n",
    "    if output:\n",
    "        df = pd.DataFrame(output, columns=columns)\n",
    "    return output, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taules\n",
    "A continuaci√≥ es mostren exemples de les entitats dins la base de dades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repositories\n",
    "query = \"SELECT * FROM Repositories WHERE mainLanguage != \\\"\\\"\"\n",
    "output_repo, df_repo = execute_select_query(cursor, query)\n",
    "df_repo.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repository visits\n",
    "query = \"SELECT * FROM RepositoryVisits;\"\n",
    "\n",
    "output_repo_visists, df_repo_visists = execute_select_query(cursor, query)\n",
    "df_repo_visists.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topics/tags of repositories\n",
    "query = \"\"\"\n",
    "SELECT r.name, r.owner, GROUP_CONCAT(t.topic) as topics FROM Repositories r\n",
    "JOIN RepositoryTopics t\n",
    "ON t.repo = r.name AND t.owner = r.owner\n",
    "GROUP BY r.name, r.owner\n",
    "\"\"\"\n",
    "\n",
    "output_repo_topics, df_repo_topics = execute_select_query(cursor, query)\n",
    "df_repo_topics.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commit messages\n",
    "query = \"SELECT * FROM Commits;\"\n",
    "output_commits, df_commits = execute_select_query(cursor, query)\n",
    "df_commits.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An√†lisi quantitatiu\n",
    "A partir de les dades tamb√© ens interessa realitzar uns an√†lisi quantitatiu:\n",
    "* Distribuci√≥ dels temes principals (gr√†fic de barres o de past√≠s). √âs a dir, en quina proporci√≥ dels nostres repositoris estudiats es tracten.\n",
    "* Distribuci√≥ de llenguatges (gr√†fic de barres o de past√≠s). Com l'anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RepositoryTopics is the N-N relationship table for Repository-Topic;\n",
    "# ie. the topics of each repository.\n",
    "output_repo_topics, df_repo_topics = execute_select_query(cursor, \"SELECT * FROM RepositoryTopics;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20 # We'll check the top 20 topics used\n",
    "topics = df_repo_topics[\"topic\"].value_counts().to_frame()\n",
    "topics_count = topics.sum()\n",
    "n_topics_count = topics[:n].sum()\n",
    "n_topics_name = df_repo_topics[\"topic\"].value_counts()[:n].index.to_list()\n",
    "print(\"Most used topics (present in {:.2f}% of repositories):\".format(float((n_topics_count / topics_count).iloc[0]) * 100))\n",
    "display(topics[:n])\n",
    "ax = topics[:n].plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el gr√†fic de barres anterior es mostra els primers 20 temes m√©s freq√ºents tractats en els nostres repositoris. No obstant aix√≤, nom√©s estan presents en el 15% d'ells, la qual cosa ens mostra que la nostra mostra est√† √†mpliament distribu√Øda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analitzem a continuaci√≥ els llenguatges m√©s usats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df_repo.drop(df_repo[df_repo[\"mainLanguage\"] == \"\"].index) # Ignore repositories where the main language is unknown\n",
    "languages = tmp[\"mainLanguage\"].value_counts().to_frame()\n",
    "display(languages)\n",
    "languages_count = languages.sum()\n",
    "n_languages_count = languages[:n].sum()\n",
    "print(\"Main topics present in {:.2f}% of repositories.\".format(float((n_languages_count / languages_count).iloc[0]) * 100))\n",
    "print(languages.shape)\n",
    "display(languages)\n",
    "languages[:n].plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Els llenguatges m√©s utilitzats en la nostra mostra de repositoris s√≥n llenguatges orientats al desenvolupament web; aquest resultat √©s coherent amb el fet que topics/tags per a frameworks de desenvolupament web com React s√≥n els m√©s usats. Seguits de Python, Go, Java i C++. Aquests llenguatges m√©s utilitzats representen m√©s del 90% dels repositoris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO a similar chart for TopicVisits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducci√≥ dimensional per a una visi√≥ general dels repositoris\n",
    "\n",
    "Una pregunta que ens interessava era si el llenguatge dels repositoris t√© correlaci√≥ amb les seves m√®triques (nombre de commits, stars, followers).\n",
    "\n",
    "Per contestar aquesta pregunta podem realitzar un Principal Component Analysis (PCA) per tenir una idea de l'estructura de tot el conjunt de dades. Despr√©s, colorar√≠em els punts de dades en funci√≥ del llenguatge utilitzat, per exemple, per veure si aquests grups tenen caracter√≠stiques similars i es troben propers en el dataframe o no.\n",
    "\n",
    "* Crear un dataframe que cont√© totes les dades de RepositoryVisits amb les dades m√©s actualitzades per a tots els repositoris estudiats.\n",
    "* Realitzar una estandarditzaci√≥ de les dades (√©s a dir, per evitar que algunes variables com els commits siguin molt m√©s importants que altres com els forks).\n",
    "* Realitzar una PCA en 2 components sobre aquest.\n",
    "* Representar els resultats agrupant els punts en funci√≥ de diferents criteris:\n",
    "    + mainLanguage\n",
    "    + topic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estructura general de les dades num√®riques dels repositoris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the numerical fields of RepositoryVisits that we'll use for the PCA\n",
    "query = \"\"\"\n",
    "SELECT CONCAT(RepositoryVisits.owner, RepositoryVisits.name) AS id, forks, commits, stars, watchers, contributors, openIssues, closedIssues, mainLanguage\n",
    "FROM RepositoryVisits\n",
    "JOIN Repositories ON RepositoryVisits.owner = Repositories.owner AND RepositoryVisits.name = Repositories.name\n",
    "WHERE CAST(date AS Date) = '2024-04-16'\n",
    "\"\"\"\n",
    "\n",
    "_, df = execute_select_query(cursor, query)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the most used languages\n",
    "top_languages = df_repo['mainLanguage'].value_counts()[:10].index.tolist()\n",
    "\n",
    "df = df[df['mainLanguage'].isin(top_languages)]\n",
    "\n",
    "# Remove outliers\n",
    "for column in df:\n",
    "    if (df[column].dtype) != \"O\":\n",
    "        q_low = df[column].quantile(0.05)\n",
    "        q_high = df[column].quantile(0.95)\n",
    "        # print(q_low)\n",
    "        # print(q_high)\n",
    "        df = df[(df[column] <= q_high) & (df[column] >= q_low)]\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[\"id\"]\n",
    "print(labels[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split numerical data.\n",
    "X = df.iloc[:,1:-1].values\n",
    "print(X[:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarize each attribute\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "print(X_std[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_std)\n",
    "print(pca.components_)\n",
    "print(sum(pca.explained_variance_ratio_))\n",
    "print(X_pca[:5])\n",
    "print(max(X_std[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_languages = df_repo['mainLanguage'].value_counts()[:10].index.tolist()\n",
    "cmap = plt.get_cmap('viridis')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, 10)]\n",
    "#color_map = dict(zip(l, colors))\n",
    "\n",
    "# Colors to use for each language in the scatterplot\n",
    "color_map = {\n",
    " 'C': (0,0.9,1,1),\n",
    " 'C#': (0.79,0,1,1),\n",
    " 'C++': (0,0.4,1,1),\n",
    " 'Go': (1,0.6,0,1),\n",
    " 'HTML': (1,0.94,0,1),\n",
    " 'Java': (1,0,0,1),\n",
    " 'JavaScript': (1,1,0,1),\n",
    " 'Python': (0,1,0.2,1),\n",
    " 'Shell': (0.6,0.6,0.6,1),\n",
    " 'TypeScript': (0.9,1,0,1),\n",
    " 'PHP': (0.5,1,0,1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter = plt.scatter(X_pca[:,0], X_pca[:,1], c = df[\"mainLanguage\"].map(color_map), alpha=0.5)\n",
    "\n",
    "legend_handles = [mpatches.Patch(color=color_map[language], label=language)\n",
    " for language in top_languages]\n",
    "plt.legend(handles=legend_handles, title='Languages')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repositoris \"Open Source\" & dades per a la p√†gina web\n",
    "Quant a l'objectiu de destacar repositoris open-source (i no nom√©s repositoris populars, com fa GitHub), volem exportar dades d'aquests repositoris en format `.json` per a que puguin ser usades per la p√†gina web.\n",
    "\n",
    "Considerem que repositoris s√≥n projectes open-source si tenen m√©s de 5 contribu√Ødors i m√©s de 50 issues en total - un criteri simple per√≤ efectiu.\n",
    "\n",
    "Recollir les dades necess√†ries involucra fer JOINs amb m√∫ltiples taules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "open_source_projects_query = \"\"\"\n",
    "SELECT r.owner, r.name, MAX(r.description) as description, r.mainLanguage,\n",
    "MAX(stars) as total_stars, MAX(contributors) as total_contributors, MAX(openIssues + closedIssues) as total_issues,\n",
    "MAX(o.avatar_url) as avatar_url, GROUP_CONCAT(DISTINCT t.topic) as topics,\n",
    "MAX(watchers) as total_watchers FROM Repositories r\n",
    "-- Join with RepositoryVisits to get metrics like contributors, stars, issues amount\n",
    "JOIN RepositoryVisits v\n",
    "ON r.owner = v.owner AND r.name = v.name\n",
    "-- Join with Owners to get the avatar URL\n",
    "JOIN Owners o\n",
    "ON r.owner = o.username\n",
    "-- Join with RepositoryTopics to get a list of the tags/topics of the repos\n",
    "JOIN RepositoryTopics t\n",
    "ON t.repo = r.name AND t.owner = r.owner\n",
    "GROUP BY r.owner, r.name, r.mainLanguage, r.license -- Must group by mainLanguage and license as well as they're non-aggregated\n",
    "HAVING total_contributors > 5 AND total_issues > 50\n",
    "ORDER BY total_contributors DESC; -- Show repos with most contributors first\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "output_repo, df_repo = execute_select_query(cursor, open_source_projects_query)\n",
    "print(len(df_repo), \"repositories matching criteria\")\n",
    "df_repo.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les dades d'aquests repositoris s'exporten a .json perque puguin ser usades per la p√†gina web.\n",
    "\n",
    "En aquest proc√©s tamb√© categoritzem els repositoris segons les seves tem√†tiques generals:\n",
    "- Desenvolupament web\n",
    "- Data science\n",
    "- Aplicacions\n",
    "- Eines de desenvolupament\n",
    "- Repositoris de recursos (ex. una col¬∑lecci√≥ d'algorismes)\n",
    "\n",
    "La categoritzaci√≥ es fa segons els \"topics\" (tags) que tenen els repositoris. Els repositoris tamb√© es categoritzen pel seu llenguatge de programaci√≥ principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "used_languages = set()\n",
    "used_tags = set()\n",
    "\n",
    "# Renames or discards languages.\n",
    "# Used to group up frameworks, variants and transpilers.\n",
    "LANGUAGE_REMAP = {\n",
    "    \"TypeScript\": \"JavaScript\",\n",
    "    \"Vue\": \"JavaScript\",\n",
    "    \"HTML\": \"JavaScript\", # Bruh\n",
    "    \"Jupyter Notebook\": \"Python\",\n",
    "    \"Kotlin\": \"Java & Kotlin\",\n",
    "    \"Java\": \"Java & Kotlin\",\n",
    "    \"C\": \"C/C++\",\n",
    "    \"C++\": \"C/C++\",\n",
    "    \"Ruby\": \"Others\",\n",
    "    \"Go\": \"Others\",\n",
    "    \"Swift\": \"Others\",\n",
    "    \"Clojure\": \"Others\",\n",
    "    \"Haskell\": \"Others\",\n",
    "    \"Dart\": \"Others\",\n",
    "    \"Shell\": \"Others\",\n",
    "    \"PowerShell\": \"Others\",\n",
    "    \"Scala\": \"Others\",\n",
    "    \"Svelte\": \"Others\",\n",
    "    \"Vim Script\": \"\",\n",
    "    \"CSS\": \"\",\n",
    "    \"SCSS\": \"\",\n",
    "    \"MDX\": \"\",\n",
    "}\n",
    "\n",
    "# Remaps GitHub topics to the tags that are used in the website.\n",
    "TAG_MAP = {}\n",
    "# Maps tags used by the website to a list of Github \"topics\" that will be considered as the same tag.\n",
    "# Ex. repos with topics \"react\", \"vue\" become tagged as \"Web\"\n",
    "TAG_ALIASES = {\n",
    "    \"Web\": [\"react\", \"vue\", \"web\", \"reactjs\", \"css\", \"chrome-extension\", \"react-grid\", \"react-table\", \"php\", \"http\", \"nodejs\", \"typescript\", \"electron\", \"search-engine\", \"webgl\", \"rest\", \"rest-api\", \"swagger\", \"static-site-generator\", \"blog-engine\", \"router\", \"webview\", \"jquery\", \"http-client\", \"website\", \"reactive-templates\", \"nuxt\", \"nat\", \"javascript\", \"http2\", \"nginx\", \"apache\", \"aws\", \"api-gateway\", \"jekyll\", \"bootstrap\"],\n",
    "    \"Modding\": [\"mod\", \"minecraft\", \"emulation\", \"emulator\", \"forge\", \"minecraft-launcher\", \"modrinth\", \"minecraft-api\", \"minecraft-server\", \"bepinex\", \"unity3d\", \"unreal\", \"unity-mono\", \"craftbukkit\", \"valheim\", \"minecraft-mod\", \"gta5\", \"fabric\", \"gamedev\", \"retroarch\", \"game\"],\n",
    "    \"Data Science\": [\"math\", \"numpy\", \"data-science\", \"graphql\", \"data-visualization\", \"jupyter-notebook\", \"big-data\"],\n",
    "    \"Machine Learning\": [\"ml\", \"pytorch\", \"deep-learning\", \"machine-learning\", \"deep-neural-networks\", \"tensorflow\", \"neural-network\", \"tensor\", \"computer-vision\", \"reinforcement-learning\", \"hyperparameter-tuning\", \"ai\", \"artificial-intelligence\", \"llama\", \"llms\", \"llm\", \"openai\"],\n",
    "    \"Tool\": [\"containers\", \"zsh\", \"docker\", \"github\", \"cli\", \"searchengine\", \"postgrest\", \"devtool\", \"cloudstorage\", \"git\", \"npm\", \"database\", \"postgresql\", \"backend\", \"shell-scripting\", \"websocket\", \"collaboration\", \"developer-tools\", \"promise\", \"api\", \"testing\", \"translation\", \"i18n\", \"language\", \"golang-library\", \"algorithm\", \"firmware\", \"style-linter\", \"linting\", \"converter\", \"blockchain\", \"wordpress\", \"static-site-generator\", \"blog-engine\", \"material\", \"material-design\", \"framework\", \"argument-parser\", \"command-line-parser\", \"readme-generator\", \"ssh\", \"backup\", \"reverse-engineering\", \"animation\", \"sdk\", \"devops\", \"jenkins\", \"documentation\", \"terminal\", \"encryption\", \"scrapers\", \"3d-printing\", \"reactive-templates\", \"image-optimization\", \"file-server\", \"nat\", \"proxy\", \"shell\", \"linters\", \"git-client\", \"raspberry-pi\", \"blogging\", \"npm-cli\", \"aws\", \"api-gateway\", \"decompiler\", \"kubernetes\", \"tools\"],\n",
    "    \"App\": [\"note-taking\", \"productivity\", \"prest\", \"download\", \"latex\", \"text-editor\", \"curl\", \"ftp\", \"bot\", \"synchronization\", \"sqlite\", \"mattermost\", \"messaging\", \"conferencing\", \"remote-desktop\", \"emacs\", \"color-picker\", \"cli-app\", \"subtitle-downloader\", \"decompiler\", \"mobile-app\"],\n",
    "\n",
    "    \"Resource\": [\"learn-to-code\", \"freecodecamp\", \"curriculum\", \"certification\", \"learnopengl\", \"lists\", \"resources\", \"resource\", \"dataset\", \"public-api\", \"public-apis\", \"practice\", \"interview\", \"styleguide\", \"list\", \"interview-questions\", \"awesome-list\", \"principles\", \"design-patterns\"],\n",
    "}\n",
    "# Tags manually added to some repositories (which otherwise lack descriptive ones)\n",
    "MANUAL_TAGS = {\n",
    "    \"minio/minio\": [\"Machine Learning\"],\n",
    "    \"Aliucord/Aliucord\": [\"Modding\"],\n",
    "    \"cli-guidelines/cli-guidelines\": [\"Resource\"],\n",
    "    \"yjs/yjs\": [\"Tool\"],\n",
    "    \"TigerVNC/tigervnc\": [\"Tool\"],\n",
    "    \"ollama/ollama\": [\"App\"],\n",
    "    \"micropython/micropython\": [\"Tool\"], # Python implementation\n",
    "    \"raspberrypi/linux\": [\"App\"],\n",
    "    \"rust-lang/rust\": [\"Tool\"],\n",
    "    \"home-assistant/core\": [\"Tool\"],\n",
    "    \"vuejs/vue-cli\": [\"Tool\", \"Web\"],\n",
    "    \"remix-run/remix\": [\"Tool\", \"Web\"],\n",
    "}\n",
    "# Same thing as above, but mapping tag to list of repos with it instead, as I realized at the end this would've been more convenient.\n",
    "MANUAL_TAGS2 = {\n",
    "    \"Tool\": [\"pytorch/tutorials\", \"vuejs/core\", \"google/googletest\", \"google/guava\", \"ReactiveX/RxJava\"],\n",
    "    \"Web\": [\"vuejs/core\"],\n",
    "    \"App\": [\"square/retrofit\"],\n",
    "}\n",
    "for tag,aliases in TAG_ALIASES.items():\n",
    "    for alias in aliases:\n",
    "        TAG_MAP[alias] = tag\n",
    "for tag,repos in MANUAL_TAGS2.items():\n",
    "    for repo in repos:\n",
    "        if repo in MANUAL_TAGS:\n",
    "            MANUAL_TAGS[repo].append(tag)\n",
    "        else:\n",
    "            MANUAL_TAGS[repo] = [tag]\n",
    "\n",
    "IGNORED_TAGS = defaultdict(int)\n",
    "\n",
    "oss_repos = {}\n",
    "for index, row in df_repo.iterrows():\n",
    "    key = row[\"owner\"] + \"/\" + row[\"name\"]\n",
    "    main_language = row[\"mainLanguage\"]\n",
    "    if main_language in LANGUAGE_REMAP:\n",
    "        main_language = LANGUAGE_REMAP[main_language]\n",
    "    repo = {\n",
    "        \"topics\": set(),\n",
    "        \"languages\": set([main_language] if main_language != \"\" else []),\n",
    "        \"stars\": row[\"total_stars\"],\n",
    "        \"contributors\": row[\"total_contributors\"],\n",
    "        \"icon\": row[\"avatar_url\"],\n",
    "        \"description\": row[\"description\"],\n",
    "    }\n",
    "    for topic in str.split(row[\"topics\"], \",\"):\n",
    "        if topic in TAG_MAP:\n",
    "            repo[\"topics\"].add(TAG_MAP[topic])\n",
    "        else:\n",
    "            IGNORED_TAGS[tag] += 1\n",
    "        if key in MANUAL_TAGS:\n",
    "            for tag in MANUAL_TAGS[key]:\n",
    "                repo[\"topics\"].add(tag)\n",
    "\n",
    "    oss_repos[key] = repo\n",
    "    used_tags = used_tags.union(repo[\"topics\"])\n",
    "    used_languages.add(main_language)\n",
    "\n",
    "# Exclude mirrors and other projects that are not contributable projects or unsuitable\n",
    "BLACKLISTED_REPOS = [\n",
    "    \"gitlabhq/gitlabhq\", # Read-only mirror.\n",
    "    \"qemu/qemu\", # Read-only mirror.\n",
    "    \"xasset/xasset\",# Not english.\n",
    "    \"jynew/jynew\", # Unity RPG game framework, documentation in chinese-only though.\n",
    "    \"doocs/advanced-java\", # Chinese-only Java interview questions.\n",
    "    \"CyC2018/CS-Notes\", # Chinese computer science course resources.\n",
    "    \"apache/kafka\", # Read-only mirror.\n",
    "]\n",
    "for blacklisted_repo in BLACKLISTED_REPOS:\n",
    "    del oss_repos[blacklisted_repo]\n",
    "\n",
    "# Convert sets to lists for json serialization, and add other\n",
    "# keys the site expects\n",
    "for key,repo in oss_repos.items():\n",
    "    repo[\"owner\"] = key.split(\"/\")[0]\n",
    "    repo[\"repo\"] = key.split(\"/\")[1]\n",
    "    repo[\"topics\"] = list(repo[\"topics\"])\n",
    "    repo[\"languages\"] = list(repo[\"languages\"])\n",
    "\n",
    "# Export the .json\n",
    "with open(\"repositories_output.json\", \"w\") as f:\n",
    "    json.dump(oss_repos, f, indent=2)\n",
    "\n",
    "print(\"Valid repositories:\", len(oss_repos))\n",
    "print(\"Languages used:\", used_languages)\n",
    "print(\"Tags used:\", used_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com els repositoris en la base de dades foren principalment trobades per scraping de les p√†gines \"trending\" de GitHub, podem concloure que un 50% dels repositoris que GitHub destaca s√≥n nom√©s repositoris populars de projectes individuals o d'equips petits i no projectes contribu√Øbles; nom√©s uns 1000 repositoris dels 2000 en la base de dades compleixen els requisits que hem imposat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis de missatges de commits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En aquesta secci√≥, farem una an√†lisi dels missatges que fan els usuaris a l‚Äôhora de fer commits en github.\n",
    "Veurem quines paraules claus usen m√©s i quin √©s l‚Äôestil m√©s usat amb l‚Äôobjectiu de determinar una bona\n",
    "forma d‚Äôescriure un missatge de commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import warnings\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "\"\"\"\n",
    "If \n",
    "!pip install wordcloud \n",
    "doesn't work:\n",
    "from os import path\n",
    "import sys\n",
    "print(sys.executable) # use the path\n",
    "\n",
    "path -m pip install wordcloud\n",
    "\"\"\"\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define constants and configurations\n",
    "STANDARDIZE_SUBSTITUTION = re.compile(r\"[!\\\"$#%&()*+,\\-./:;<=>?[\\]^_`{|}~]\")\n",
    "CONSECUTIVE_SPACE_SUBSTITUTION = re.compile(r\"  +\")\n",
    "LEMMATIZATION_BLACKLIST = {\"was\", \"as\"}\n",
    "WORD_REPLACEMENTS = {\n",
    "    \"read-me\": \"readme\",\n",
    "    \"read.me\": \"readme\",\n",
    "    \"readme.md\": \"readme\"\n",
    "}\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "# stopwords.update([\"a\"]) # Manually add stopwords if needed\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcions auxiliars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accents(input_str):\n",
    "    \"\"\"\n",
    "    Convert accented characters to base characters.\n",
    "    Source: https://stackoverflow.com/a/1207479\n",
    "    \"\"\"\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str).encode(\"ascii\", \"ignore\")\n",
    "    return bytes.decode(nfkd_form)\n",
    "\n",
    "lemmatizer = nltk.wordnet.WordNetLemmatizer()\n",
    "\n",
    "# Function to get the wordnet POS tag, where POS means Part of Speech, which is basically the grammatical category of a word\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def standardize(word):\n",
    "    \"\"\"Standardize a word.\"\"\"\n",
    "    word = word.lower()\n",
    "    word = re.sub(STANDARDIZE_SUBSTITUTION, \" \", word)\n",
    "    word = re.sub(CONSECUTIVE_SPACE_SUBSTITUTION, \" \", word)\n",
    "    word = remove_accents(word)\n",
    "    word = word.replace(\"'s\", \"\").replace(\"#\", \"\").strip()\n",
    "    words = [WORD_REPLACEMENTS.get(w, w) for w in word.split()]\n",
    "    \n",
    "    # Enhanced lemmatization with POS tagging\n",
    "    words = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) if w not in LEMMATIZATION_BLACKLIST else w for w in words]\n",
    "    \n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(standardize(\"thIs a TesT\"))\n",
    "print(standardize(\"readme.md\"))\n",
    "_standardize_test_words = [\n",
    "    \"Testing @here as#Dasd 333232 tests testings gItHUbs testing added\",  # tests -> test due lemmatization\n",
    "    \"Test_ing !!, (asd,as d).\",  # Underscore are considered separated words\n",
    "    \"Test ≈Ç≈Ç≈Ç≈Ç √±a√±√±a√± √°aaa\",  # Removal of accents and non standard characters\n",
    "    \"„ÉÜ„Çπ„Éà #asdasd arabian, wolves\",\n",
    "]\n",
    "\n",
    "for word in _standardize_test_words:\n",
    "    print(standardize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neteja de dades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com que l'objectiu es analitzar com escriuen les persones, hi ha un conjunt de missatges que no ens interessen. Per aix√≤ cal excloure tots aquells missatges que no compleixin uns requisits.\n",
    "\n",
    "Els nostres criteris d'exclusi√≥ s√≥n:\n",
    "- Missatges de bots, els quals podem identificar per un autor que tingui en el nom `[bot]` i missatges que tinguin noms com pot ser: `dependabot` o `renovatebot`\n",
    "- Eliminar files repetides, producte d'error en la recolecci√≥ de dades.\n",
    "- Els missatges automatics que genera github com poden ser: `Merge pull request`, `Merge branch`, `Squash`, `Initial commit` i `Revert`, ja que no s√≥n escrits per les persones.\n",
    "\n",
    "A partir d'aquests criteris, hem realitzat una neteja de les dades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the commits authored by a bot.\n",
    "from IPython.display import display\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "cond_a = df_commits[df_commits[\"author\"].str.contains(\"\\[bot]|-bot\")]\n",
    "cond_c = df_commits[df_commits[\"message\"].str.contains(\"dependabot\")]\n",
    "cond_d = df_commits[df_commits[\"message\"].str.contains(\"renovatebot\")]\n",
    "\n",
    "temp = pd.concat([cond_a])\n",
    "\n",
    "unique_auth = pd.unique(temp.author)\n",
    "unique_msg = pd.unique(cond_c.message)\n",
    "\n",
    "dict = {'bots' : unique_auth}\n",
    "df_bots = pd.DataFrame(dict)\n",
    "\n",
    "# displaying the bots names\n",
    "display(df_bots)\n",
    "\n",
    "dict = {'bots_message' : unique_msg}\n",
    "df_bots_msg = pd.DataFrame(dict)\n",
    "\n",
    "display(df_bots_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the bots\n",
    "print(\"With bots\", len(df_commits))\n",
    "df_commits = df_commits[~df_commits[\"author\"].isin(df_bots[\"bots\"])]\n",
    "print(\"Without bots\", len(df_commits))\n",
    "\n",
    "# Filter out repeated rows\n",
    "df_commits.drop_duplicates(inplace=True)\n",
    "\n",
    "# Filter out the automatic messages\n",
    "df_commits = df_commits[\n",
    "    ~(df_commits.message.str.startswith(\"Merge pull request\") |\n",
    "      df_commits.message.str.startswith(\"Merge branch\") |\n",
    "      df_commits.message.str.startswith(\"Merge remote\") |\n",
    "      df_commits.message.str.startswith(\"Merge commit\") |\n",
    "      df_commits.message.str.startswith(\"Merge tag\") |\n",
    "      df_commits.message.str.startswith(\"Squash\") |\n",
    "      df_commits.message.str.startswith(\"Initial commit\") |\n",
    "      df_commits.message.str.startswith(\"Revert \") |\n",
    "      df_commits.message.str.startswith(\"Add files via upload\") |\n",
    "      df_commits.message.str.startswith(\"üîÑ\") |\n",
    "      df_commits.message.str.startswith(\"[auto]\") |\n",
    "      df_commits.message.str.startswith(\"[maven-release-plugin]\")\n",
    "     )\n",
    "]\n",
    "print(\"Without automatic messages\", len(df_commits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui podem veure que de 94.872 missatges, hem reduit els missatges usables a 77.729 . Tot i que segur que queden alguns altres missatges autom√†tics que no hem pogut detectar a temps, per√≤ hem cobert la majoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis paraules m√©s usades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per fer l'an√†lisis de les paraules m√©s usades, ho farem de dues formes. \n",
    "- Usarem wordcloud, el qual √©s una representac√≥ gr√†fica d'un dibuix creat per paraules on segons les ocurr√®ncies d'aquestes, s√≥n m√©s grans o si apareixen poc, m√©s petites. Obtenint aix√≠, una visi√≥ r√†pida, de quines paraules s√≥n les m√©s usades.\n",
    "- Comptarem les paraules i mostrarem un gr√†fic dels top n paraules m√©s usats en ordre descendent, segons paraula m√©s usada en missatges individuals o ocurr√®ncies totals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionalment, tenim `obtain_mask()` el qual usant `transform_format()` converteix una imatge blanc i negra en una m√†scara que podem usar en el wordcloud perqu√® nom√©s apareguin lletres dins de la regi√≥ negra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_cloud(word_cloud, text, save_image=False, image_name=\"none\"):\n",
    "    \"\"\"Create and display a word cloud image.\"\"\"\n",
    "    word_cloud_output = word_cloud.generate(text)\n",
    "    plt.imshow(word_cloud_output, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    if save_image:\n",
    "        word_cloud_output.to_file(f\"./Images/wordclouds/{image_name}_word_cloud.png\")\n",
    "\n",
    "def transform_format(val):\n",
    "    \"\"\"Transform format for mask creation.\"\"\"\n",
    "    return 255 if val == 0 else val\n",
    "\n",
    "def obtain_mask(mask):\n",
    "    \"\"\"Obtain mask for word cloud.\"\"\"\n",
    "    trans_mask = np.ndarray((mask.shape[0], mask.shape[1]), np.int32)\n",
    "    for i in range(len(mask)):\n",
    "        trans_mask[i] = list(map(transform_format, mask[i]))\n",
    "    return trans_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all text\n",
    "all_text = \" \".join(commit for commit in df_commits.message)\n",
    "# Set the wordcloud params\n",
    "word_cloud = WordCloud(stopwords=stopwords, max_font_size=30, max_words=200, background_color=\"black\")\n",
    "# Plot the wordcloud\n",
    "plot_word_cloud(word_cloud, all_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source of the GitHub image: [Icon by Dryicons](https://dryicons.com/icon/square-github-icon-8312)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a mask and a color map\n",
    "github_image = np.array(Image.open(\"./Images/wordclouds/github_square.png\"))\n",
    "\n",
    "word_cloud_mask = obtain_mask(github_image)\n",
    "word_cloud = WordCloud(stopwords=stopwords, colormap='rainbow', mask=word_cloud_mask, max_font_size=30, max_words=10000, background_color=\"black\")\n",
    "plot_word_cloud(word_cloud, all_text, True, \"git_mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a mask        \n",
    "git_image = np.array(Image.open(\"./Images/wordclouds/git.png\"))\n",
    "\n",
    "word_cloud_mask = obtain_mask(git_image)\n",
    "word_cloud = WordCloud(stopwords=stopwords, mask=word_cloud_mask,colormap='hot', max_font_size=20, max_words=10000, background_color=\"#474747\")\n",
    "plot_word_cloud(word_cloud, all_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podem veure que les paraules m√©s usades, s√≥n: `fix, add, update` sense comptar els stopwords, ja que podem veure que s√≥n les paraules m√©s grans. Tot seguit veurem els valors exactes, de les paraules m√©s usades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def count_words(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    :param df: DataFrame with the messages and associated information\n",
    "    :return: Dictionary with the format {word: {n_ocur: value, n_messages: value}, ...}\n",
    "    \"\"\"\n",
    "    # Using defaultdict for convenience; we won't have to add keys explicitly\n",
    "    dicc = defaultdict(lambda: {\"n_ocur\": 0, \"n_messages\": 0})\n",
    "\n",
    "    total_rows = len(df)\n",
    "    processed_rows_count = 0\n",
    "    for row in df.iterrows():\n",
    "        text = row[1][0]\n",
    "        text = standardize(text)  # Apply standardization\n",
    "        \n",
    "        # Split the text, remove digits and skip if empty\n",
    "        words = text.split(\" \")\n",
    "        words_without_digits = [word for word in words if not re.search(r'\\d', word)]\n",
    "        unique_words = set(words_without_digits)\n",
    "        \n",
    "        if len(unique_words) == 1 and '' in unique_words:\n",
    "            continue\n",
    "            \n",
    "        # Times that a word appears and times that appears in different messages.\n",
    "        for word in words_without_digits:\n",
    "            dicc[word][\"n_ocur\"] += 1\n",
    "        for word in unique_words:\n",
    "            dicc[word][\"n_messages\"] += 1\n",
    "\n",
    "        processed_rows_count += 1\n",
    "        # print(f\"Word {processed_rows_count}/{total_rows} done; {processed_rows_count / total_rows * 100:.2f}%\")\n",
    "    \n",
    "    return dicc\n",
    "\n",
    "frame = {'Messages': df_commits.message}\n",
    "\n",
    "result = pd.DataFrame(frame)\n",
    "\n",
    "ocurrencesDict = count_words(result)\n",
    "\n",
    "# Sort the words by occurrences and occurrences in messages.\n",
    "def obtain_top_n_words(dictionary, N, filter=\"n_ocur\", desc=True):\n",
    "    # Get all the words and their frequency\n",
    "    filtered_words = [(word, freq[filter]) for word, freq in dictionary.items() if word not in stopwords]\n",
    "    \n",
    "    # Sort from most to least frequent\n",
    "    sorted_words = sorted(filtered_words, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Take the N most frequent\n",
    "    return [(word, freq) for word, freq in sorted_words[:N]]\n",
    "\n",
    "top_words_list_ocur = obtain_top_n_words(ocurrencesDict, 30, \"n_ocur\", True)\n",
    "top_words_list_msg = obtain_top_n_words(ocurrencesDict, 30, \"n_messages\", True)\n",
    "\n",
    "print(\"Ocurrences Top\", top_words_list_ocur)\n",
    "print(\"\\nOcurrences per message Top\", top_words_list_msg)\n",
    "\n",
    "# Function to plot the top words\n",
    "def plot_top_words(topList, yLabel, ax):\n",
    "    x = [word for word, freq in topList]\n",
    "    y = [freq for word, freq in topList]\n",
    "    \n",
    "    # Making the bar chart on the data\n",
    "    ax.bar(x, y)\n",
    "    \n",
    "    # Giving title to the plot\n",
    "    ax.set_title(\"Top paraules m√©s usades\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(x, rotation=90, ha='right')\n",
    "    # Giving X and Y labels\n",
    "    ax.set_xlabel(\"Paraules\")\n",
    "    ax.set_ylabel(yLabel)\n",
    "     \n",
    "    # We do not call plt.show() here, as we want to show both plots together\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "# Plot the top words by occurrences\n",
    "plot_top_words(top_words_list_ocur, \"Ocurr√®ncia de paraules\", axs[0])\n",
    "\n",
    "# Plot the top words by occurrences in different messages\n",
    "plot_top_words(top_words_list_msg, \"Ocurrencies de paraules per missatge\", axs[1])\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plot\")\n",
    "# Show the plots\n",
    "plt.show()\n",
    "\n",
    "# We can see that there is no significant difference, since normally, in a commit message, we do not repeat the same word twice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De les gr√†fiques, podem veure que en termes quantitatius, s√≥n casi id√®ntics, tenen la mateixa escala i en general, el mateix ordre de les paraules m√©s usades, encara que la paraula fix, √©s el que t√© m√©s difer√®ncia indicant, que les persones usen m√©s d'un cop la paraula fix en un missatge. Tot i aix√≤ denota que no se sol repetir les paraules en un mateix text, ja que totes les altres paraules, tenen un valor similar i la gr√†fica de l'esquerra, compta les paraules totals, mentre que el de la dreta, compta les paraules per missatge.\n",
    "\n",
    "Les tres paraules m√©s usades, son `update, add, fix`, els quals eren els mateixos que hem vist en el wordcloud. Hem agrupat les paraules segons un possible significat sem√†ntic.\n",
    "\n",
    "### Cicle de Desenvolupament:\n",
    "\n",
    "- **Add:** Adici√≥ de noves caracter√≠stiques i creixement del projecte.\n",
    "- **Fix:** Correci√≥ d'errors per assegurar l'estabilitat del projecte.\n",
    "- **Update:** Millores i actualitzacions del codi existent, assegurant que el projecte es mantingui actualitzat amb les millors pr√†ctiques i tecnologies.\n",
    "\n",
    "### Paraules Complement√†ries:\n",
    "\n",
    "- **Remove:** Reflecteix l'eliminaci√≥ de codi o funcionalitats obsoletes o innecess√†ries, la qual cosa √©s crucial per mantenir el codi net i eficient.\n",
    "- **Use:** Indica la implementaci√≥ o reutilitzaci√≥ de codi o biblioteques, suggerint un √∫s eficient dels recursos disponibles.\n",
    "- **Test:** Subratlla la import√†ncia de les proves en el cicle de desenvolupament per assegurar la funcionalitat correcta i evitar regressions.\n",
    "- **Readme:** Reflecteix la documentaci√≥ del projecte, important per a la col¬∑laboraci√≥ i la claredat entre els membres de l'equip i els usuaris.\n",
    "- **Feat:** Similar a \"add\", mostra l'enfocament en noves funcionalitats.\n",
    "- **Chore:** Tasques de manteniment o administratives per a la neteja i organitzaci√≥ del projecte.\n",
    "  \n",
    "### Paraules Clau en el Manteniment i Millora:\n",
    "\n",
    "- **Version:** Indica canvis en les versions del projecte, reflectint una evoluci√≥ cont√≠nua i la gesti√≥ de versions.\n",
    "- **File:** Pot referir-se a la gesti√≥ de fitxers dins del projecte, com la creaci√≥, modificaci√≥ o eliminaci√≥ de fitxers.\n",
    "- **Refactor:** Subratlla l'esfor√ß de reorganitzaci√≥ del codi per millorar la seva estructura sense canviar-ne el comportament extern.\n",
    "- **Improve:** Reflecteix les millores cont√≠nues del codi o funcionalitats existents.\n",
    "\n",
    "### Altres Paraules Rellevants:\n",
    "\n",
    "- **Build, Link , Error:** Aquestes paraules indiquen activitats espec√≠fiques relacionades amb la compilaci√≥ del projecte, la gesti√≥ d'enlla√ßos, i la correcci√≥ d'errors respectivament.\n",
    "- **CI:** Fa refer√®ncia a la integraci√≥ cont√≠nua, important per al desplegament autom√†tic i la verificaci√≥ cont√≠nua de canvis.\n",
    "- **Config:** Reflecteix la configuraci√≥ del projecte, que √©s crucial per a la seva correcta execuci√≥ i desplegament\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grups de paraules m√©s usades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En aquesta secci√≥, usarem **√≠tems freq√ºents** per trobar patrons els quals els usuaris escriuen. La idea principal, es trobar grups de paraules els quals quan apareix un, apareix un altre. Aix√≤ indicaria que les paraules estan relacionades o s'utilitzen conjuntament amb freq√º√®ncia. Aquest proc√©s ens ajuda a identificar associacions o patrons significatius en el text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/apriori/#apriori-frequent-itemsets-via-the-apriori-algorithm\n",
    "def generate_data_set(df: pd.DataFrame, rmStopWords = True, useStandard = True):\n",
    "    \"\"\"\n",
    "    :param df: DataFrame with the messages and associated information\n",
    "    :return dataSet: The message of the commit is the transaction and the words the items.\n",
    "    \"\"\"\n",
    "    data_set = []\n",
    "    total_rows = len(df)\n",
    "    processed_rows_count = 0\n",
    "    for row in df.iterrows():\n",
    "        text = row[1][0]\n",
    "        if (useStandard):\n",
    "            text = standardize(text)  # Apply standardization\n",
    "       \n",
    "        # Split the words, but don't hold digits.\n",
    "        words = [word for word in text.split() if not any(char.isdigit() for char in word)]\n",
    "        if (rmStopWords):\n",
    "            unique_words = list(set(words) - stopwords)\n",
    "        else:\n",
    "            unique_words = list(set(words))\n",
    "        data_set.append(unique_words)\n",
    "\n",
    "        processed_rows_count += 1\n",
    "        # print(f\"Word {processed_rows_count}/{total_rows} done; {processed_rows_count / total_rows * 100:.2f}%\")\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "import math\n",
    "#!pip install mlxtend\n",
    "\n",
    "def get_frequent_itemsets(data, min_supp = 0.01):\n",
    "    \"\"\"\n",
    "    To save memory, you may want to represent your transaction data in the sparse format. \n",
    "    This is especially useful if you have lots of products and small transactions.\n",
    "    \"\"\"\n",
    "    te = TransactionEncoder()\n",
    "    oht_ary = te.fit(data).transform(data, sparse=True)\n",
    "    sparse_df = pd.DataFrame.sparse.from_spmatrix(oht_ary, columns=te.columns_)\n",
    "    \n",
    "    # total message * support = number of messages which the bundle appears.\n",
    "    print(\"We consider that at least the bundle appears in:\", math.ceil(len(data) * min_supp), \" messages\")\n",
    "    \n",
    "    # Calculate it using apriori algorithm\n",
    "    frequent_itemsets = apriori(sparse_df, min_support=min_supp, use_colnames=True, verbose=1)\n",
    "    frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "    return frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data frame using the commits.\n",
    "frame = {'Messages': df_commits.message}\n",
    "result = pd.DataFrame(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the data set is too big, and our computers can't handle it, we are going to use a randomly sampled fraction of it.\n",
    "partial_data_set = generate_data_set(result.sample(frac =.50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets = get_frequent_itemsets(partial_data_set, 0.001)\n",
    "# Sort them from more length and then support\n",
    "frequent_itemsets_sorted = frequent_itemsets.sort_values(by=['length', 'support'], ascending=[False, False])\n",
    "print(frequent_itemsets_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_support = math.ceil(frequent_itemsets_sorted['support'].mean() * len(partial_data_set))\n",
    "print(\"Average amount of support: \", average_support, \" messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_group = frequent_itemsets[(frequent_itemsets['length'] >= 2)]\n",
    "data_group['itemsets'] = data_group['itemsets'].apply(lambda x: str(sorted(list(x))))\n",
    "data_group['support messages'] = data_group['support'].apply(lambda x: math.ceil(len(partial_data_set) * x))\n",
    "data_group = data_group.sort_values(by=['support messages'], ascending=False)\n",
    "# Show only those groups, who at least, have the average support.\n",
    "# With this, we can preserve significant data, while reducing the amount of itemsets\n",
    "data_group = data_group[(data_group['support messages'] >= average_support)]\n",
    "\n",
    "print(data_group[['itemsets', 'support messages']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to group itemsets by starting word\n",
    "def group_by_starting_word(df):\n",
    "    grouped_phrases = {}\n",
    "    for itemset in df['itemsets']:\n",
    "        cleaned_string = itemset.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")\n",
    "        items = cleaned_string.split(', ')\n",
    "        items_list = list(map(str.strip, items))  # This removes leading/trailing spaces\n",
    "        if items_list[0] in grouped_phrases:\n",
    "            grouped_phrases[items_list[0]].append(items_list[1])\n",
    "        else:\n",
    "            grouped_phrases[items_list[0]] = [items_list[1]]\n",
    "    return grouped_phrases\n",
    "\n",
    "# Group itemsets by starting word\n",
    "grouped_phrases = group_by_starting_word(data_group)\n",
    "print(grouped_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "# Show the \n",
    "def plot_graph(key, values):\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add main node (key)\n",
    "    G.add_node(key)\n",
    "\n",
    "    # Add edges between main node and each phrase (value)\n",
    "    for value in values:\n",
    "        G.add_edge(key, value)\n",
    "\n",
    "    # Plotting the graph\n",
    "    pos = nx.spring_layout(G, seed=42)  # positions for all nodes\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=2000, font_size=10, font_weight='bold', arrows=False)\n",
    "    plt.title(f'Undirected Graph for \"{key}\"')\n",
    "    plt.show()\n",
    "\n",
    "# Iterate through each key-value pair and plot the graph\n",
    "for key, values in grouped_phrases.items():\n",
    "    plot_graph(key, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qu√® passa si no normalitzem o comptem els stop words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_with_stop_words = generate_data_set(result.sample(frac =.30), rmStopWords = False, useStandard = True)\n",
    "frequent_itemsets = get_frequent_itemsets(data_set_with_stop_words, 0.001)\n",
    "# Sort them from more length and then support\n",
    "frequent_itemsets_sorted = frequent_itemsets.sort_values(by=['length', 'support'], ascending=[False, False])\n",
    "print(frequent_itemsets_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_group = frequent_itemsets[(frequent_itemsets['length'] >= 2)]\n",
    "data_group['itemsets'] = data_group['itemsets'].apply(lambda x: str(sorted(list(x))))\n",
    "data_group['support messages'] = data_group['support'].apply(lambda x: math.ceil(len(partial_data_set) * x))\n",
    "data_group = data_group.sort_values(by=['support messages'], ascending=False)\n",
    "\n",
    "print(data_group[['itemsets', 'support messages']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_phrases = group_by_starting_word(data_group)\n",
    "print(grouped_phrases)\n",
    "# Iterate through each key-value pair and plot the graph\n",
    "for key, values in grouped_phrases.items():\n",
    "    plot_graph(key, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_no_standard = generate_data_set(result.sample(frac =.5), rmStopWords = False, useStandard = False)\n",
    "frequent_itemsets = get_frequent_itemsets(data_set_no_standard, min_supp = 0.005) \n",
    "\n",
    "# Sort them from more length and then support\n",
    "data_group = frequent_itemsets[(frequent_itemsets['length'] >= 2)]\n",
    "data_group['itemsets'] = data_group['itemsets'].apply(lambda x: str(sorted(list(x))))\n",
    "data_group['support messages'] = data_group['support'].apply(lambda x: math.ceil(len(partial_data_set) * x))\n",
    "data_group = data_group.sort_values(by=['support messages'], ascending=False)\n",
    "\n",
    "print(data_group[['itemsets', 'support messages']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_phrases = group_by_starting_word(data_group)\n",
    "print(grouped_phrases)\n",
    "# Iterate through each key-value pair and plot the graph\n",
    "for key, values in grouped_phrases.items():\n",
    "    plot_graph(key, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per limitacions d'espai, i seguint el consell del professor, hem decidit utilitzar fragments parcials del conjunt de dades en lloc de calcular l'algoritme amb tot el data set.\n",
    "\n",
    "El resultat s'ha decidit mostrar usant graphs, ja que l'algoritme d'√≠tems freq√ºents concentra la major quantitat de grups en els paquets de 2 i √©s f√†cil de veure com una paraula, es relaciona amb una altra usant aquesta.\n",
    "I el que hem obtingut, es el conjunt de paraules clau que solen apareixer junts. Per exemple, per fix, tenim:\n",
    "\n",
    "`'fix': ['in', 'to', 'for', 'on', 'the', 'of', 'issue', 'with']`\n",
    "\n",
    "El qual significa, que les persones solen escriure algo com:\n",
    "- \"fix xxxx to yyyyy\"\n",
    "- \"fix ssss with nnnn\"\n",
    "- \"fix issue eeee\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploraci√≥ sint√†ctica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En aquesta secci√≥, busquem identificar les categories gramaticals m√©s comunes o dominants dels missatges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from collections import Counter\n",
    "\n",
    "# Uncomment to dowload the necessary data to run the functions.\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map to make the tags more reaedable //https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "tag_map_cat = {\n",
    "    'CC': 'Conjunci√≥ coordinativa',\n",
    "    'CD': 'Nombre cardinal',\n",
    "    'DT': 'Determinant',\n",
    "    'EX': 'Hi ha existencial',\n",
    "    'FW': 'Paraula estrangera',\n",
    "    'IN': 'Preposici√≥ o conjunci√≥ subordinant',\n",
    "    'JJ': 'Adjectiu',\n",
    "    'JJR': 'Adjectiu comparatiu',\n",
    "    'JJS': 'Adjectiu superlatiu',\n",
    "    'LS': 'Marcador √≠tems de llista ',\n",
    "    'MD': 'Modal',\n",
    "    'NN': 'Nom, singular o massa',\n",
    "    'NNS': 'Nom, plural',\n",
    "    'NNP': 'Nom propi, singular',\n",
    "    'NNPS': 'Nom propi, plural',\n",
    "    'PDT': 'Predeterminant',\n",
    "    'POS': 'Sufix possessiu',\n",
    "    'PRP': 'Pronom personal',\n",
    "    'PRP$': 'Pronom possessiu',\n",
    "    'RB': 'Adverbi',\n",
    "    'RBR': 'Adverbi comparatiu',\n",
    "    'RBS': 'Adverbi superlatiu',\n",
    "    'RP': 'Part√≠cula',\n",
    "    'SYM': 'S√≠mbol',\n",
    "    'TO': 'a',\n",
    "    'UH': 'Interjecci√≥',\n",
    "    'VB': 'Verb, forma base',\n",
    "    'VBD': 'Verb, pret√®rit',\n",
    "    'VBG': 'Verb, gerundi o participi present',\n",
    "    'VBN': 'Verb, participi passat',\n",
    "    'VBP': 'Verb, present no tercera persona singular',\n",
    "    'VBZ': 'Verb, present tercera persona singular',\n",
    "    'WDT': 'Determinant WH',\n",
    "    'WP': 'Pronom WH',\n",
    "    'WP$': 'Pronom possessiu WH',\n",
    "    'WRB': 'Adverbi WH'\n",
    "}\n",
    "# English version\n",
    "tag_map_en = {\n",
    "    'CC': 'Coordinating conjunction',\n",
    "    'CD': 'Cardinal number',\n",
    "    'DT': 'Determiner',\n",
    "    'EX': 'Existential there',\n",
    "    'FW': 'Foreign word',\n",
    "    'IN': 'Preposition or subordinating conjunction',\n",
    "    'JJ': 'Adjective',\n",
    "    'JJR': 'Adjective, comparative',\n",
    "    'JJS': 'Adjective, superlative',\n",
    "    'LS': 'List item marker',\n",
    "    'MD': 'Modal',\n",
    "    'NN': 'Noun, singular or mass',\n",
    "    'NNS': 'Noun, plural',\n",
    "    'NNP': 'Proper noun, singular',\n",
    "    'NNPS': 'Proper noun, plural',\n",
    "    'PDT': 'Predeterminer',\n",
    "    'POS': 'Possessive ending',\n",
    "    'PRP': 'Personal pronoun',\n",
    "    'PRP$': 'Possessive pronoun',\n",
    "    'RB': 'Adverb',\n",
    "    'RBR': 'Adverb, comparative',\n",
    "    'RBS': 'Adverb, superlative',\n",
    "    'RP': 'Particle',\n",
    "    'SYM': 'Symbol',\n",
    "    'TO': 'to',\n",
    "    'UH': 'Interjection',\n",
    "    'VB': 'Verb, base form',\n",
    "    'VBD': 'Verb, past tense',\n",
    "    'VBG': 'Verb, gerund or present participle',\n",
    "    'VBN': 'Verb, past participle',\n",
    "    'VBP': 'Verb, non-3rd person singular present',\n",
    "    'VBZ': 'Verb, 3rd person singular present',\n",
    "    'WDT': 'Wh-determiner',\n",
    "    'WP': 'Wh-pronoun',\n",
    "    'WP$': 'Possessive wh-pronoun',\n",
    "    'WRB': 'Wh-adverb'\n",
    "}\n",
    "\n",
    "# Load all text\n",
    "all_text = \" \".join(commit for commit in df_commits.message)\n",
    "print(\"Totes les paraules:\", len(all_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ratios(all_text, tag_map):\n",
    "    # apply tokenization to the text and tag it\n",
    "    tokens = word_tokenize(all_text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    \n",
    "    # Count the occurrence of each tag\n",
    "    tag_counts = Counter(tag for word, tag in tagged)\n",
    "    \n",
    "    # Convert tags to full wording in Catalan\n",
    "    english_tag_counts = {tag_map.get(tag, tag): count for tag, count in tag_counts.items()}\n",
    "    \n",
    "    # Calculate the ratio of each tag\n",
    "    total_tags = sum(tag_counts.values())\n",
    "    tag_ratios = {tag: count / total_tags for tag, count in english_tag_counts.items()}\n",
    "    \n",
    "    # Sort ratios by values in descending order\n",
    "    sorted_ratios = sorted(tag_ratios.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    # Create a dataframe to show the result\n",
    "    df_ratios = pd.DataFrame(sorted_ratios, columns=['Etiqueta', 'Ratio'])\n",
    "    \n",
    "    # Show the equivalence of ratio in words\n",
    "    data_size = len(all_text)\n",
    "    df_ratios['Paraules'] = df_ratios['Ratio'].apply(lambda x: math.ceil(x * data_size))\n",
    "    df_ratios['Ratio'] = df_ratios['Ratio'].apply(lambda x: (x * 100)) # Put it in percentage\n",
    "    \n",
    "    return df_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "titles_cat = ['Categoria gramatical o s√≠mbols', 'Percentatge (%)', 'Distribuci√≥ √∫s de categories gramaticals']\n",
    "titles_en = ['Grammatical category or symbols', 'Percentage (%)', 'Distribution of use of grammatical categories']\n",
    "\n",
    "titles = titles_cat\n",
    "tag_map = tag_map_cat\n",
    "    \n",
    "df_ratios = calc_ratios(all_text, tag_map)\n",
    "print(df_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the ratios\n",
    "tags = df_ratios[\"Etiqueta\"]\n",
    "ratios = df_ratios[\"Ratio\"]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(tags, ratios, color='skyblue')\n",
    "plt.xlabel(titles[0])\n",
    "plt.ylabel(titles[1])\n",
    "plt.title(titles[2])\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A trav√©s dels resultats, podem destacar que hi ha una domin√†ncia dels noms: Les categories \"Nom, singular o massa\" i \"Nom propi, singular\" ocupen les dues primeres posicions en termes de freq√º√®ncia relativa (ratio). Aix√≤ podria indicar que solen descriure o senyalar molt a entitas concrets i per aix√≤ potser tenim en tercera posici√≥ els adejctius. \n",
    "\n",
    "Per tant, es podria dir que els missatges s√≥n m√©s aviat descriptives, amb l'objectiu de facilitar i saber a qu√® fan refer√®ncia. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I quines formes verbals s'usen m√©s?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ratio of verbs\n",
    "data_size = len(all_text)\n",
    "verb_usage = df_ratios[df_ratios['Etiqueta'].str.startswith('Verb')].copy()\n",
    "verb_usage['Paraules'] = verb_usage['Ratio'].apply(lambda x: math.ceil(x * len(all_text)))\n",
    "print(verb_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_cat = ['Temps verbals dels verbs', 'Percentatge (%)', 'Distribuci√≥ √∫s de categories gramaticals']\n",
    "titles_en = ['Verb tenses of verb', 'Percentage (%)', 'Verb usage distribution']\n",
    "\n",
    "titles = titles_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = verb_usage['Etiqueta']\n",
    "ratios = verb_usage['Ratio']\n",
    "\n",
    "print(\"Total percentage of verbs: {:.2f}%\".format(verb_usage['Ratio'].sum()))\n",
    "print(\"Total verbs:\", verb_usage['Paraules'].sum())\n",
    "\n",
    "# Plotting the ratios\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(tags, ratios, color='skyblue')\n",
    "plt.xlabel(titles[0])\n",
    "plt.ylabel(titles[1])\n",
    "plt.title(titles[2])\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per tant, com a resultat, podem concloure que els missatges estan enfocats a clarificar i identificar els canvis realitzats en el codi. Ja que els verbs, ocupen un ~8% i l'altre part, queda dominada sobretot per noms, adjectius i simbols que els quals tots tene un enfoc de clarificar i assenyalar. Aquest enfocament √©s crucial per a la comunicaci√≥ eficient entre els membres de l'equip de desenvolupament i per a la comprensi√≥ precisa de les modificacions implementades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I si ho fem per missatges?\n",
    "Ara veurem, la mitjana, la desviaci√≥ i la mediana d'un missatge en termes de paraules usades. I farem el comput de la mitja de percentatge de √∫s gramatical segons missatges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_words = df_commits['message'].apply(lambda x: len(x.split())).mean()\n",
    "std_dev_words = df_commits['message'].apply(lambda x: len(x.split())).std()\n",
    "median_words = df_commits['message'].apply(lambda x: len(x.split())).median()\n",
    "\n",
    "print(f\"The average message of a commit has {math.ceil(average_words)} words. With a standard deviation of {math.ceil(std_dev_words)} words and a median of {median_words}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ratios_message(df_commits, tag_map, average_words):\n",
    "    all_ratios = []\n",
    "\n",
    "    for message in df_commits.message:\n",
    "        # apply tokenization to the text and tag it\n",
    "        tokens = word_tokenize(message)\n",
    "        tagged = pos_tag(tokens)\n",
    "\n",
    "        # Count the occurrence of each tag\n",
    "        tag_counts = Counter(tag for word, tag in tagged)\n",
    "\n",
    "        # Convert tags to full wording in Catalan\n",
    "        english_tag_counts = {tag_map.get(tag, tag): count for tag, count in tag_counts.items()}\n",
    "\n",
    "        # Calculate the ratio of each tag\n",
    "        total_tags = sum(tag_counts.values())\n",
    "        tag_ratios = {tag: count / total_tags for tag, count in english_tag_counts.items()}\n",
    "\n",
    "        all_ratios.append(tag_ratios)\n",
    "\n",
    "    # Calculate the average ratios\n",
    "    avg_ratios = pd.DataFrame(all_ratios).mean().sort_values(ascending=False)\n",
    "\n",
    "    # Create a dataframe to show the result\n",
    "    df_ratios = pd.DataFrame(avg_ratios.reset_index())\n",
    "    df_ratios.columns = ['Etiqueta', 'Ratio']\n",
    "\n",
    "    # Show the equivalence of ratio in words\n",
    "    data_size = len(df_commits)\n",
    "    df_ratios['Paraules'] = df_ratios['Ratio'].apply(lambda x: math.ceil(x * average_words))\n",
    "    df_ratios['Ratio'] = df_ratios['Ratio'].apply(lambda x: (x * 100)) # Put it in percentage\n",
    "\n",
    "    return df_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_cat = ['Categoria gramatical o s√≠mbols', 'Percentatge (%)', 'Categoria gramatical mitjana d\\'un missatge en termes de paraules utilitzades']\n",
    "titles_en = ['Grammatical category or Symbols', 'Percentage (%)', 'Average grammatical category of a message in terms of words used']\n",
    "\n",
    "titles = titles_cat\n",
    "tag_map = tag_map_cat\n",
    "\n",
    "df_average_ratio_per_message = calc_ratios_message(df_commits, tag_map, average_words)\n",
    "print(df_average_ratio_per_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the ratios\n",
    "tags = df_average_ratio_per_message[\"Etiqueta\"]\n",
    "ratios = df_average_ratio_per_message[\"Ratio\"]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(tags, ratios, color='skyblue')\n",
    "plt.xlabel(titles[0])\n",
    "plt.ylabel(titles[1])\n",
    "plt.title(titles[2])\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_cat = ['Temps verbals dels verbs', 'Percentatge (%)', 'Distribuci√≥ √∫s de verbs en un missatge segons temps verbal']\n",
    "titles_en = ['Verb tenses of verb', 'Percentage (%)', 'Verb usage distribution in a message based on verb tense']\n",
    "\n",
    "titles = titles_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ratio of verbs\n",
    "verb_usage = df_average_ratio_per_message[df_average_ratio_per_message['Etiqueta'].str.startswith('Verb')].copy()\n",
    "print(verb_usage)\n",
    "\n",
    "tags = verb_usage['Etiqueta']\n",
    "ratios = verb_usage['Ratio']\n",
    "\n",
    "# Plotting the ratios\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(tags, ratios, color='skyblue')\n",
    "plt.xlabel(titles[0])\n",
    "plt.ylabel(titles[1])\n",
    "plt.title(titles[2])\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podem veure que en un missatge, els noms, segueixen dominant la distribuci√≥ de la categoria gramatical en un commit. Indicant, per exemple, que si tenim un missatge de 6 paraules, el ~37% de les paraules, s√≥n noms, singular o massa. A difer√®ncia de la distribuci√≥ de totes les paraules, podem destacar, que els verbs, ara tenent m√©s protagonisme, on els pret√®rits i participi passat solen apareixer m√©s. Aix√≤ suggerix que els missatges de commit sovint descriuen accions que s'han completat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Altres analisis no-conclusius\n",
    "Per defici√®ncies en la recollida de dades desafortunadament hi ha altres an√†lisis que no hem pogut fer. A continuaci√≥ mostrem com es farien si tingu√©ssim les dades necess√†ries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer d'aquests era visualitzar l'evoluci√≥ de la quantitat de repositoris en el temps per cada topic. Desafortunadament hem perdut la majoria de les dades recollides; nom√©s les tenim per uns pocs dies, i per tant la gr√†fica no es gaire √∫til."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, df_topic_visits = execute_select_query(cursor, \"SELECT CAST(date AS Date) AS date, name, repositories, followers FROM TopicVisits;\")\n",
    "df_topic_visits_n_cut = df_topic_visits[df_topic_visits[\"name\"].isin(n_topics_name)]\n",
    "df_topicv_repos = df_topic_visits_n_cut[[\"date\", \"name\", \"repositories\"]]\n",
    "n_topics_name\n",
    "display(df_topic_visits_n_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topicv_repos.pivot_table(index='date', columns=\"name\", values=\"repositories\", aggfunc=\"sum\").plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volem ara veure una evoluci√≥ de la popularitat dels repositoris tractant els temes que hem identificat com a principals. √âs a dir, volem veure com ha anat canviant el nombre d'estrelles que reben aquests repositoris. Aquesta √©s una bona oportunitat que tenim per a treballar els joins entre taules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH RepositoriesMainTopics AS (\n",
    "\tSELECT repo AS name, topic\n",
    "    FROM RepositoryTopics\n",
    "    WHERE topic IN ('hacktoberfest', 'javascript', 'react', 'typescript', 'nodejs', 'python', 'docker', 'nextjs', 'golang', 'go', 'git', 'minecraft', 'java', 'vue', 'machine-learning', 'android', 'deep-learning', 'github', 'ios', 'cli')\n",
    "), TrendingTopic AS (\n",
    "\tSELECT date, repoName AS name, topic, starsToday as stars\n",
    "    FROM RepositoriesMainTopics\n",
    "    JOIN TrendVisits ON TrendVisits.repoName = RepositoriesMainTopics.name\n",
    ")\n",
    "SELECT CAST(date AS Date) AS date, topic, SUM(stars) AS stars\n",
    "FROM TrendingTopic\n",
    "GROUP BY date, topic\n",
    "\"\"\"\n",
    "_, df_trendRepositoriesMainTopics = execute_select_query(cursor, query)\n",
    "df_trendRepositoriesMainTopics.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trendRepositoriesMainTopics[\"stars\"] = df_trendRepositoriesMainTopics[\"stars\"].astype(float)\n",
    "df_trendRepositoriesMainTopics.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trendRepositoriesMainTopics.pivot_table(index='date', columns=\"topic\", values=\"stars\", aggfunc=\"sum\").plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closes the connection\n",
    "dataBaseConnection.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-python3-kernel",
   "language": "python",
   "name": "my-python3-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
