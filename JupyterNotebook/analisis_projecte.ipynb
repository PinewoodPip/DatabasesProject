{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projecte de Bases de Dades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquest notebook conté els queries que hem usat per obtenir les dades i l'anàlisis d'aquestes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install dependencies if necessary\n",
    "#!pip install PyMySQL\n",
    "#!pip install scikit-learn\n",
    "#!pip install nltk\n",
    "#!pip install wordcloud\n",
    "#!pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Draw the plots immediately after the current cell\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "plt.style.use('default')\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creació de la base de dades\n",
    "\n",
    "El backup de la base de dades final es pot trobar en `Database/github.sql`. Aquest conté el schema i les dades; per crear la base de dades només cal executar tot el fitxer (amb el botó ⚡ en MySQL Workbench). La base de dades s'anomena `github`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connexió\n",
    "\n",
    "Per començar a treballar amb la base de dades, cal connectar-se primer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = \"github\"\n",
    "db_host = \"localhost\"\n",
    "db_port = 3306\n",
    "db_username = \"root\"\n",
    "db_password = input(\"Enter the DB password\")\n",
    "\n",
    "dataBaseConnection = pymysql.connect(host=db_host,\n",
    "                            port=db_port,\n",
    "                            user=db_username,\n",
    "                            password=db_password,\n",
    "                            db=db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cursor to interact with the database\n",
    "cursor = dataBaseConnection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_select_query(cursor, query):\n",
    "    \"\"\"\n",
    "    Executes a query and returns the results as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    cursor.execute(query)\n",
    "    output = cursor.fetchall()\n",
    "\n",
    "    # Fetch column names from cursor's description\n",
    "    columns = [desc[0] for desc in cursor.description]\n",
    "    \n",
    "    # Convert output to pandas DataFrame\n",
    "    if output:\n",
    "        df = pd.DataFrame(output, columns=columns)\n",
    "    return output, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taules\n",
    "A continuació es mostren exemples de les entitats dins la base de dades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repositories\n",
    "query = \"SELECT * FROM Repositories WHERE mainLanguage != \\\"\\\"\"\n",
    "output_repo, df_repo = execute_select_query(cursor, query)\n",
    "df_repo.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repository visits\n",
    "query = \"SELECT * FROM RepositoryVisits;\"\n",
    "\n",
    "output_repo_visists, df_repo_visists = execute_select_query(cursor, query)\n",
    "df_repo_visists.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topics/tags of repositories\n",
    "query = \"\"\"\n",
    "SELECT r.name, r.owner, GROUP_CONCAT(t.topic) as topics FROM Repositories r\n",
    "JOIN RepositoryTopics t\n",
    "ON t.repo = r.name AND t.owner = r.owner\n",
    "GROUP BY r.name, r.owner\n",
    "\"\"\"\n",
    "\n",
    "output_repo_topics, df_repo_topics = execute_select_query(cursor, query)\n",
    "df_repo_topics.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commit messages\n",
    "query = \"SELECT * FROM Commits;\"\n",
    "output_commits, df_commits = execute_select_query(cursor, query)\n",
    "df_commits.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anàlisi quantitatiu\n",
    "A partir de les dades també ens interessa realitzar uns anàlisi quantitatiu:\n",
    "* Distribució dels temes principals (gràfic de barres o de pastís). És a dir, en quina proporció dels nostres repositoris estudiats es tracten.\n",
    "* Distribució de llenguatges (gràfic de barres o de pastís). Com l'anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RepositoryTopics is the N-N relationship table for Repository-Topic;\n",
    "# ie. the topics of each repository.\n",
    "output_repo_topics, df_repo_topics = execute_select_query(cursor, \"SELECT * FROM RepositoryTopics;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20 # We'll check the top 20 topics used\n",
    "topics = df_repo_topics[\"topic\"].value_counts().to_frame()\n",
    "topics_count = topics.sum()\n",
    "n_topics_count = topics[:n].sum()\n",
    "n_topics_name = df_repo_topics[\"topic\"].value_counts()[:n].index.to_list()\n",
    "print(\"Most used topics (present in {:.2f}% of repositories):\".format(float((n_topics_count / topics_count).iloc[0]) * 100))\n",
    "display(topics[:n])\n",
    "ax = topics[:n].plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el gràfic de barres anterior es mostra els primers 20 temes més freqüents tractats en els nostres repositoris. No obstant això, només estan presents en el 15% d'ells, la qual cosa ens mostra que la nostra mostra està àmpliament distribuïda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analitzem a continuació els llenguatges més usats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df_repo.drop(df_repo[df_repo[\"mainLanguage\"] == \"\"].index) # Ignore repositories where the main language is unknown\n",
    "languages = tmp[\"mainLanguage\"].value_counts().to_frame()\n",
    "display(languages)\n",
    "languages_count = languages.sum()\n",
    "n_languages_count = languages[:n].sum()\n",
    "print(\"Main topics present in {:.2f}% of repositories.\".format(float((n_languages_count / languages_count).iloc[0]) * 100))\n",
    "print(languages.shape)\n",
    "display(languages)\n",
    "languages[:n].plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Els llenguatges més utilitzats en la nostra mostra de repositoris són llenguatges orientats al desenvolupament web; aquest resultat és coherent amb el fet que topics/tags per a frameworks de desenvolupament web com React són els més usats. Seguits de Python, Go, Java i C++. Aquests llenguatges més utilitzats representen més del 90% dels repositoris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO a similar chart for TopicVisits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducció dimensional per a una visió general dels repositoris\n",
    "\n",
    "Una pregunta que ens interessava era si el llenguatge dels repositoris té correlació amb les seves mètriques (nombre de commits, stars, followers).\n",
    "\n",
    "Per contestar aquesta pregunta podem realitzar un Principal Component Analysis (PCA) per tenir una idea de l'estructura de tot el conjunt de dades. Després, coloraríem els punts de dades en funció del llenguatge utilitzat, per exemple, per veure si aquests grups tenen característiques similars i es troben propers en el dataframe o no.\n",
    "\n",
    "* Crear un dataframe que conté totes les dades de RepositoryVisits amb les dades més actualitzades per a tots els repositoris estudiats.\n",
    "* Realitzar una estandardització de les dades (és a dir, per evitar que algunes variables com els commits siguin molt més importants que altres com els forks).\n",
    "* Realitzar una PCA en 2 components sobre aquest.\n",
    "* Representar els resultats agrupant els punts en funció de diferents criteris:\n",
    "    + mainLanguage\n",
    "    + topic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estructura general de les dades numèriques dels repositoris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the numerical fields of RepositoryVisits that we'll use for the PCA\n",
    "query = \"\"\"\n",
    "SELECT CONCAT(RepositoryVisits.owner, RepositoryVisits.name) AS id, forks, commits, stars, watchers, contributors, openIssues, closedIssues, mainLanguage\n",
    "FROM RepositoryVisits\n",
    "JOIN Repositories ON RepositoryVisits.owner = Repositories.owner AND RepositoryVisits.name = Repositories.name\n",
    "WHERE CAST(date AS Date) = '2024-04-16'\n",
    "\"\"\"\n",
    "\n",
    "_, df = execute_select_query(cursor, query)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the most used languages\n",
    "top_languages = df_repo['mainLanguage'].value_counts()[:10].index.tolist()\n",
    "\n",
    "df = df[df['mainLanguage'].isin(top_languages)]\n",
    "\n",
    "# Remove outliers\n",
    "for column in df:\n",
    "    if (df[column].dtype) != \"O\":\n",
    "        q_low = df[column].quantile(0.05)\n",
    "        q_high = df[column].quantile(0.95)\n",
    "        # print(q_low)\n",
    "        # print(q_high)\n",
    "        df = df[(df[column] <= q_high) & (df[column] >= q_low)]\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[\"id\"]\n",
    "print(labels[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split numerical data.\n",
    "X = df.iloc[:,1:-1].values\n",
    "print(X[:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarize each attribute\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "print(X_std[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_std)\n",
    "print(pca.components_)\n",
    "print(sum(pca.explained_variance_ratio_))\n",
    "print(X_pca[:5])\n",
    "print(max(X_std[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_languages = df_repo['mainLanguage'].value_counts()[:10].index.tolist()\n",
    "cmap = plt.get_cmap('viridis')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, 10)]\n",
    "#color_map = dict(zip(l, colors))\n",
    "\n",
    "# Colors to use for each language in the scatterplot\n",
    "color_map = {\n",
    " 'C': (0,0.9,1,1),\n",
    " 'C#': (0.79,0,1,1),\n",
    " 'C++': (0,0.4,1,1),\n",
    " 'Go': (1,0.6,0,1),\n",
    " 'HTML': (1,0.94,0,1),\n",
    " 'Java': (1,0,0,1),\n",
    " 'JavaScript': (1,1,0,1),\n",
    " 'Python': (0,1,0.2,1),\n",
    " 'Shell': (0.6,0.6,0.6,1),\n",
    " 'TypeScript': (0.9,1,0,1),\n",
    " 'PHP': (0.5,1,0,1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter = plt.scatter(X_pca[:,0], X_pca[:,1], c = df[\"mainLanguage\"].map(color_map), alpha=0.5)\n",
    "\n",
    "legend_handles = [mpatches.Patch(color=color_map[language], label=language)\n",
    " for language in top_languages]\n",
    "plt.legend(handles=legend_handles, title='Languages')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repositoris \"Open Source\" & dades per a la pàgina web\n",
    "Quant a l'objectiu de destacar repositoris open-source (i no només repositoris populars, com fa GitHub), volem exportar dades d'aquests repositoris en format `.json` per a que puguin ser usades per la pàgina web.\n",
    "\n",
    "Considerem que repositoris són projectes open-source si tenen més de 5 contribuïdors i més de 50 issues en total - un criteri simple però efectiu.\n",
    "\n",
    "Recollir les dades necessàries involucra fer JOINs amb múltiples taules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "open_source_projects_query = \"\"\"\n",
    "SELECT r.owner, r.name, MAX(r.description) as description, r.mainLanguage,\n",
    "MAX(stars) as total_stars, MAX(contributors) as total_contributors, MAX(openIssues + closedIssues) as total_issues,\n",
    "MAX(o.avatar_url) as avatar_url, GROUP_CONCAT(DISTINCT t.topic) as topics,\n",
    "MAX(watchers) as total_watchers FROM Repositories r\n",
    "-- Join with RepositoryVisits to get metrics like contributors, stars, issues amount\n",
    "JOIN RepositoryVisits v\n",
    "ON r.owner = v.owner AND r.name = v.name\n",
    "-- Join with Owners to get the avatar URL\n",
    "JOIN Owners o\n",
    "ON r.owner = o.username\n",
    "-- Join with RepositoryTopics to get a list of the tags/topics of the repos\n",
    "JOIN RepositoryTopics t\n",
    "ON t.repo = r.name AND t.owner = r.owner\n",
    "GROUP BY r.owner, r.name, r.mainLanguage, r.license -- Must group by mainLanguage and license as well as they're non-aggregated\n",
    "HAVING total_contributors > 5 AND total_issues > 50\n",
    "ORDER BY total_contributors DESC; -- Show repos with most contributors first\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "output_repo, df_repo = execute_select_query(cursor, open_source_projects_query)\n",
    "print(len(df_repo), \"repositories matching criteria\")\n",
    "df_repo.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les dades d'aquests repositoris s'exporten a .json perque puguin ser usades per la pàgina web.\n",
    "\n",
    "En aquest procés també categoritzem els repositoris segons les seves temàtiques generals:\n",
    "- Desenvolupament web\n",
    "- Data science\n",
    "- Aplicacions\n",
    "- Eines de desenvolupament\n",
    "- Repositoris de recursos (ex. una col·lecció d'algorismes)\n",
    "\n",
    "La categorització es fa segons els \"topics\" (tags) que tenen els repositoris. Els repositoris també es categoritzen pel seu llenguatge de programació principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "used_languages = set()\n",
    "used_tags = set()\n",
    "\n",
    "# Renames or discards languages.\n",
    "# Used to group up frameworks, variants and transpilers.\n",
    "LANGUAGE_REMAP = {\n",
    "    \"TypeScript\": \"JavaScript\",\n",
    "    \"Vue\": \"JavaScript\",\n",
    "    \"HTML\": \"JavaScript\", # Bruh\n",
    "    \"Jupyter Notebook\": \"Python\",\n",
    "    \"Kotlin\": \"Java & Kotlin\",\n",
    "    \"Java\": \"Java & Kotlin\",\n",
    "    \"C\": \"C/C++\",\n",
    "    \"C++\": \"C/C++\",\n",
    "    \"Ruby\": \"Others\",\n",
    "    \"Go\": \"Others\",\n",
    "    \"Swift\": \"Others\",\n",
    "    \"Clojure\": \"Others\",\n",
    "    \"Haskell\": \"Others\",\n",
    "    \"Dart\": \"Others\",\n",
    "    \"Shell\": \"Others\",\n",
    "    \"PowerShell\": \"Others\",\n",
    "    \"Scala\": \"Others\",\n",
    "    \"Svelte\": \"Others\",\n",
    "    \"Vim Script\": \"\",\n",
    "    \"CSS\": \"\",\n",
    "    \"SCSS\": \"\",\n",
    "    \"MDX\": \"\",\n",
    "}\n",
    "\n",
    "# Remaps GitHub topics to the tags that are used in the website.\n",
    "TAG_MAP = {}\n",
    "# Maps tags used by the website to a list of Github \"topics\" that will be considered as the same tag.\n",
    "# Ex. repos with topics \"react\", \"vue\" become tagged as \"Web\"\n",
    "TAG_ALIASES = {\n",
    "    \"Web\": [\"react\", \"vue\", \"web\", \"reactjs\", \"css\", \"chrome-extension\", \"react-grid\", \"react-table\", \"php\", \"http\", \"nodejs\", \"typescript\", \"electron\", \"search-engine\", \"webgl\", \"rest\", \"rest-api\", \"swagger\", \"static-site-generator\", \"blog-engine\", \"router\", \"webview\", \"jquery\", \"http-client\", \"website\", \"reactive-templates\", \"nuxt\", \"nat\", \"javascript\", \"http2\", \"nginx\", \"apache\", \"aws\", \"api-gateway\", \"jekyll\", \"bootstrap\"],\n",
    "    \"Modding\": [\"mod\", \"minecraft\", \"emulation\", \"emulator\", \"forge\", \"minecraft-launcher\", \"modrinth\", \"minecraft-api\", \"minecraft-server\", \"bepinex\", \"unity3d\", \"unreal\", \"unity-mono\", \"craftbukkit\", \"valheim\", \"minecraft-mod\", \"gta5\", \"fabric\", \"gamedev\", \"retroarch\", \"game\"],\n",
    "    \"Data Science\": [\"math\", \"numpy\", \"data-science\", \"graphql\", \"data-visualization\", \"jupyter-notebook\", \"big-data\"],\n",
    "    \"Machine Learning\": [\"ml\", \"pytorch\", \"deep-learning\", \"machine-learning\", \"deep-neural-networks\", \"tensorflow\", \"neural-network\", \"tensor\", \"computer-vision\", \"reinforcement-learning\", \"hyperparameter-tuning\", \"ai\", \"artificial-intelligence\", \"llama\", \"llms\", \"llm\", \"openai\"],\n",
    "    \"Tool\": [\"containers\", \"zsh\", \"docker\", \"github\", \"cli\", \"searchengine\", \"postgrest\", \"devtool\", \"cloudstorage\", \"git\", \"npm\", \"database\", \"postgresql\", \"backend\", \"shell-scripting\", \"websocket\", \"collaboration\", \"developer-tools\", \"promise\", \"api\", \"testing\", \"translation\", \"i18n\", \"language\", \"golang-library\", \"algorithm\", \"firmware\", \"style-linter\", \"linting\", \"converter\", \"blockchain\", \"wordpress\", \"static-site-generator\", \"blog-engine\", \"material\", \"material-design\", \"framework\", \"argument-parser\", \"command-line-parser\", \"readme-generator\", \"ssh\", \"backup\", \"reverse-engineering\", \"animation\", \"sdk\", \"devops\", \"jenkins\", \"documentation\", \"terminal\", \"encryption\", \"scrapers\", \"3d-printing\", \"reactive-templates\", \"image-optimization\", \"file-server\", \"nat\", \"proxy\", \"shell\", \"linters\", \"git-client\", \"raspberry-pi\", \"blogging\", \"npm-cli\", \"aws\", \"api-gateway\", \"decompiler\", \"kubernetes\", \"tools\"],\n",
    "    \"App\": [\"note-taking\", \"productivity\", \"prest\", \"download\", \"latex\", \"text-editor\", \"curl\", \"ftp\", \"bot\", \"synchronization\", \"sqlite\", \"mattermost\", \"messaging\", \"conferencing\", \"remote-desktop\", \"emacs\", \"color-picker\", \"cli-app\", \"subtitle-downloader\", \"decompiler\", \"mobile-app\"],\n",
    "\n",
    "    \"Resource\": [\"learn-to-code\", \"freecodecamp\", \"curriculum\", \"certification\", \"learnopengl\", \"lists\", \"resources\", \"resource\", \"dataset\", \"public-api\", \"public-apis\", \"practice\", \"interview\", \"styleguide\", \"list\", \"interview-questions\", \"awesome-list\", \"principles\", \"design-patterns\"],\n",
    "}\n",
    "# Tags manually added to some repositories (which otherwise lack descriptive ones)\n",
    "MANUAL_TAGS = {\n",
    "    \"minio/minio\": [\"Machine Learning\"],\n",
    "    \"Aliucord/Aliucord\": [\"Modding\"],\n",
    "    \"cli-guidelines/cli-guidelines\": [\"Resource\"],\n",
    "    \"yjs/yjs\": [\"Tool\"],\n",
    "    \"TigerVNC/tigervnc\": [\"Tool\"],\n",
    "    \"ollama/ollama\": [\"App\"],\n",
    "    \"micropython/micropython\": [\"Tool\"], # Python implementation\n",
    "    \"raspberrypi/linux\": [\"App\"],\n",
    "    \"rust-lang/rust\": [\"Tool\"],\n",
    "    \"home-assistant/core\": [\"Tool\"],\n",
    "    \"vuejs/vue-cli\": [\"Tool\", \"Web\"],\n",
    "    \"remix-run/remix\": [\"Tool\", \"Web\"],\n",
    "}\n",
    "# Same thing as above, but mapping tag to list of repos with it instead, as I realized at the end this would've been more convenient.\n",
    "MANUAL_TAGS2 = {\n",
    "    \"Tool\": [\"pytorch/tutorials\", \"vuejs/core\", \"google/googletest\", \"google/guava\", \"ReactiveX/RxJava\"],\n",
    "    \"Web\": [\"vuejs/core\"],\n",
    "    \"App\": [\"square/retrofit\"],\n",
    "}\n",
    "for tag,aliases in TAG_ALIASES.items():\n",
    "    for alias in aliases:\n",
    "        TAG_MAP[alias] = tag\n",
    "for tag,repos in MANUAL_TAGS2.items():\n",
    "    for repo in repos:\n",
    "        if repo in MANUAL_TAGS:\n",
    "            MANUAL_TAGS[repo].append(tag)\n",
    "        else:\n",
    "            MANUAL_TAGS[repo] = [tag]\n",
    "\n",
    "IGNORED_TAGS = defaultdict(int)\n",
    "\n",
    "oss_repos = {}\n",
    "for index, row in df_repo.iterrows():\n",
    "    key = row[\"owner\"] + \"/\" + row[\"name\"]\n",
    "    main_language = row[\"mainLanguage\"]\n",
    "    if main_language in LANGUAGE_REMAP:\n",
    "        main_language = LANGUAGE_REMAP[main_language]\n",
    "    repo = {\n",
    "        \"topics\": set(),\n",
    "        \"languages\": set([main_language] if main_language != \"\" else []),\n",
    "        \"stars\": row[\"total_stars\"],\n",
    "        \"contributors\": row[\"total_contributors\"],\n",
    "        \"icon\": row[\"avatar_url\"],\n",
    "        \"description\": row[\"description\"],\n",
    "    }\n",
    "    for topic in str.split(row[\"topics\"], \",\"):\n",
    "        if topic in TAG_MAP:\n",
    "            repo[\"topics\"].add(TAG_MAP[topic])\n",
    "        else:\n",
    "            IGNORED_TAGS[tag] += 1\n",
    "        if key in MANUAL_TAGS:\n",
    "            for tag in MANUAL_TAGS[key]:\n",
    "                repo[\"topics\"].add(tag)\n",
    "\n",
    "    oss_repos[key] = repo\n",
    "    used_tags = used_tags.union(repo[\"topics\"])\n",
    "    used_languages.add(main_language)\n",
    "\n",
    "# Exclude mirrors and other projects that are not contributable projects or unsuitable\n",
    "BLACKLISTED_REPOS = [\n",
    "    \"gitlabhq/gitlabhq\", # Read-only mirror.\n",
    "    \"qemu/qemu\", # Read-only mirror.\n",
    "    \"xasset/xasset\",# Not english.\n",
    "    \"jynew/jynew\", # Unity RPG game framework, documentation in chinese-only though.\n",
    "    \"doocs/advanced-java\", # Chinese-only Java interview questions.\n",
    "    \"CyC2018/CS-Notes\", # Chinese computer science course resources.\n",
    "    \"apache/kafka\", # Read-only mirror.\n",
    "]\n",
    "for blacklisted_repo in BLACKLISTED_REPOS:\n",
    "    del oss_repos[blacklisted_repo]\n",
    "\n",
    "# Convert sets to lists for json serialization, and add other\n",
    "# keys the site expects\n",
    "for key,repo in oss_repos.items():\n",
    "    repo[\"owner\"] = key.split(\"/\")[0]\n",
    "    repo[\"repo\"] = key.split(\"/\")[1]\n",
    "    repo[\"topics\"] = list(repo[\"topics\"])\n",
    "    repo[\"languages\"] = list(repo[\"languages\"])\n",
    "\n",
    "# Export the .json\n",
    "with open(\"repositories_output.json\", \"w\") as f:\n",
    "    json.dump(oss_repos, f, indent=2)\n",
    "\n",
    "print(\"Valid repositories:\", len(oss_repos))\n",
    "print(\"Languages used:\", used_languages)\n",
    "print(\"Tags used:\", used_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com els repositoris en la base de dades foren principalment trobades per scraping de les pàgines \"trending\" de GitHub, podem concloure que un 50% dels repositoris que GitHub destaca són només repositoris populars de projectes individuals o d'equips petits i no projectes contribuïbles; només uns 1000 repositoris dels 2000 en la base de dades compleixen els requisits que hem imposat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis de missatges de commits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En aquesta secció, farem una anàlisi dels missatges que fan els usuaris a l’hora de fer commits en github.\n",
    "Veurem quines paraules claus usen més i quin és l’estil més usat amb l’objectiu de determinar una bona\n",
    "forma d’escriure un missatge de commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import warnings\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "\"\"\"\n",
    "If \n",
    "!pip install wordcloud \n",
    "doesn't work:\n",
    "from os import path\n",
    "import sys\n",
    "print(sys.executable) # use the path\n",
    "\n",
    "path -m pip install wordcloud\n",
    "\"\"\"\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define constants and configurations\n",
    "STANDARDIZE_SUBSTITUTION = re.compile(r\"[!\\\"$#%&()*+,\\-./:;<=>?[\\]^_`{|}~]\")\n",
    "CONSECUTIVE_SPACE_SUBSTITUTION = re.compile(r\"  +\")\n",
    "LEMMATIZATION_BLACKLIST = {\"was\", \"as\"}\n",
    "WORD_REPLACEMENTS = {\n",
    "    \"read-me\": \"readme\",\n",
    "    \"read.me\": \"readme\",\n",
    "    \"readme.md\": \"readme\"\n",
    "}\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "# stopwords.update([\"a\"]) # Manually add stopwords if needed\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcions auxiliars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accents(input_str):\n",
    "    \"\"\"\n",
    "    Convert accented characters to base characters.\n",
    "    Source: https://stackoverflow.com/a/1207479\n",
    "    \"\"\"\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str).encode(\"ascii\", \"ignore\")\n",
    "    return bytes.decode(nfkd_form)\n",
    "\n",
    "lemmatizer = nltk.wordnet.WordNetLemmatizer()\n",
    "\n",
    "# Function to get the wordnet POS tag, where POS means Part of Speech, which is basically the grammatical category of a word\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def standardize(word):\n",
    "    \"\"\"Standardize a word.\"\"\"\n",
    "    word = word.lower()\n",
    "    word = re.sub(STANDARDIZE_SUBSTITUTION, \" \", word)\n",
    "    word = re.sub(CONSECUTIVE_SPACE_SUBSTITUTION, \" \", word)\n",
    "    word = remove_accents(word)\n",
    "    word = word.replace(\"'s\", \"\").replace(\"#\", \"\").strip()\n",
    "    words = [WORD_REPLACEMENTS.get(w, w) for w in word.split()]\n",
    "    \n",
    "    # Enhanced lemmatization with POS tagging\n",
    "    words = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) if w not in LEMMATIZATION_BLACKLIST else w for w in words]\n",
    "    \n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(standardize(\"thIs a TesT\"))\n",
    "print(standardize(\"readme.md\"))\n",
    "_standardize_test_words = [\n",
    "    \"Testing @here as#Dasd 333232 tests testings gItHUbs testing added\",  # tests -> test due lemmatization\n",
    "    \"Test_ing !!, (asd,as d).\",  # Underscore are considered separated words\n",
    "    \"Test łłłł ñaññañ áaaa\",  # Removal of accents and non standard characters\n",
    "    \"テスト #asdasd arabian, wolves\",\n",
    "]\n",
    "\n",
    "for word in _standardize_test_words:\n",
    "    print(standardize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neteja de dades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com que l'objectiu es analitzar com escriuen les persones, hi ha un conjunt de missatges que no ens interessen. Per això cal excloure tots aquells missatges que no compleixin uns requisits.\n",
    "\n",
    "Els nostres criteris d'exclusió són:\n",
    "- Missatges de bots, els quals podem identificar per un autor que tingui en el nom `[bot]` i missatges que tinguin noms com pot ser: `dependabot` o `renovatebot`\n",
    "- Eliminar files repetides, producte d'error en la recolecció de dades.\n",
    "- Els missatges automatics que genera github com poden ser: `Merge pull request`, `Merge branch`, `Squash`, `Initial commit` i `Revert`, ja que no són escrits per les persones.\n",
    "\n",
    "A partir d'aquests criteris, hem realitzat una neteja de les dades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the commits authored by a bot.\n",
    "from IPython.display import display\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "cond_a = df_commits[df_commits[\"author\"].str.contains(\"\\[bot]|-bot\")]\n",
    "cond_c = df_commits[df_commits[\"message\"].str.contains(\"dependabot\")]\n",
    "cond_d = df_commits[df_commits[\"message\"].str.contains(\"renovatebot\")]\n",
    "\n",
    "temp = pd.concat([cond_a])\n",
    "\n",
    "unique_auth = pd.unique(temp.author)\n",
    "unique_msg = pd.unique(cond_c.message)\n",
    "\n",
    "dict = {'bots' : unique_auth}\n",
    "df_bots = pd.DataFrame(dict)\n",
    "\n",
    "# displaying the bots names\n",
    "display(df_bots)\n",
    "\n",
    "dict = {'bots_message' : unique_msg}\n",
    "df_bots_msg = pd.DataFrame(dict)\n",
    "\n",
    "display(df_bots_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the bots\n",
    "print(\"With bots\", len(df_commits))\n",
    "df_commits = df_commits[~df_commits[\"author\"].isin(df_bots[\"bots\"])]\n",
    "print(\"Without bots\", len(df_commits))\n",
    "\n",
    "# Filter out repeated rows\n",
    "df_commits.drop_duplicates(inplace=True)\n",
    "\n",
    "# Filter out the automatic messages\n",
    "df_commits = df_commits[\n",
    "    ~(df_commits.message.str.startswith(\"Merge pull request\") |\n",
    "      df_commits.message.str.startswith(\"Merge branch\") |\n",
    "      df_commits.message.str.startswith(\"Merge remote\") |\n",
    "      df_commits.message.str.startswith(\"Merge commit\") |\n",
    "      df_commits.message.str.startswith(\"Merge tag\") |\n",
    "      df_commits.message.str.startswith(\"Squash\") |\n",
    "      df_commits.message.str.startswith(\"Initial commit\") |\n",
    "      df_commits.message.str.startswith(\"Revert \") |\n",
    "      df_commits.message.str.startswith(\"Add files via upload\") |\n",
    "      df_commits.message.str.startswith(\"🔄\") |\n",
    "      df_commits.message.str.startswith(\"[auto]\") |\n",
    "      df_commits.message.str.startswith(\"[maven-release-plugin]\")\n",
    "     )\n",
    "]\n",
    "print(\"Without automatic messages\", len(df_commits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui podem veure que de 94.872 missatges, hem reduit els missatges usables a 77.729 . Tot i que segur que queden alguns altres missatges automàtics que no hem pogut detectar a temps, però hem cobert la majoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis paraules més usades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per fer l'anàlisis de les paraules més usades, ho farem de dues formes. \n",
    "- Usarem wordcloud, el qual és una representacó gràfica d'un dibuix creat per paraules on segons les ocurrències d'aquestes, són més grans o si apareixen poc, més petites. Obtenint així, una visió ràpida, de quines paraules són les més usades.\n",
    "- Comptarem les paraules i mostrarem un gràfic dels top n paraules més usats en ordre descendent, segons paraula més usada en missatges individuals o ocurrències totals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionalment, tenim `obtain_mask()` el qual usant `transform_format()` converteix una imatge blanc i negra en una màscara que podem usar en el wordcloud perquè només apareguin lletres dins de la regió negra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_cloud(word_cloud, text, save_image=False, image_name=\"none\"):\n",
    "    \"\"\"Create and display a word cloud image.\"\"\"\n",
    "    word_cloud_output = word_cloud.generate(text)\n",
    "    plt.imshow(word_cloud_output, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    if save_image:\n",
    "        word_cloud_output.to_file(f\"./Images/wordclouds/{image_name}_word_cloud.png\")\n",
    "\n",
    "def transform_format(val):\n",
    "    \"\"\"Transform format for mask creation.\"\"\"\n",
    "    return 255 if val == 0 else val\n",
    "\n",
    "def obtain_mask(mask):\n",
    "    \"\"\"Obtain mask for word cloud.\"\"\"\n",
    "    trans_mask = np.ndarray((mask.shape[0], mask.shape[1]), np.int32)\n",
    "    for i in range(len(mask)):\n",
    "        trans_mask[i] = list(map(transform_format, mask[i]))\n",
    "    return trans_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all text\n",
    "all_text = \" \".join(commit for commit in df_commits.message)\n",
    "# Set the wordcloud params\n",
    "word_cloud = WordCloud(stopwords=stopwords, max_font_size=30, max_words=200, background_color=\"black\")\n",
    "# Plot the wordcloud\n",
    "plot_word_cloud(word_cloud, all_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source of the GitHub image: [Icon by Dryicons](https://dryicons.com/icon/square-github-icon-8312)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a mask and a color map\n",
    "github_image = np.array(Image.open(\"./Images/wordclouds/github_square.png\"))\n",
    "\n",
    "word_cloud_mask = obtain_mask(github_image)\n",
    "word_cloud = WordCloud(stopwords=stopwords, colormap='rainbow', mask=word_cloud_mask, max_font_size=30, max_words=10000, background_color=\"black\")\n",
    "plot_word_cloud(word_cloud, all_text, True, \"git_mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a mask        \n",
    "git_image = np.array(Image.open(\"./Images/wordclouds/git.png\"))\n",
    "\n",
    "word_cloud_mask = obtain_mask(git_image)\n",
    "word_cloud = WordCloud(stopwords=stopwords, mask=word_cloud_mask,colormap='hot', max_font_size=20, max_words=10000, background_color=\"#474747\")\n",
    "plot_word_cloud(word_cloud, all_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podem veure que les paraules més usades, són: `fix, add, update` sense comptar els stopwords, ja que podem veure que són les paraules més grans. Tot seguit veurem els valors exactes, de les paraules més usades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def count_words(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    :param df: DataFrame with the messages and associated information\n",
    "    :return: Dictionary with the format {word: {n_ocur: value, n_messages: value}, ...}\n",
    "    \"\"\"\n",
    "    # Using defaultdict for convenience; we won't have to add keys explicitly\n",
    "    dicc = defaultdict(lambda: {\"n_ocur\": 0, \"n_messages\": 0})\n",
    "\n",
    "    total_rows = len(df)\n",
    "    processed_rows_count = 0\n",
    "    for row in df.iterrows():\n",
    "        text = row[1][0]\n",
    "        text = standardize(text)  # Apply standardization\n",
    "        \n",
    "        # Split the text, remove digits and skip if empty\n",
    "        words = text.split(\" \")\n",
    "        words_without_digits = [word for word in words if not re.search(r'\\d', word)]\n",
    "        unique_words = set(words_without_digits)\n",
    "        \n",
    "        if len(unique_words) == 1 and '' in unique_words:\n",
    "            continue\n",
    "            \n",
    "        # Times that a word appears and times that appears in different messages.\n",
    "        for word in words_without_digits:\n",
    "            dicc[word][\"n_ocur\"] += 1\n",
    "        for word in unique_words:\n",
    "            dicc[word][\"n_messages\"] += 1\n",
    "\n",
    "        processed_rows_count += 1\n",
    "        # print(f\"Word {processed_rows_count}/{total_rows} done; {processed_rows_count / total_rows * 100:.2f}%\")\n",
    "    \n",
    "    return dicc\n",
    "\n",
    "frame = {'Messages': df_commits.message}\n",
    "\n",
    "result = pd.DataFrame(frame)\n",
    "\n",
    "ocurrencesDict = count_words(result)\n",
    "\n",
    "# Sort the words by occurrences and occurrences in messages.\n",
    "def obtain_top_n_words(dictionary, N, filter=\"n_ocur\", desc=True):\n",
    "    # Get all the words and their frequency\n",
    "    filtered_words = [(word, freq[filter]) for word, freq in dictionary.items() if word not in stopwords]\n",
    "    \n",
    "    # Sort from most to least frequent\n",
    "    sorted_words = sorted(filtered_words, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Take the N most frequent\n",
    "    return [(word, freq) for word, freq in sorted_words[:N]]\n",
    "\n",
    "top_words_list_ocur = obtain_top_n_words(ocurrencesDict, 30, \"n_ocur\", True)\n",
    "top_words_list_msg = obtain_top_n_words(ocurrencesDict, 30, \"n_messages\", True)\n",
    "\n",
    "print(\"Ocurrences Top\", top_words_list_ocur)\n",
    "print(\"\\nOcurrences per message Top\", top_words_list_msg)\n",
    "\n",
    "# Function to plot the top words\n",
    "def plot_top_words(topList, yLabel, ax):\n",
    "    x = [word for word, freq in topList]\n",
    "    y = [freq for word, freq in topList]\n",
    "    \n",
    "    # Making the bar chart on the data\n",
    "    ax.bar(x, y)\n",
    "    \n",
    "    # Giving title to the plot\n",
    "    ax.set_title(\"Top paraules més usades\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(x, rotation=90, ha='right')\n",
    "    # Giving X and Y labels\n",
    "    ax.set_xlabel(\"Paraules\")\n",
    "    ax.set_ylabel(yLabel)\n",
    "     \n",
    "    # We do not call plt.show() here, as we want to show both plots together\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "# Plot the top words by occurrences\n",
    "plot_top_words(top_words_list_ocur, \"Ocurrència de paraules\", axs[0])\n",
    "\n",
    "# Plot the top words by occurrences in different messages\n",
    "plot_top_words(top_words_list_msg, \"Ocurrencies de paraules per missatge\", axs[1])\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plot\")\n",
    "# Show the plots\n",
    "plt.show()\n",
    "\n",
    "# We can see that there is no significant difference, since normally, in a commit message, we do not repeat the same word twice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De les gràfiques, podem veure que en termes quantitatius, són casi idèntics, tenen la mateixa escala i en general, el mateix ordre de les paraules més usades, encara que la paraula fix, és el que té més diferència indicant, que les persones usen més d'un cop la paraula fix en un missatge. Tot i això denota que no se sol repetir les paraules en un mateix text, ja que totes les altres paraules, tenen un valor similar i la gràfica de l'esquerra, compta les paraules totals, mentre que el de la dreta, compta les paraules per missatge.\n",
    "\n",
    "Les tres paraules més usades, son `update, add, fix`, els quals eren els mateixos que hem vist en el wordcloud. Hem agrupat les paraules segons un possible significat semàntic.\n",
    "\n",
    "### Cicle de Desenvolupament:\n",
    "\n",
    "- **Add:** Adició de noves característiques i creixement del projecte.\n",
    "- **Fix:** Correció d'errors per assegurar l'estabilitat del projecte.\n",
    "- **Update:** Millores i actualitzacions del codi existent, assegurant que el projecte es mantingui actualitzat amb les millors pràctiques i tecnologies.\n",
    "\n",
    "### Paraules Complementàries:\n",
    "\n",
    "- **Remove:** Reflecteix l'eliminació de codi o funcionalitats obsoletes o innecessàries, la qual cosa és crucial per mantenir el codi net i eficient.\n",
    "- **Use:** Indica la implementació o reutilització de codi o biblioteques, suggerint un ús eficient dels recursos disponibles.\n",
    "- **Test:** Subratlla la importància de les proves en el cicle de desenvolupament per assegurar la funcionalitat correcta i evitar regressions.\n",
    "- **Readme:** Reflecteix la documentació del projecte, important per a la col·laboració i la claredat entre els membres de l'equip i els usuaris.\n",
    "- **Feat:** Similar a \"add\", mostra l'enfocament en noves funcionalitats.\n",
    "- **Chore:** Tasques de manteniment o administratives per a la neteja i organització del projecte.\n",
    "  \n",
    "### Paraules Clau en el Manteniment i Millora:\n",
    "\n",
    "- **Version:** Indica canvis en les versions del projecte, reflectint una evolució contínua i la gestió de versions.\n",
    "- **File:** Pot referir-se a la gestió de fitxers dins del projecte, com la creació, modificació o eliminació de fitxers.\n",
    "- **Refactor:** Subratlla l'esforç de reorganització del codi per millorar la seva estructura sense canviar-ne el comportament extern.\n",
    "- **Improve:** Reflecteix les millores contínues del codi o funcionalitats existents.\n",
    "\n",
    "### Altres Paraules Rellevants:\n",
    "\n",
    "- **Build, Link , Error:** Aquestes paraules indiquen activitats específiques relacionades amb la compilació del projecte, la gestió d'enllaços, i la correcció d'errors respectivament.\n",
    "- **CI:** Fa referència a la integració contínua, important per al desplegament automàtic i la verificació contínua de canvis.\n",
    "- **Config:** Reflecteix la configuració del projecte, que és crucial per a la seva correcta execució i desplegament\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grups de paraules més usades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En aquesta secció, usarem **ítems freqüents** per trobar patrons els quals els usuaris escriuen. La idea principal, es trobar grups de paraules els quals quan apareix un, apareix un altre. Això indicaria que les paraules estan relacionades o s'utilitzen conjuntament amb freqüència. Aquest procés ens ajuda a identificar associacions o patrons significatius en el text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/apriori/#apriori-frequent-itemsets-via-the-apriori-algorithm\n",
    "def generate_data_set(df: pd.DataFrame, rmStopWords = True, useStandard = True):\n",
    "    \"\"\"\n",
    "    :param df: DataFrame with the messages and associated information\n",
    "    :return dataSet: The message of the commit is the transaction and the words the items.\n",
    "    \"\"\"\n",
    "    data_set = []\n",
    "    total_rows = len(df)\n",
    "    processed_rows_count = 0\n",
    "    for row in df.iterrows():\n",
    "        text = row[1][0]\n",
    "        if (useStandard):\n",
    "            text = standardize(text)  # Apply standardization\n",
    "       \n",
    "        # Split the words, but don't hold digits.\n",
    "        words = [word for word in text.split() if not any(char.isdigit() for char in word)]\n",
    "        if (rmStopWords):\n",
    "            unique_words = list(set(words) - stopwords)\n",
    "        else:\n",
    "            unique_words = list(set(words))\n",
    "        data_set.append(unique_words)\n",
    "\n",
    "        processed_rows_count += 1\n",
    "        # print(f\"Word {processed_rows_count}/{total_rows} done; {processed_rows_count / total_rows * 100:.2f}%\")\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "import math\n",
    "#!pip install mlxtend\n",
    "\n",
    "def get_frequent_itemsets(data, min_supp = 0.01):\n",
    "    \"\"\"\n",
    "    To save memory, you may want to represent your transaction data in the sparse format. \n",
    "    This is especially useful if you have lots of products and small transactions.\n",
    "    \"\"\"\n",
    "    te = TransactionEncoder()\n",
    "    oht_ary = te.fit(data).transform(data, sparse=True)\n",
    "    sparse_df = pd.DataFrame.sparse.from_spmatrix(oht_ary, columns=te.columns_)\n",
    "    \n",
    "    # total message * support = number of messages which the bundle appears.\n",
    "    print(\"We consider that at least the bundle appears in:\", math.ceil(len(data) * min_supp), \" messages\")\n",
    "    \n",
    "    # Calculate it using apriori algorithm\n",
    "    frequent_itemsets = apriori(sparse_df, min_support=min_supp, use_colnames=True, verbose=1)\n",
    "    frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "    return frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data frame using the commits.\n",
    "frame = {'Messages': df_commits.message}\n",
    "result = pd.DataFrame(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the data set is too big, and our computers can't handle it, we are going to use a randomly sampled fraction of it.\n",
    "partial_data_set = generate_data_set(result.sample(frac =.50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets = get_frequent_itemsets(partial_data_set, 0.001)\n",
    "# Sort them from more length and then support\n",
    "frequent_itemsets_sorted = frequent_itemsets.sort_values(by=['length', 'support'], ascending=[False, False])\n",
    "print(frequent_itemsets_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_support = math.ceil(frequent_itemsets_sorted['support'].mean() * len(partial_data_set))\n",
    "print(\"Average amount of support: \", average_support, \" messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_group = frequent_itemsets[(frequent_itemsets['length'] >= 2)]\n",
    "data_group['itemsets'] = data_group['itemsets'].apply(lambda x: str(sorted(list(x))))\n",
    "data_group['support messages'] = data_group['support'].apply(lambda x: math.ceil(len(partial_data_set) * x))\n",
    "data_group = data_group.sort_values(by=['support messages'], ascending=False)\n",
    "# Show only those groups, who at least, have the average support.\n",
    "# With this, we can preserve significant data, while reducing the amount of itemsets\n",
    "data_group = data_group[(data_group['support messages'] >= average_support)]\n",
    "\n",
    "print(data_group[['itemsets', 'support messages']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to group itemsets by starting word\n",
    "def group_by_starting_word(df):\n",
    "    grouped_phrases = {}\n",
    "    for itemset in df['itemsets']:\n",
    "        cleaned_string = itemset.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")\n",
    "        items = cleaned_string.split(', ')\n",
    "        items_list = list(map(str.strip, items))  # This removes leading/trailing spaces\n",
    "        if items_list[0] in grouped_phrases:\n",
    "            grouped_phrases[items_list[0]].append(items_list[1])\n",
    "        else:\n",
    "            grouped_phrases[items_list[0]] = [items_list[1]]\n",
    "    return grouped_phrases\n",
    "\n",
    "# Group itemsets by starting word\n",
    "grouped_phrases = group_by_starting_word(data_group)\n",
    "print(grouped_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "# Show the \n",
    "def plot_graph(key, values):\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add main node (key)\n",
    "    G.add_node(key)\n",
    "\n",
    "    # Add edges between main node and each phrase (value)\n",
    "    for value in values:\n",
    "        G.add_edge(key, value)\n",
    "\n",
    "    # Plotting the graph\n",
    "    pos = nx.spring_layout(G, seed=42)  # positions for all nodes\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=2000, font_size=10, font_weight='bold', arrows=False)\n",
    "    plt.title(f'Undirected Graph for \"{key}\"')\n",
    "    plt.show()\n",
    "\n",
    "# Iterate through each key-value pair and plot the graph\n",
    "for key, values in grouped_phrases.items():\n",
    "    plot_graph(key, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Què passa si no normalitzem o comptem els stop words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_with_stop_words = generate_data_set(result.sample(frac =.30), rmStopWords = False, useStandard = True)\n",
    "frequent_itemsets = get_frequent_itemsets(data_set_with_stop_words, 0.001)\n",
    "# Sort them from more length and then support\n",
    "frequent_itemsets_sorted = frequent_itemsets.sort_values(by=['length', 'support'], ascending=[False, False])\n",
    "print(frequent_itemsets_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_group = frequent_itemsets[(frequent_itemsets['length'] >= 2)]\n",
    "data_group['itemsets'] = data_group['itemsets'].apply(lambda x: str(sorted(list(x))))\n",
    "data_group['support messages'] = data_group['support'].apply(lambda x: math.ceil(len(partial_data_set) * x))\n",
    "data_group = data_group.sort_values(by=['support messages'], ascending=False)\n",
    "\n",
    "print(data_group[['itemsets', 'support messages']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_phrases = group_by_starting_word(data_group)\n",
    "print(grouped_phrases)\n",
    "# Iterate through each key-value pair and plot the graph\n",
    "for key, values in grouped_phrases.items():\n",
    "    plot_graph(key, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_no_standard = generate_data_set(result.sample(frac =.5), rmStopWords = False, useStandard = False)\n",
    "frequent_itemsets = get_frequent_itemsets(data_set_no_standard, min_supp = 0.005) \n",
    "\n",
    "# Sort them from more length and then support\n",
    "data_group = frequent_itemsets[(frequent_itemsets['length'] >= 2)]\n",
    "data_group['itemsets'] = data_group['itemsets'].apply(lambda x: str(sorted(list(x))))\n",
    "data_group['support messages'] = data_group['support'].apply(lambda x: math.ceil(len(partial_data_set) * x))\n",
    "data_group = data_group.sort_values(by=['support messages'], ascending=False)\n",
    "\n",
    "print(data_group[['itemsets', 'support messages']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_phrases = group_by_starting_word(data_group)\n",
    "print(grouped_phrases)\n",
    "# Iterate through each key-value pair and plot the graph\n",
    "for key, values in grouped_phrases.items():\n",
    "    plot_graph(key, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per limitacions d'espai, i seguint el consell del professor, hem decidit utilitzar fragments parcials del conjunt de dades en lloc de calcular l'algoritme amb tot el data set.\n",
    "\n",
    "El resultat s'ha decidit mostrar usant graphs, ja que l'algoritme d'ítems freqüents concentra la major quantitat de grups en els paquets de 2 i és fàcil de veure com una paraula, es relaciona amb una altra usant aquesta.\n",
    "I el que hem obtingut, es el conjunt de paraules clau que solen apareixer junts. Per exemple, per fix, tenim:\n",
    "\n",
    "`'fix': ['in', 'to', 'for', 'on', 'the', 'of', 'issue', 'with']`\n",
    "\n",
    "El qual significa, que les persones solen escriure algo com:\n",
    "- \"fix xxxx to yyyyy\"\n",
    "- \"fix ssss with nnnn\"\n",
    "- \"fix issue eeee\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploració sintàctica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En aquesta secció, busquem identificar les categories gramaticals més comunes o dominants dels missatges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from collections import Counter\n",
    "\n",
    "# Uncomment to dowload the necessary data to run the functions.\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map to make the tags more reaedable //https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "tag_map_cat = {\n",
    "    'CC': 'Conjunció coordinativa',\n",
    "    'CD': 'Nombre cardinal',\n",
    "    'DT': 'Determinant',\n",
    "    'EX': 'Hi ha existencial',\n",
    "    'FW': 'Paraula estrangera',\n",
    "    'IN': 'Preposició o conjunció subordinant',\n",
    "    'JJ': 'Adjectiu',\n",
    "    'JJR': 'Adjectiu comparatiu',\n",
    "    'JJS': 'Adjectiu superlatiu',\n",
    "    'LS': 'Marcador ítems de llista ',\n",
    "    'MD': 'Modal',\n",
    "    'NN': 'Nom, singular o massa',\n",
    "    'NNS': 'Nom, plural',\n",
    "    'NNP': 'Nom propi, singular',\n",
    "    'NNPS': 'Nom propi, plural',\n",
    "    'PDT': 'Predeterminant',\n",
    "    'POS': 'Sufix possessiu',\n",
    "    'PRP': 'Pronom personal',\n",
    "    'PRP$': 'Pronom possessiu',\n",
    "    'RB': 'Adverbi',\n",
    "    'RBR': 'Adverbi comparatiu',\n",
    "    'RBS': 'Adverbi superlatiu',\n",
    "    'RP': 'Partícula',\n",
    "    'SYM': 'Símbol',\n",
    "    'TO': 'a',\n",
    "    'UH': 'Interjecció',\n",
    "    'VB': 'Verb, forma base',\n",
    "    'VBD': 'Verb, pretèrit',\n",
    "    'VBG': 'Verb, gerundi o participi present',\n",
    "    'VBN': 'Verb, participi passat',\n",
    "    'VBP': 'Verb, present no tercera persona singular',\n",
    "    'VBZ': 'Verb, present tercera persona singular',\n",
    "    'WDT': 'Determinant WH',\n",
    "    'WP': 'Pronom WH',\n",
    "    'WP$': 'Pronom possessiu WH',\n",
    "    'WRB': 'Adverbi WH'\n",
    "}\n",
    "# English version\n",
    "tag_map_en = {\n",
    "    'CC': 'Coordinating conjunction',\n",
    "    'CD': 'Cardinal number',\n",
    "    'DT': 'Determiner',\n",
    "    'EX': 'Existential there',\n",
    "    'FW': 'Foreign word',\n",
    "    'IN': 'Preposition or subordinating conjunction',\n",
    "    'JJ': 'Adjective',\n",
    "    'JJR': 'Adjective, comparative',\n",
    "    'JJS': 'Adjective, superlative',\n",
    "    'LS': 'List item marker',\n",
    "    'MD': 'Modal',\n",
    "    'NN': 'Noun, singular or mass',\n",
    "    'NNS': 'Noun, plural',\n",
    "    'NNP': 'Proper noun, singular',\n",
    "    'NNPS': 'Proper noun, plural',\n",
    "    'PDT': 'Predeterminer',\n",
    "    'POS': 'Possessive ending',\n",
    "    'PRP': 'Personal pronoun',\n",
    "    'PRP$': 'Possessive pronoun',\n",
    "    'RB': 'Adverb',\n",
    "    'RBR': 'Adverb, comparative',\n",
    "    'RBS': 'Adverb, superlative',\n",
    "    'RP': 'Particle',\n",
    "    'SYM': 'Symbol',\n",
    "    'TO': 'to',\n",
    "    'UH': 'Interjection',\n",
    "    'VB': 'Verb, base form',\n",
    "    'VBD': 'Verb, past tense',\n",
    "    'VBG': 'Verb, gerund or present participle',\n",
    "    'VBN': 'Verb, past participle',\n",
    "    'VBP': 'Verb, non-3rd person singular present',\n",
    "    'VBZ': 'Verb, 3rd person singular present',\n",
    "    'WDT': 'Wh-determiner',\n",
    "    'WP': 'Wh-pronoun',\n",
    "    'WP$': 'Possessive wh-pronoun',\n",
    "    'WRB': 'Wh-adverb'\n",
    "}\n",
    "\n",
    "# Load all text\n",
    "all_text = \" \".join(commit for commit in df_commits.message)\n",
    "print(\"Totes les paraules:\", len(all_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ratios(all_text, tag_map):\n",
    "    # apply tokenization to the text and tag it\n",
    "    tokens = word_tokenize(all_text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    \n",
    "    # Count the occurrence of each tag\n",
    "    tag_counts = Counter(tag for word, tag in tagged)\n",
    "    \n",
    "    # Convert tags to full wording in Catalan\n",
    "    english_tag_counts = {tag_map.get(tag, tag): count for tag, count in tag_counts.items()}\n",
    "    \n",
    "    # Calculate the ratio of each tag\n",
    "    total_tags = sum(tag_counts.values())\n",
    "    tag_ratios = {tag: count / total_tags for tag, count in english_tag_counts.items()}\n",
    "    \n",
    "    # Sort ratios by values in descending order\n",
    "    sorted_ratios = sorted(tag_ratios.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    # Create a dataframe to show the result\n",
    "    df_ratios = pd.DataFrame(sorted_ratios, columns=['Etiqueta', 'Ratio'])\n",
    "    \n",
    "    # Show the equivalence of ratio in words\n",
    "    data_size = len(all_text)\n",
    "    df_ratios['Paraules'] = df_ratios['Ratio'].apply(lambda x: math.ceil(x * data_size))\n",
    "    df_ratios['Ratio'] = df_ratios['Ratio'].apply(lambda x: (x * 100)) # Put it in percentage\n",
    "    \n",
    "    return df_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "titles_cat = ['Categoria gramatical o símbols', 'Percentatge (%)', 'Distribució ús de categories gramaticals']\n",
    "titles_en = ['Grammatical category or symbols', 'Percentage (%)', 'Distribution of use of grammatical categories']\n",
    "\n",
    "titles = titles_cat\n",
    "tag_map = tag_map_cat\n",
    "    \n",
    "df_ratios = calc_ratios(all_text, tag_map)\n",
    "print(df_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the ratios\n",
    "tags = df_ratios[\"Etiqueta\"]\n",
    "ratios = df_ratios[\"Ratio\"]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(tags, ratios, color='skyblue')\n",
    "plt.xlabel(titles[0])\n",
    "plt.ylabel(titles[1])\n",
    "plt.title(titles[2])\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A través dels resultats, podem destacar que hi ha una dominància dels noms: Les categories \"Nom, singular o massa\" i \"Nom propi, singular\" ocupen les dues primeres posicions en termes de freqüència relativa (ratio). Això podria indicar que solen descriure o senyalar molt a entitas concrets i per això potser tenim en tercera posició els adejctius. \n",
    "\n",
    "Per tant, es podria dir que els missatges són més aviat descriptives, amb l'objectiu de facilitar i saber a què fan referència. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I quines formes verbals s'usen més?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ratio of verbs\n",
    "data_size = len(all_text)\n",
    "verb_usage = df_ratios[df_ratios['Etiqueta'].str.startswith('Verb')].copy()\n",
    "verb_usage['Paraules'] = verb_usage['Ratio'].apply(lambda x: math.ceil(x * len(all_text)))\n",
    "print(verb_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_cat = ['Temps verbals dels verbs', 'Percentatge (%)', 'Distribució ús de categories gramaticals']\n",
    "titles_en = ['Verb tenses of verb', 'Percentage (%)', 'Verb usage distribution']\n",
    "\n",
    "titles = titles_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = verb_usage['Etiqueta']\n",
    "ratios = verb_usage['Ratio']\n",
    "\n",
    "print(\"Total percentage of verbs: {:.2f}%\".format(verb_usage['Ratio'].sum()))\n",
    "print(\"Total verbs:\", verb_usage['Paraules'].sum())\n",
    "\n",
    "# Plotting the ratios\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(tags, ratios, color='skyblue')\n",
    "plt.xlabel(titles[0])\n",
    "plt.ylabel(titles[1])\n",
    "plt.title(titles[2])\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per tant, com a resultat, podem concloure que els missatges estan enfocats a clarificar i identificar els canvis realitzats en el codi. Ja que els verbs, ocupen un ~8% i l'altre part, queda dominada sobretot per noms, adjectius i simbols que els quals tots tene un enfoc de clarificar i assenyalar. Aquest enfocament és crucial per a la comunicació eficient entre els membres de l'equip de desenvolupament i per a la comprensió precisa de les modificacions implementades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I si ho fem per missatges?\n",
    "Ara veurem, la mitjana, la desviació i la mediana d'un missatge en termes de paraules usades. I farem el comput de la mitja de percentatge de ús gramatical segons missatges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_words = df_commits['message'].apply(lambda x: len(x.split())).mean()\n",
    "std_dev_words = df_commits['message'].apply(lambda x: len(x.split())).std()\n",
    "median_words = df_commits['message'].apply(lambda x: len(x.split())).median()\n",
    "\n",
    "print(f\"The average message of a commit has {math.ceil(average_words)} words. With a standard deviation of {math.ceil(std_dev_words)} words and a median of {median_words}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ratios_message(df_commits, tag_map, average_words):\n",
    "    all_ratios = []\n",
    "\n",
    "    for message in df_commits.message:\n",
    "        # apply tokenization to the text and tag it\n",
    "        tokens = word_tokenize(message)\n",
    "        tagged = pos_tag(tokens)\n",
    "\n",
    "        # Count the occurrence of each tag\n",
    "        tag_counts = Counter(tag for word, tag in tagged)\n",
    "\n",
    "        # Convert tags to full wording in Catalan\n",
    "        english_tag_counts = {tag_map.get(tag, tag): count for tag, count in tag_counts.items()}\n",
    "\n",
    "        # Calculate the ratio of each tag\n",
    "        total_tags = sum(tag_counts.values())\n",
    "        tag_ratios = {tag: count / total_tags for tag, count in english_tag_counts.items()}\n",
    "\n",
    "        all_ratios.append(tag_ratios)\n",
    "\n",
    "    # Calculate the average ratios\n",
    "    avg_ratios = pd.DataFrame(all_ratios).mean().sort_values(ascending=False)\n",
    "\n",
    "    # Create a dataframe to show the result\n",
    "    df_ratios = pd.DataFrame(avg_ratios.reset_index())\n",
    "    df_ratios.columns = ['Etiqueta', 'Ratio']\n",
    "\n",
    "    # Show the equivalence of ratio in words\n",
    "    data_size = len(df_commits)\n",
    "    df_ratios['Paraules'] = df_ratios['Ratio'].apply(lambda x: math.ceil(x * average_words))\n",
    "    df_ratios['Ratio'] = df_ratios['Ratio'].apply(lambda x: (x * 100)) # Put it in percentage\n",
    "\n",
    "    return df_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_cat = ['Categoria gramatical o símbols', 'Percentatge (%)', 'Categoria gramatical mitjana d\\'un missatge en termes de paraules utilitzades']\n",
    "titles_en = ['Grammatical category or Symbols', 'Percentage (%)', 'Average grammatical category of a message in terms of words used']\n",
    "\n",
    "titles = titles_cat\n",
    "tag_map = tag_map_cat\n",
    "\n",
    "df_average_ratio_per_message = calc_ratios_message(df_commits, tag_map, average_words)\n",
    "print(df_average_ratio_per_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the ratios\n",
    "tags = df_average_ratio_per_message[\"Etiqueta\"]\n",
    "ratios = df_average_ratio_per_message[\"Ratio\"]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(tags, ratios, color='skyblue')\n",
    "plt.xlabel(titles[0])\n",
    "plt.ylabel(titles[1])\n",
    "plt.title(titles[2])\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_cat = ['Temps verbals dels verbs', 'Percentatge (%)', 'Distribució ús de verbs en un missatge segons temps verbal']\n",
    "titles_en = ['Verb tenses of verb', 'Percentage (%)', 'Verb usage distribution in a message based on verb tense']\n",
    "\n",
    "titles = titles_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ratio of verbs\n",
    "verb_usage = df_average_ratio_per_message[df_average_ratio_per_message['Etiqueta'].str.startswith('Verb')].copy()\n",
    "print(verb_usage)\n",
    "\n",
    "tags = verb_usage['Etiqueta']\n",
    "ratios = verb_usage['Ratio']\n",
    "\n",
    "# Plotting the ratios\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(tags, ratios, color='skyblue')\n",
    "plt.xlabel(titles[0])\n",
    "plt.ylabel(titles[1])\n",
    "plt.title(titles[2])\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podem veure que en un missatge, els noms, segueixen dominant la distribució de la categoria gramatical en un commit. Indicant, per exemple, que si tenim un missatge de 6 paraules, el ~37% de les paraules, són noms, singular o massa. A diferència de la distribució de totes les paraules, podem destacar, que els verbs, ara tenent més protagonisme, on els pretèrits i participi passat solen apareixer més. Això suggerix que els missatges de commit sovint descriuen accions que s'han completat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Altres analisis no-conclusius\n",
    "Per deficiències en la recollida de dades desafortunadament hi ha altres anàlisis que no hem pogut fer. A continuació mostrem com es farien si tinguéssim les dades necessàries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer d'aquests era visualitzar l'evolució de la quantitat de repositoris en el temps per cada topic. Desafortunadament hem perdut la majoria de les dades recollides; només les tenim per uns pocs dies, i per tant la gràfica no es gaire útil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, df_topic_visits = execute_select_query(cursor, \"SELECT CAST(date AS Date) AS date, name, repositories, followers FROM TopicVisits;\")\n",
    "df_topic_visits_n_cut = df_topic_visits[df_topic_visits[\"name\"].isin(n_topics_name)]\n",
    "df_topicv_repos = df_topic_visits_n_cut[[\"date\", \"name\", \"repositories\"]]\n",
    "n_topics_name\n",
    "display(df_topic_visits_n_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topicv_repos.pivot_table(index='date', columns=\"name\", values=\"repositories\", aggfunc=\"sum\").plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volem ara veure una evolució de la popularitat dels repositoris tractant els temes que hem identificat com a principals. És a dir, volem veure com ha anat canviant el nombre d'estrelles que reben aquests repositoris. Aquesta és una bona oportunitat que tenim per a treballar els joins entre taules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH RepositoriesMainTopics AS (\n",
    "\tSELECT repo AS name, topic\n",
    "    FROM RepositoryTopics\n",
    "    WHERE topic IN ('hacktoberfest', 'javascript', 'react', 'typescript', 'nodejs', 'python', 'docker', 'nextjs', 'golang', 'go', 'git', 'minecraft', 'java', 'vue', 'machine-learning', 'android', 'deep-learning', 'github', 'ios', 'cli')\n",
    "), TrendingTopic AS (\n",
    "\tSELECT date, repoName AS name, topic, starsToday as stars\n",
    "    FROM RepositoriesMainTopics\n",
    "    JOIN TrendVisits ON TrendVisits.repoName = RepositoriesMainTopics.name\n",
    ")\n",
    "SELECT CAST(date AS Date) AS date, topic, SUM(stars) AS stars\n",
    "FROM TrendingTopic\n",
    "GROUP BY date, topic\n",
    "\"\"\"\n",
    "_, df_trendRepositoriesMainTopics = execute_select_query(cursor, query)\n",
    "df_trendRepositoriesMainTopics.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trendRepositoriesMainTopics[\"stars\"] = df_trendRepositoriesMainTopics[\"stars\"].astype(float)\n",
    "df_trendRepositoriesMainTopics.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trendRepositoriesMainTopics.pivot_table(index='date', columns=\"topic\", values=\"stars\", aggfunc=\"sum\").plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closes the connection\n",
    "dataBaseConnection.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-python3-kernel",
   "language": "python",
   "name": "my-python3-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
