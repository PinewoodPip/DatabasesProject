{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projecte github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to install PyMySQL if necessary\n",
    "#!pip install PyMySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Draw the plots immediately after the current cell\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project configuration\n",
    "\n",
    "The file that we are going to use to set up the basic structure:\n",
    "\n",
    "GitHubScraper/JupyterNotebook/Structure/githubProjectStructure.sql\n",
    "\n",
    "from the github repository.\n",
    "\n",
    "### Step 1, import the structure\n",
    "In workbench: \n",
    "\"Server\" -> \"Data Import\", and then you should select the direction of \"githubProjectStructure.sql\" using the 3 dots and import from self-contained file like in the following image:\n",
    "\n",
    "![Javatpoint](./Images/Project_Setup/data_import.png)  \n",
    "\n",
    "Then scroll down, and create a new Schema by clicking on the \"New...\" button. Name it: \"githubProject\".\n",
    "\n",
    "![Javatpoint](./Images/Project_Setup/create_new_schema.png)  \n",
    "\n",
    "Scroll to the bottom, and in the bottom right, click on \"Start import\". If there are no errors, it will transitionate to \"Import progress\" and indicate success.\n",
    "\n",
    "Refresh the scehmas:\n",
    "\n",
    "![Javatpoint](./Images/Project_Setup/refresh.png)  \n",
    "\n",
    "### Step 2, importing the data from the csv files\n",
    "\n",
    "Download the csv files [here](https://drive.google.com/drive/folders/1NWhfFss0_M9V_clkcE9TH-Fy3oIEndWV?usp=sharing).\n",
    "\n",
    "Selecting the \"githubProject\" scheme, right click and \"Table Data Import Wizard\", with this, we are going to import each csv file to the matching table.\n",
    "\n",
    "You have to select the .csv you want to import, for example in this case, the trendVisits.csv\n",
    "\n",
    "![Javatpoint](./Images/Project_Setup/file_import.png) \n",
    "\n",
    "Then, since you already have the structure, you have to use the existing table that matches with the correct csv file:\n",
    "\n",
    "![Javatpoint](./Images/Project_Setup/use_existing.png) \n",
    "\n",
    "After that, check that the variables type are correct and import it. Do this for each csv file.\n",
    "\n",
    "You can also verify the data by doing this:\n",
    "\n",
    "![Javatpoint](./Images/Project_Setup/verify.png) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connexió\n",
    "\n",
    "Primer de tot, cal connectar-se a la base de dades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = \"githubProject\"\n",
    "db_host = \"localhost\"\n",
    "db_port = 3306\n",
    "db_username = \"root\"\n",
    "db_password = input(\"Enter the DB password\")\n",
    "\n",
    "dataBaseConnection = pymysql.connect(host=db_host,\n",
    "                            port=db_port,\n",
    "                            user=db_username,\n",
    "                            password=db_password,\n",
    "                            db=db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cursor to interact with the database\n",
    "cursor = dataBaseConnection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_select_query(cursor, query):\n",
    "    \"\"\"\n",
    "    Executes a query and returns the results as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        output = cursor.fetchall()\n",
    "        # Fetch column names from cursor's description\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "        \n",
    "        # Convert output to pandas DataFrame\n",
    "        if output:\n",
    "            df = pd.DataFrame(output, columns=columns)\n",
    "            print(\"Query executed successfully!\")\n",
    "        return output, df\n",
    "            \n",
    "    except pymysql.Error as e:\n",
    "        print(\"Error executing query:\")\n",
    "        warnings.warn(str(e))\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repositoris \"Open Source\" & dades per a la pàgina web\n",
    "Quant a l'objectiu de destacar repositoris open-source (i no només repositoris populars, com fa GitHub), volem exportar dades d'aquests repositoris en format `.json` per a que puguin ser usades per la pàgina web.\n",
    "\n",
    "Considerem que repositoris són projectes open-source si tenen més de 5 contribuïdors i més de 50 issues en total - un criteri simple però efectiu.\n",
    "\n",
    "Recollir les dades necessàries involucra fer JOINs amb múltiples taules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "open_source_projects_query = \"\"\"\n",
    "SELECT r.owner, r.name, MAX(r.description) as description, r.mainLanguage,\n",
    "MAX(stars) as total_stars, MAX(contributors) as total_contributors, MAX(openIssues + closedIssues) as total_issues,\n",
    "MAX(o.avatar_url) as avatar_url, GROUP_CONCAT(DISTINCT t.topic) as topics,\n",
    "MAX(watchers) as total_watchers FROM Repositories r\n",
    "-- Join with RepositoryVisits to get metrics like contributors, stars, issues amount\n",
    "JOIN RepositoryVisits v\n",
    "ON r.owner = v.owner AND r.name = v.name\n",
    "-- Join with Owners to get the avatar URL\n",
    "JOIN Owners o\n",
    "ON r.owner = o.username\n",
    "-- Join with RepositoryTopics to get a list of the tags/topics of the repos\n",
    "JOIN RepositoryTopics t\n",
    "ON t.repo = r.name AND t.owner = r.owner\n",
    "GROUP BY r.owner, r.name, r.mainLanguage, r.license -- Must group by mainLanguage and license as well as they're non-aggregated\n",
    "HAVING total_contributors > 5 AND total_issues > 50\n",
    "ORDER BY total_contributors DESC; -- Show repos with most contributors first\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "output_repo, df_repo = execute_select_query(cursor, open_source_projects_query)\n",
    "print(len(df_repo), \"repositories matching criteria\")\n",
    "df_repo.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les dades d'aquests repositoris s'exporten a .json perque puguin ser usades per la pàgina web.\n",
    "\n",
    "En aquest procés també categoritzem els repositoris segons les seves temàtiques generals:\n",
    "- Desenvolupament web\n",
    "- Data science\n",
    "- Aplicacions\n",
    "- Eines de desenvolupament\n",
    "- Repositoris de recursos (ex. una col·lecció d'algorismes)\n",
    "\n",
    "La categorització es fa segons els \"topics\" (tags) que tenen els repositoris. Els repositoris també es categoritzen pel seu llenguatge de programació principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "used_languages = set()\n",
    "used_tags = set()\n",
    "\n",
    "# Renames or discards languages.\n",
    "# Used to group up frameworks, variants and transpilers.\n",
    "LANGUAGE_REMAP = {\n",
    "    \"TypeScript\": \"JavaScript\",\n",
    "    \"Vue\": \"JavaScript\",\n",
    "    \"HTML\": \"JavaScript\", # Bruh\n",
    "    \"Jupyter Notebook\": \"Python\",\n",
    "    \"Kotlin\": \"Java & Kotlin\",\n",
    "    \"Java\": \"Java & Kotlin\",\n",
    "    \"C\": \"C/C++\",\n",
    "    \"C++\": \"C/C++\",\n",
    "    \"Ruby\": \"Others\",\n",
    "    \"Go\": \"Others\",\n",
    "    \"Swift\": \"Others\",\n",
    "    \"Clojure\": \"Others\",\n",
    "    \"Haskell\": \"Others\",\n",
    "    \"Dart\": \"Others\",\n",
    "    \"Shell\": \"Others\",\n",
    "    \"PowerShell\": \"Others\",\n",
    "    \"Scala\": \"Others\",\n",
    "    \"Svelte\": \"Others\",\n",
    "    \"Vim Script\": \"\",\n",
    "    \"CSS\": \"\",\n",
    "    \"SCSS\": \"\",\n",
    "    \"MDX\": \"\",\n",
    "}\n",
    "\n",
    "# Remaps GitHub topics to the tags that are used in the website.\n",
    "TAG_MAP = {}\n",
    "# Maps tags used by the website to a list of Github \"topics\" that will be considered as the same tag.\n",
    "# Ex. repos with topics \"react\", \"vue\" become tagged as \"Web\"\n",
    "TAG_ALIASES = {\n",
    "    \"Web\": [\"react\", \"vue\", \"web\", \"reactjs\", \"css\", \"chrome-extension\", \"react-grid\", \"react-table\", \"php\", \"http\", \"nodejs\", \"typescript\", \"electron\", \"search-engine\", \"webgl\", \"rest\", \"rest-api\", \"swagger\", \"static-site-generator\", \"blog-engine\", \"router\", \"webview\", \"jquery\", \"http-client\", \"website\", \"reactive-templates\", \"nuxt\", \"nat\", \"javascript\", \"http2\", \"nginx\", \"apache\", \"aws\", \"api-gateway\", \"jekyll\", \"bootstrap\"],\n",
    "    \"Modding\": [\"mod\", \"minecraft\", \"emulation\", \"emulator\", \"forge\", \"minecraft-launcher\", \"modrinth\", \"minecraft-api\", \"minecraft-server\", \"bepinex\", \"unity3d\", \"unreal\", \"unity-mono\", \"craftbukkit\", \"valheim\", \"minecraft-mod\", \"gta5\", \"fabric\", \"gamedev\", \"retroarch\", \"game\"],\n",
    "    \"Data Science\": [\"math\", \"numpy\", \"data-science\", \"graphql\", \"data-visualization\", \"jupyter-notebook\", \"big-data\"],\n",
    "    \"Machine Learning\": [\"ml\", \"pytorch\", \"deep-learning\", \"machine-learning\", \"deep-neural-networks\", \"tensorflow\", \"neural-network\", \"tensor\", \"computer-vision\", \"reinforcement-learning\", \"hyperparameter-tuning\", \"ai\", \"artificial-intelligence\", \"llama\", \"llms\", \"llm\", \"openai\"],\n",
    "    \"Tool\": [\"containers\", \"zsh\", \"docker\", \"github\", \"cli\", \"searchengine\", \"postgrest\", \"devtool\", \"cloudstorage\", \"git\", \"npm\", \"database\", \"postgresql\", \"backend\", \"shell-scripting\", \"websocket\", \"collaboration\", \"developer-tools\", \"promise\", \"api\", \"testing\", \"translation\", \"i18n\", \"language\", \"golang-library\", \"algorithm\", \"firmware\", \"style-linter\", \"linting\", \"converter\", \"blockchain\", \"wordpress\", \"static-site-generator\", \"blog-engine\", \"material\", \"material-design\", \"framework\", \"argument-parser\", \"command-line-parser\", \"readme-generator\", \"ssh\", \"backup\", \"reverse-engineering\", \"animation\", \"sdk\", \"devops\", \"jenkins\", \"documentation\", \"terminal\", \"encryption\", \"scrapers\", \"3d-printing\", \"reactive-templates\", \"image-optimization\", \"file-server\", \"nat\", \"proxy\", \"shell\", \"linters\", \"git-client\", \"raspberry-pi\", \"blogging\", \"npm-cli\", \"aws\", \"api-gateway\", \"decompiler\", \"kubernetes\", \"tools\"],\n",
    "    \"App\": [\"note-taking\", \"productivity\", \"prest\", \"download\", \"latex\", \"text-editor\", \"curl\", \"ftp\", \"bot\", \"synchronization\", \"sqlite\", \"mattermost\", \"messaging\", \"conferencing\", \"remote-desktop\", \"emacs\", \"color-picker\", \"cli-app\", \"subtitle-downloader\", \"decompiler\", \"mobile-app\"],\n",
    "\n",
    "    \"Resource\": [\"learn-to-code\", \"freecodecamp\", \"curriculum\", \"certification\", \"learnopengl\", \"lists\", \"resources\", \"resource\", \"dataset\", \"public-api\", \"public-apis\", \"practice\", \"interview\", \"styleguide\", \"list\", \"interview-questions\", \"awesome-list\", \"principles\", \"design-patterns\"],\n",
    "}\n",
    "# Tags manually added to some repositories (which otherwise lack descriptive ones)\n",
    "MANUAL_TAGS = {\n",
    "    \"minio/minio\": [\"Machine Learning\"],\n",
    "    \"Aliucord/Aliucord\": [\"Modding\"],\n",
    "    \"cli-guidelines/cli-guidelines\": [\"Resource\"],\n",
    "    \"yjs/yjs\": [\"Tool\"],\n",
    "    \"TigerVNC/tigervnc\": [\"Tool\"],\n",
    "    \"ollama/ollama\": [\"App\"],\n",
    "    \"micropython/micropython\": [\"Tool\"], # Python implementation\n",
    "    \"raspberrypi/linux\": [\"App\"],\n",
    "    \"rust-lang/rust\": [\"Tool\"],\n",
    "    \"home-assistant/core\": [\"Tool\"],\n",
    "    \"vuejs/vue-cli\": [\"Tool\", \"Web\"],\n",
    "    \"remix-run/remix\": [\"Tool\", \"Web\"],\n",
    "}\n",
    "# Same thing as above, but mapping tag to list of repos with it instead, as I realized at the end this would've been more convenient.\n",
    "MANUAL_TAGS2 = {\n",
    "    \"Tool\": [\"pytorch/tutorials\", \"vuejs/core\", \"google/googletest\", \"google/guava\", \"ReactiveX/RxJava\"],\n",
    "    \"Web\": [\"vuejs/core\"],\n",
    "    \"App\": [\"square/retrofit\"],\n",
    "}\n",
    "for tag,aliases in TAG_ALIASES.items():\n",
    "    for alias in aliases:\n",
    "        TAG_MAP[alias] = tag\n",
    "for tag,repos in MANUAL_TAGS2.items():\n",
    "    for repo in repos:\n",
    "        if repo in MANUAL_TAGS:\n",
    "            MANUAL_TAGS[repo].append(tag)\n",
    "        else:\n",
    "            MANUAL_TAGS[repo] = [tag]\n",
    "\n",
    "IGNORED_TAGS = defaultdict(int)\n",
    "\n",
    "oss_repos = {}\n",
    "for index, row in df_repo.iterrows():\n",
    "    key = row[\"owner\"] + \"/\" + row[\"name\"]\n",
    "    main_language = row[\"mainLanguage\"]\n",
    "    if main_language in LANGUAGE_REMAP:\n",
    "        main_language = LANGUAGE_REMAP[main_language]\n",
    "    repo = {\n",
    "        \"topics\": set(),\n",
    "        \"languages\": set([main_language] if main_language != \"\" else []),\n",
    "        \"stars\": row[\"total_stars\"],\n",
    "        \"contributors\": row[\"total_contributors\"],\n",
    "        \"icon\": row[\"avatar_url\"],\n",
    "        \"description\": row[\"description\"],\n",
    "    }\n",
    "    for topic in str.split(row[\"topics\"], \",\"):\n",
    "        if topic in TAG_MAP:\n",
    "            repo[\"topics\"].add(TAG_MAP[topic])\n",
    "        else:\n",
    "            IGNORED_TAGS[tag] += 1\n",
    "        if key in MANUAL_TAGS:\n",
    "            for tag in MANUAL_TAGS[key]:\n",
    "                repo[\"topics\"].add(tag)\n",
    "\n",
    "    oss_repos[key] = repo\n",
    "    used_tags = used_tags.union(repo[\"topics\"])\n",
    "    used_languages.add(main_language)\n",
    "\n",
    "# Exclude mirrors and other projects that are not contributable projects or unsuitable\n",
    "BLACKLISTED_REPOS = [\n",
    "    \"gitlabhq/gitlabhq\", # Read-only mirror.\n",
    "    \"qemu/qemu\", # Read-only mirror.\n",
    "    \"xasset/xasset\",# Not english.\n",
    "    \"jynew/jynew\", # Unity RPG game framework, documentation in chinese-only though.\n",
    "    \"doocs/advanced-java\", # Chinese-only Java interview questions.\n",
    "    \"CyC2018/CS-Notes\", # Chinese computer science course resources.\n",
    "    \"apache/kafka\", # Read-only mirror.\n",
    "]\n",
    "for blacklisted_repo in BLACKLISTED_REPOS:\n",
    "    del oss_repos[blacklisted_repo]\n",
    "\n",
    "# Convert sets to lists for json serialization, and add other\n",
    "# keys the site expects\n",
    "for key,repo in oss_repos.items():\n",
    "    repo[\"owner\"] = key.split(\"/\")[0]\n",
    "    repo[\"repo\"] = key.split(\"/\")[1]\n",
    "    repo[\"topics\"] = list(repo[\"topics\"])\n",
    "    repo[\"languages\"] = list(repo[\"languages\"])\n",
    "\n",
    "# Export the .json\n",
    "with open(\"repositories_output.json\", \"w\") as f:\n",
    "    json.dump(oss_repos, f, indent=2)\n",
    "\n",
    "print(\"Valid repositories:\", len(oss_repos))\n",
    "print(\"Languages used:\", used_languages)\n",
    "print(\"Tags used:\", used_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com els repositoris en la base de dades foren principalment trobades per scraping de les pàgines \"trending\" de GitHub, podem concloure que un 50% dels repositoris que GitHub destaca són només repositoris populars de projectes individuals o d'equips petits i no projectes contribuïbles; només uns 1000 repositoris dels 2000 en la base de dades compleixen els requisits que hem imposat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tables\n",
    "Shows the table for the different entities of the Data Base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM Repositories;\"\n",
    "\n",
    "output_repo, df_repo = execute_select_query(cursor, query)\n",
    "df_repo.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM RepositoryVisits;\"\n",
    "\n",
    "output_repo_visists, df_repo_visists = execute_select_query(cursor, query)\n",
    "df_repo_visists.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM RepositoryTopics;\"\n",
    "\n",
    "output_repo_topics, df_repo_topics = execute_select_query(cursor, query)\n",
    "df_repo_topics.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Owners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM Owners;\"\n",
    "# Use _ to ignore\n",
    "_, df_owners = execute_select_query(cursor, query)\n",
    "df_owners.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM OwnerVisits;\"\n",
    "\n",
    "output_owner_visits, df_owner_visits = execute_select_query(cursor, query)\n",
    "df_owner_visits.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM Commits;\"\n",
    "\n",
    "output_commits, df_commits = execute_select_query(cursor, query)\n",
    "\n",
    "df_commits.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM Topics;\"\n",
    "\n",
    "output_topics, df_topics = execute_select_query(cursor, query)\n",
    "df_topics.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM TopicVisits;\"\n",
    "\n",
    "_, df_topic_visits = execute_select_query(cursor, query)\n",
    "df_topic_visits.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM TrendVisits;\"\n",
    "\n",
    "_, df_trend_visits = execute_select_query(cursor, query)\n",
    "df_trend_visits.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "### Basic statistics\n",
    "* Distribution of main topics (bar or pie chart). Aka, in what proportion of our studied repos are they treated.\n",
    "* Distribution of languages (bar or pie chart). Like above.\n",
    "* Time evolution of interest in topics: repositories per topic, followers per topic.\n",
    "\n",
    "### Dimensional reduction for an overview on repositories\n",
    "What we mean here is to perform a PCA (principal components analysis) in order to able to have an insight on the structre of the whole dataset. We would then color the data points in it depending on the language used, for instance, to see if these groups have similar characteristics and lie close in the dataframe or not.\n",
    "* Create a dataframe containing all RepositoryVisits data for a certain date for all the studied repositories.\n",
    "* Perform a standarization on the data (i.e., to prevent some variables such as commits to be far more important than others such as forks).\n",
    "* Perform a PCA into 2 components on it.\n",
    "* Plot the results while clustering the points depending on different criteria:\n",
    "    + mainLanguage\n",
    "    + topic\n",
    "* Supervised machine learning using stars.\n",
    "### Open questions\n",
    "* How to use stars and trends?\n",
    "* How to use contributions by owners?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis missatge de commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "\"\"\"\n",
    "If \n",
    "!pip install wordcloud \n",
    "doesn't work:\n",
    "import sys\n",
    "print(sys.executable) # use the path\n",
    "\n",
    "path -m pip install wordcloud\n",
    "\"\"\"\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define constants and configurations\n",
    "STANDARDIZE_SUBSTITUTION = re.compile(r\"[!\\\"$#%&()*+,\\-./:;<=>?[\\]^_`{|}~]\")\n",
    "CONSECUTIVE_SPACE_SUBSTITUTION = re.compile(r\"  +\")\n",
    "LEMMATIZATION_BLACKLIST = {\"was\", \"as\"}\n",
    "WORD_REPLACEMENTS = {\n",
    "    \"read-me\": \"readme\",\n",
    "    \"read.me\": \"readme\",\n",
    "    \"readme.md\": \"readme\"\n",
    "}\n",
    "stopwords = set(STOPWORDS)\n",
    "# stopwords.update([\"a\"]) # Manually add stopwords if needed\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcions auxiliars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accents(input_str):\n",
    "    \"\"\"\n",
    "    Convert accented characters to base characters.\n",
    "    Source: https://stackoverflow.com/a/1207479\n",
    "    \"\"\"\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str).encode(\"ascii\", \"ignore\")\n",
    "    return bytes.decode(nfkd_form)\n",
    "\n",
    "lemmatizer = nltk.wordnet.WordNetLemmatizer()\n",
    "\n",
    "# Function to get the wordnet POS tag\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def standardize(word):\n",
    "    \"\"\"Standardize a word.\"\"\"\n",
    "    word = word.lower()\n",
    "    word = re.sub(STANDARDIZE_SUBSTITUTION, \" \", word)\n",
    "    word = re.sub(CONSECUTIVE_SPACE_SUBSTITUTION, \" \", word)\n",
    "    word = remove_accents(word)\n",
    "    word = word.replace(\"'s\", \"\").replace(\"#\", \"\").strip()\n",
    "    words = [WORD_REPLACEMENTS.get(w, w) for w in word.split()]\n",
    "    \n",
    "    # Enhanced lemmatization with POS tagging\n",
    "    words = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) if w not in LEMMATIZATION_BLACKLIST else w for w in words]\n",
    "    \n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(standardize(\"thIs a TesT\"))\n",
    "print(standardize(\"readme.md\"))\n",
    "_standardize_test_words = [\n",
    "    \"Testing @here as#Dasd 333232 tests testings gItHUbs testing added\",  # tests -> test due lemmatization\n",
    "    \"Test_ing !!, (asd,as d).\",  # Underscore are considered separated words\n",
    "    \"Test łłłł ñaññañ áaaa\",  # Removal of accents and non standard characters\n",
    "    \"テスト #asdasd arabian, wolves\",\n",
    "]\n",
    "\n",
    "for word in _standardize_test_words:\n",
    "    print(standardize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neteja de dades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com que l'objectiu es analitzar com escriuen les persones, hi ha un conjunt de missatges que no ens interessen. Per això cal excloure tots aquells missatges que no compleixin uns requisits.\n",
    "\n",
    "Els nostres criteris d'exclusió són:\n",
    "- Missatges de bots, els quals podem identificar per un autor que tingui en el nom `[bot]` i missatges que tinguin noms com pot ser: `dependabot` o `renovatebot`\n",
    "- Qualsevol tipus de misstage duplicat, ja que això faria que en l'anàlisis es comptéssin dades duplicades.\n",
    "- Els missatges automatics que genera github com poden ser: `Merge pull request`, `Merge branch`, `Squash`, `Initial commit` i `Revert`, ja que no són escrits per les persones.\n",
    "\n",
    "A partir d'aquests criteris, hem realitzat una neteja de les dades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the commits authored by a bot.\n",
    "from IPython.display import display\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "cond_a = df_commits[df_commits[\"author\"].str.contains(\"\\[bot]|-bot\")]\n",
    "cond_c = df_commits[df_commits[\"message\"].str.contains(\"dependabot\")]\n",
    "cond_d = df_commits[df_commits[\"message\"].str.contains(\"renovatebot\")]\n",
    "\n",
    "temp = pd.concat([cond_a])\n",
    "\n",
    "unique_auth = pd.unique(temp.author)\n",
    "unique_msg = pd.unique(cond_c.message)\n",
    "\n",
    "dict = {'bots' : unique_auth}\n",
    "df_bots = pd.DataFrame(dict)\n",
    "\n",
    "# displaying the bots names\n",
    "display(df_bots)\n",
    "\n",
    "dict = {'bots_message' : unique_msg}\n",
    "df_bots_msg = pd.DataFrame(dict)\n",
    "\n",
    "display(df_bots_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the bots\n",
    "print(\"With bots\", len(df_commits))\n",
    "df_commits = df_commits[~df_commits[\"author\"].isin(df_bots[\"bots\"])]\n",
    "print(\"Without bots\", len(df_commits))\n",
    "\n",
    "# Filtering duplicated messages (probably automatic messages)\n",
    "df_commits = df_commits.drop_duplicates(subset=['message'])\n",
    "print(\"Without dupplicated messages\", len(df_commits))\n",
    "\n",
    "# Filter out the automatic messages\n",
    "df_commits = df_commits[\n",
    "    ~(df_commits.message.str.startswith(\"Merge pull request\") |\n",
    "      df_commits.message.str.startswith(\"Merge branch\") |\n",
    "      df_commits.message.str.startswith(\"Squash\") |\n",
    "      df_commits.message.str.startswith(\"Initial commit\") |\n",
    "      df_commits.message.str.startswith(\"Revert \")\n",
    "     )\n",
    "]\n",
    "print(\"Without automatic messages\", len(df_commits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui podem veure que de 94.872 missatges, hem reduit els missatges usables a 66.582 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis paraules més usades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per fer l'anàlisis de les paraules més usades, ho farem de dues formes. \n",
    "- Usarem wordcloud, el qual és una representacó gràfica d'un dibuix creat per paraules on segons les ocurrències d'aquestes, són més grans o si apareixen poc, més petites. Obtenint així, una visió ràpida, de quines paraules són les més usades.\n",
    "- Comptarem les paraules i mostrarem un gràfic dels top n paraules més usats en ordre descendent, segons paraula més usada en missatges individuals o ocurrències totals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_cloud(word_cloud, text, save_image=False, image_name=\"none\"):\n",
    "    \"\"\"Create and display a word cloud image.\"\"\"\n",
    "    word_cloud_output = word_cloud.generate(text)\n",
    "    plt.imshow(word_cloud_output, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    if save_image:\n",
    "        word_cloud_output.to_file(f\"./Images/wordclouds/{image_name}_word_cloud.png\")\n",
    "\n",
    "def transform_format(val):\n",
    "    \"\"\"Transform format for mask creation.\"\"\"\n",
    "    return 255 if val == 0 else val\n",
    "\n",
    "def obtain_mask(mask):\n",
    "    \"\"\"Obtain mask for word cloud.\"\"\"\n",
    "    trans_mask = np.ndarray((mask.shape[0], mask.shape[1]), np.int32)\n",
    "    for i in range(len(mask)):\n",
    "        trans_mask[i] = list(map(transform_format, mask[i]))\n",
    "    return trans_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all text\n",
    "all_text = \" \".join(commit for commit in df_commits.message)\n",
    "# Set the wordcloud params\n",
    "word_cloud = WordCloud(stopwords=stopwords, max_font_size=30, max_words=200, background_color=\"black\")\n",
    "# Plot the wordcloud\n",
    "plot_word_cloud(word_cloud, all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a mask and a color map\n",
    "\n",
    "# <a href='https://dryicons.com/icon/square-github-icon-8312'> Icon by Dryicons </a>\n",
    "github_image = np.array(Image.open(\"./Images/wordclouds/github_square.png\"))\n",
    "\n",
    "word_cloud_mask = obtain_mask(github_image)\n",
    "# Word cloud repeats words since he needs fill the gaps using the correct size.\n",
    "# print(len(all_text.split(' ')))\n",
    "# print(len(pd.unique(all_text.split(' '))))\n",
    "\n",
    "word_cloud = WordCloud(stopwords=stopwords, colormap='rainbow', mask=word_cloud_mask, max_font_size=30, max_words=10000, background_color=\"black\")\n",
    "plot_word_cloud(word_cloud, all_text, True, \"git_mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a mask        \n",
    "git_image = np.array(Image.open(\"./Images/wordclouds/git.png\"))\n",
    "\n",
    "word_cloud_mask = obtain_mask(git_image)\n",
    "\n",
    "word_cloud = WordCloud(stopwords=stopwords, mask=word_cloud_mask,colormap='hot', max_font_size=20, max_words=10000, background_color=\"#474747\")\n",
    "plot_word_cloud(word_cloud, all_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podem veure que les paraules més usades, són: `fix, add, update` sense comptar els stopwords, ja que podem veure que són les paraules més grans. Tot seguit veurem els valors exactes, de les paraules més usades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    :param df: DataFrame with the messages and associated information\n",
    "    :return: Dictionary with the format {word: {n_ocur: value, n_messages: value}, ...}\n",
    "    \"\"\"\n",
    "    # Using defaultdict for convenience; we won't have to add keys explicitly\n",
    "    dicc = defaultdict(lambda: {\"n_ocur\": 0, \"n_messages\": 0})\n",
    "\n",
    "    total_rows = len(df)\n",
    "    processed_rows_count = 0\n",
    "    for row in df.iterrows():\n",
    "        text = row[1][0]\n",
    "        text = standardize(text)  # Apply standardization\n",
    "        if any(char.isdigit() for char in text):\n",
    "            continue\n",
    "        # Count the words\n",
    "        words = text.split(\" \")\n",
    "        unique_words = set(words)\n",
    "        # Times that a word appears and times that appears in different messages.\n",
    "        for word in words:\n",
    "            dicc[word][\"n_ocur\"] += 1\n",
    "        for word in unique_words:\n",
    "            dicc[word][\"n_messages\"] += 1\n",
    "\n",
    "        processed_rows_count += 1\n",
    "        # print(f\"Word {processed_rows_count}/{total_rows} done; {processed_rows_count / total_rows * 100:.2f}%\")\n",
    "    \n",
    "    return dicc\n",
    "\n",
    "frame = {'Messages': df_commits.message}\n",
    "\n",
    "result = pd.DataFrame(frame)\n",
    "\n",
    "ocurrencesDict = count_words(result)\n",
    "\n",
    "# Sort the words by occurrences and occurrences in messages.\n",
    "def obtain_top_n_words(dictionary, N, filter=\"n_ocur\", desc=True):\n",
    "    # Get all the words and their frequency\n",
    "    filtered_words = [(word, freq[filter]) for word, freq in dictionary.items() if word not in stopwords]\n",
    "    \n",
    "    # Sort from most to least frequent\n",
    "    sorted_words = sorted(filtered_words, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Take the N most frequent\n",
    "    return [(word, freq) for word, freq in sorted_words[:N]]\n",
    "\n",
    "top_words_list_ocur = obtain_top_n_words(ocurrencesDict, 30, \"n_ocur\", True)\n",
    "top_words_list_msg = obtain_top_n_words(ocurrencesDict, 30, \"n_messages\", True)\n",
    "\n",
    "print(\"Ocurrences Top\", top_words_list_ocur)\n",
    "print(\"\\nOcurrences per message Top\", top_words_list_msg)\n",
    "\n",
    "# Function to plot the top words\n",
    "def plot_top_words(topList, yLabel, ax):\n",
    "    x = [word for word, freq in topList]\n",
    "    y = [freq for word, freq in topList]\n",
    "    \n",
    "    # Making the bar chart on the data\n",
    "    ax.bar(x, y)\n",
    "    \n",
    "    # Giving title to the plot\n",
    "    ax.set_title(\"Top paraules més usades\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(x, rotation=90, ha='right')\n",
    "    # Giving X and Y labels\n",
    "    ax.set_xlabel(\"Word\")\n",
    "    ax.set_ylabel(yLabel)\n",
    "     \n",
    "    # We do not call plt.show() here, as we want to show both plots together\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "# Plot the top words by occurrences\n",
    "plot_top_words(top_words_list_ocur, \"Ocurrències de la paraula\", axs[0])\n",
    "\n",
    "# Plot the top words by occurrences in different messages\n",
    "plot_top_words(top_words_list_msg, \"Ocurrències de la paraula en diferents missatges\", axs[1])\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n",
    "\n",
    "# We can see that there is no significant difference, since normally, in a commit message, we do not repeat the same word twice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De les gràfiques, podem veure que en termes quantitatius, són casi idèntics, tenen la mateixa escala i en general, el mateix ordre de les paraules més usades. Això denota que no se sol repetir les paraules en un mateix text, ja que la gràfica de l'esquerra, compta les paraules totals, mentre que el de la dreta, compta les paraules per missatge.\n",
    "\n",
    "Les tres paraules més usades, son `fix, add, update`, els quals eren els mateixos que hem vist en el wordcloud. Hem agrupat les paraules segons un possible significat semàntic.\n",
    "\n",
    "### Cicle de Desenvolupament:\n",
    "\n",
    "- **Add (7121 occurrences):** Adició de noves característiques i creixement del projecte.\n",
    "- **Fix (6692 occurrences):** Correció d'errors per assegurar l'estabilitat del projecte.\n",
    "- **Update (5167 occurrences):** Millores i actualitzacions del codi existent, assegurant que el projecte es mantingui actualitzat amb les millors pràctiques i tecnologies.\n",
    "\n",
    "### Paraules Complementàries:\n",
    "\n",
    "- **Remove (1943 occurrences):** Reflecteix l'eliminació de codi o funcionalitats obsoletes o innecessàries, la qual cosa és crucial per mantenir el codi net i eficient.\n",
    "- **Use (1491 occurrences):** Indica la implementació o reutilització de codi o biblioteques, suggerint un ús eficient dels recursos disponibles.\n",
    "- **Test (1537 occurrences):** Subratlla la importància de les proves en el cicle de desenvolupament per assegurar la funcionalitat correcta i evitar regressions.\n",
    "- **Readme (1176 occurrences):** Reflecteix la documentació del projecte, important per a la col·laboració i la claredat entre els membres de l'equip i els usuaris.\n",
    "- **Feat (1149 occurrences):** Similar a \"add\", mostra l'enfocament en noves funcionalitats.\n",
    "  \n",
    "### Paraules Clau en el Manteniment i Millora:\n",
    "\n",
    "- **Version (1080 occurrences):** Indica canvis en les versions del projecte, reflectint una evolució contínua i la gestió de versions.\n",
    "- **File (1077 occurrences):** Pot referir-se a la gestió de fitxers dins del projecte, com la creació, modificació o eliminació de fitxers.\n",
    "- **Refactor (606 occurrences):** Subratlla l'esforç de reorganització del codi per millorar la seva estructura sense canviar-ne el comportament extern.\n",
    "- **Improve (576 occurrences):** Reflecteix les millores contínues del codi o funcionalitats existents.\n",
    "\n",
    "### Altres Paraules Rellevants:\n",
    "\n",
    "- **Build (812 occurrences), Link (773 occurrences), Error (732 occurrences):** Aquestes paraules indiquen activitats específiques relacionades amb la compilació del projecte, la gestió d'enllaços, i la correcció d'errors respectivament.\n",
    "- **CI (640 occurrences):** Fa referència a la integració contínua, important per al desplegament automàtic i la verificació contínua de canvis.\n",
    "- **Config (479 occurrences):** Reflecteix la configuració del projecte, que és crucial per a la seva correcta execució i desplegament\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grups de paraules més usades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En aquesta secció, usarem **ítems freqüents** per trobar patrons els quals els usuaris escriuen. La idea principal, es trobar grups de paraules els quals quan apareix un, apareix un altre. Això indicaria que les paraules estan relacionades o s'utilitzen conjuntament amb freqüència. Aquest procés ens ajuda a identificar associacions o patrons significatius en el text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/apriori/#apriori-frequent-itemsets-via-the-apriori-algorithm\n",
    "def generate_data_set(df: pd.DataFrame, rmStopWords = True, useStandard = True):\n",
    "    \"\"\"\n",
    "    :param df: DataFrame with the messages and associated information\n",
    "    :return dataSet: The message of the commit is the transaction and the words the items.\n",
    "    \"\"\"\n",
    "    data_set = []\n",
    "    total_rows = len(df)\n",
    "    processed_rows_count = 0\n",
    "    for row in df.iterrows():\n",
    "        text = row[1][0]\n",
    "        if (useStandard):\n",
    "            text = standardize(text)  # Apply standardization\n",
    "       \n",
    "        # Split the words, but don't hold digits.\n",
    "        words = [word for word in text.split() if not any(char.isdigit() for char in word)]\n",
    "        if (rmStopWords):\n",
    "            unique_words = list(set(words) - stopwords)\n",
    "        else:\n",
    "            unique_words = list(set(words))\n",
    "        data_set.append(unique_words)\n",
    "\n",
    "        processed_rows_count += 1\n",
    "        # print(f\"Word {processed_rows_count}/{total_rows} done; {processed_rows_count / total_rows * 100:.2f}%\")\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "import math\n",
    "#!pip install mlxtend\n",
    "\n",
    "def get_frequent_itemsets(data, min_supp = 0.01):\n",
    "    \"\"\"\n",
    "    To save memory, you may want to represent your transaction data in the sparse format. \n",
    "    This is especially useful if you have lots of products and small transactions.\n",
    "    \"\"\"\n",
    "    te = TransactionEncoder()\n",
    "    oht_ary = te.fit(data).transform(data, sparse=True)\n",
    "    sparse_df = pd.DataFrame.sparse.from_spmatrix(oht_ary, columns=te.columns_)\n",
    "    \n",
    "    # total message * support = number of messages which the bundle appears.\n",
    "    print(\"We consider that at least the bundle appears in:\", math.ceil(len(data) * min_supp), \" messages\")\n",
    "    \n",
    "    # Calculate it using apriori algorithm\n",
    "    frequent_itemsets = apriori(sparse_df, min_support=min_supp, use_colnames=True, verbose=1)\n",
    "    frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "    return frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data frame using the commits.\n",
    "frame = {'Messages': df_commits.message}\n",
    "result = pd.DataFrame(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the data set is too big, and our computers can't handle it, we are going to use a randomly sampled fraction of it.\n",
    "partial_data_set = generate_data_set(result.sample(frac =.50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets = get_frequent_itemsets(partial_data_set, 0.001)\n",
    "# Sort them from more length and then support\n",
    "frequent_itemsets_sorted = frequent_itemsets.sort_values(by=['length', 'support'], ascending=[False, False])\n",
    "print(frequent_itemsets_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_support = math.ceil(frequent_itemsets_sorted['support'].mean() * len(partial_data_set))\n",
    "print(\"Average amount of support: \", average_support, \" messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_group = frequent_itemsets[(frequent_itemsets['length'] >= 2)]\n",
    "data_group['itemsets'] = data_group['itemsets'].apply(lambda x: str(sorted(list(x))))\n",
    "data_group['support messages'] = data_group['support'].apply(lambda x: math.ceil(len(partial_data_set) * x))\n",
    "data_group = data_group.sort_values(by=['support messages'], ascending=False)\n",
    "# Show only those groups, who at least, have the average support.\n",
    "# With this, we can preserve significant data, while reducing the amount of itemsets\n",
    "data_group = data_group[(data_group['support messages'] >= average_support)]\n",
    "\n",
    "print(data_group[['itemsets', 'support messages']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to group itemsets by starting word\n",
    "def group_by_starting_word(df):\n",
    "    grouped_phrases = {}\n",
    "    for itemset in df['itemsets']:\n",
    "        cleaned_string = itemset.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")\n",
    "        items = cleaned_string.split(', ')\n",
    "        items_list = list(map(str.strip, items))  # This removes leading/trailing spaces\n",
    "        if items_list[0] in grouped_phrases:\n",
    "            grouped_phrases[items_list[0]].append(items_list[1])\n",
    "        else:\n",
    "            grouped_phrases[items_list[0]] = [items_list[1]]\n",
    "    return grouped_phrases\n",
    "\n",
    "# Group itemsets by starting word\n",
    "grouped_phrases = group_by_starting_word(data_group)\n",
    "print(grouped_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "# Show the \n",
    "def plot_graph(key, values):\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add main node (key)\n",
    "    G.add_node(key)\n",
    "\n",
    "    # Add edges between main node and each phrase (value)\n",
    "    for value in values:\n",
    "        G.add_edge(key, value)\n",
    "\n",
    "    # Plotting the graph\n",
    "    pos = nx.spring_layout(G, seed=42)  # positions for all nodes\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=2000, font_size=10, font_weight='bold', arrows=False)\n",
    "    plt.title(f'Undirected Graph for \"{key}\"')\n",
    "    plt.show()\n",
    "\n",
    "# Iterate through each key-value pair and plot the graph\n",
    "for key, values in grouped_phrases.items():\n",
    "    plot_graph(key, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Què passa si no normalitzem o comptem els stop words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_no_standard = generate_data_set(result.sample(frac =.50), rmStopWords = False, useStandard = True)\n",
    "frequent_itemsets = get_frequent_itemsets(data_set_no_standard)\n",
    "# Sort them from more length and then support\n",
    "frequent_itemsets_sorted = frequent_itemsets.sort_values(by=['length', 'support'], ascending=[False, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_group = frequent_itemsets[(frequent_itemsets['length'] >= 2)]\n",
    "data_group['itemsets'] = data_group['itemsets'].apply(lambda x: str(sorted(list(x))))\n",
    "data_group['support messages'] = data_group['support'].apply(lambda x: math.ceil(len(partial_data_set) * x))\n",
    "data_group = data_group.sort_values(by=['support messages'], ascending=False)\n",
    "\n",
    "print(data_group[['itemsets', 'support messages']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_phrases = group_by_starting_word(data_group)\n",
    "print(grouped_phrases)\n",
    "# Iterate through each key-value pair and plot the graph\n",
    "for key, values in grouped_phrases.items():\n",
    "    plot_graph(key, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_no_standard = generate_data_set(result.sample(frac =.5), rmStopWords = False, useStandard = False)\n",
    "frequent_itemsets = get_frequent_itemsets(data_set_no_standard, min_supp = 0.001) \n",
    "\n",
    "# Sort them from more length and then support\n",
    "data_group = frequent_itemsets[(frequent_itemsets['length'] >= 2)]\n",
    "data_group['itemsets'] = data_group['itemsets'].apply(lambda x: str(sorted(list(x))))\n",
    "data_group['support messages'] = data_group['support'].apply(lambda x: math.ceil(len(partial_data_set) * x))\n",
    "data_group = data_group.sort_values(by=['support messages'], ascending=False)\n",
    "\n",
    "print(data_group[['itemsets', 'support messages']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_phrases = group_by_starting_word(data_group)\n",
    "print(grouped_phrases)\n",
    "# Iterate through each key-value pair and plot the graph\n",
    "for key, values in grouped_phrases.items():\n",
    "    plot_graph(key, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per limitacions d'espai, i seguint el consell del professor, hem decidit utilitzar fragments parcials del conjunt de dades en lloc de calcular l'algoritme amb tot el data set.\n",
    "\n",
    "El resultat s'ha decidit mostrar usant graphs, ja que l'algoritme d'ítems freqüents concentra la major quantitat de grups en els paquets de 2 i és fàcil de veure com una paraula, es relaciona amb una altra usant aquesta.\n",
    "I el que hem obtingut, es el conjunt de paraules clau que solen apareixer junts. Per exemple, per fix, tenim:\n",
    "\n",
    "`'fix': ['in', 'to', 'for', 'on', 'the', 'of', 'issue', 'with']`\n",
    "\n",
    "El qual significa, que les persones solen escriure algo com:\n",
    "- \"fix xxxx to yyyyy\"\n",
    "- \"fix ssss with nnnn\"\n",
    "- \"fix issue eeee\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploració sintàctica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En aquesta secció, busquem identificar les categories gramaticals més comunes o dominants dels missatges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Uncomment to dowload the necessary data to run the functions.\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map to make the tags more reaedable //https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "tag_map = {\n",
    "    'CC': 'Conjunció coordinativa',\n",
    "    'CD': 'Nombre cardinal',\n",
    "    'DT': 'Determinant',\n",
    "    'EX': 'Hi ha existencial',\n",
    "    'FW': 'Paraula estrangera',\n",
    "    'IN': 'Preposició o conjunció subordinant',\n",
    "    'JJ': 'Adjectiu',\n",
    "    'JJR': 'Adjectiu comparatiu',\n",
    "    'JJS': 'Adjectiu superlatiu',\n",
    "    'LS': 'Marcador ítems de llista ',\n",
    "    'MD': 'Modal',\n",
    "    'NN': 'Nom, singular o massa',\n",
    "    'NNS': 'Nom, plural',\n",
    "    'NNP': 'Nom propi, singular',\n",
    "    'NNPS': 'Nom propi, plural',\n",
    "    'PDT': 'Predeterminant',\n",
    "    'POS': 'Sufix possessiu',\n",
    "    'PRP': 'Pronom personal',\n",
    "    'PRP$': 'Pronom possessiu',\n",
    "    'RB': 'Adverbi',\n",
    "    'RBR': 'Adverbi comparatiu',\n",
    "    'RBS': 'Adverbi superlatiu',\n",
    "    'RP': 'Partícula',\n",
    "    'SYM': 'Símbol',\n",
    "    'TO': 'a',\n",
    "    'UH': 'Interjecció',\n",
    "    'VB': 'Verb, forma base',\n",
    "    'VBD': 'Verb, pretèrit',\n",
    "    'VBG': 'Verb, gerundi o participi present',\n",
    "    'VBN': 'Verb, participi passat',\n",
    "    'VBP': 'Verb, present no tercera persona singular',\n",
    "    'VBZ': 'Verb, present tercera persona singular',\n",
    "    'WDT': 'Determinant WH',\n",
    "    'WP': 'Pronom WH',\n",
    "    'WP$': 'Pronom possessiu WH',\n",
    "    'WRB': 'Adverbi WH'\n",
    "}\n",
    "\n",
    "# Load all text\n",
    "all_text = \" \".join(commit for commit in df_commits.message)\n",
    "print(\"Totes les paraules:\", len(all_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ratios(all_text, tag_map):\n",
    "    # apply tokenization to the text and tag it\n",
    "    tokens = word_tokenize(all_text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    \n",
    "    # Count the occurrence of each tag\n",
    "    tag_counts = Counter(tag for word, tag in tagged)\n",
    "    \n",
    "    # Convert tags to full wording in English\n",
    "    english_tag_counts = {tag_map.get(tag, tag): count for tag, count in tag_counts.items()}\n",
    "    \n",
    "    # Calculate the ratio of each tag\n",
    "    total_tags = sum(tag_counts.values())\n",
    "    tag_ratios = {tag: count / total_tags for tag, count in english_tag_counts.items()}\n",
    "    \n",
    "    # Sort ratios by values in descending order\n",
    "    sorted_ratios = sorted(tag_ratios.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    # Create a dataframe to show the result\n",
    "    df_ratios = pd.DataFrame(sorted_ratios, columns=['Etiqueta', 'Ratio'])\n",
    "    \n",
    "    # Show the equivalence of ratio in words\n",
    "    data_size = len(all_text)\n",
    "    df_ratios['Paraules'] = df_ratios['Ratio'].apply(lambda x: math.ceil(x * data_size))\n",
    "    \n",
    "    return df_ratios\n",
    "    \n",
    "df_ratios = calc_ratios(all_text, tag_map)\n",
    "print(df_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the ratios\n",
    "tags = df_ratios[\"Etiqueta\"]\n",
    "ratios = df_ratios[\"Ratio\"]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(tags, ratios, color='skyblue')\n",
    "plt.xlabel('Etiquetes de les paraules')\n",
    "plt.ylabel('Percentatge (%)')\n",
    "plt.title('Distribució ús categoria gramatical')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A través dels resultats, podem destacar que hi ha una dominància dels noms: Les categories \"Nom, singular o massa\" i \"Nom propi, singular\" ocupen les dues primeres posicions en termes de freqüència relativa (ratio). Això podria indicar que solen descriure o senyalar molt a entitas concrets i per això potser tenim en tercera posició els adejctius. \n",
    "\n",
    "Per tant, es podria dir que els missatges són més aviat descriptives, amb l'objectiu de facilitar i saber a què fan referència. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I quines formes verbals s'usen més?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ratio of verbs\n",
    "data_size = len(all_text)\n",
    "verb_usage = df_ratios[df_ratios['Etiqueta'].str.startswith('Verb')].copy()\n",
    "verb_usage['Paraules'] = verb_usage['Ratio'].apply(lambda x: math.ceil(x * len(all_text)))\n",
    "print(verb_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = verb_usage['Etiqueta']\n",
    "ratios = verb_usage['Ratio']\n",
    "\n",
    "# Plotting the ratios\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(tags, ratios, color='skyblue')\n",
    "plt.xlabel('Etiquetes de les paraules')\n",
    "plt.ylabel('Percentatge (%)')\n",
    "plt.title('Distribució ús verbs')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per tant, com a resultat, podem concloure que els missatges estan enfocats a clarificar i identificar els canvis realitzats en el codi. Aquest enfocament és crucial per a la comunicació eficient entre els membres de l'equip de desenvolupament i per a la comprensió precisa de les modificacions implementades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closes the connection\n",
    "dataBaseConnection.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-python3-kernel",
   "language": "python",
   "name": "my-python3-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
