{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projecte github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Draw the plots immediately after the current cell\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "# Uncomment the following line to install pymysql\n",
    "#!pip install PyMySQL\n",
    "import pymysql\n",
    "import warnings\n",
    "\n",
    "# POLARS # <- teacher said that this one is 1000% faster than pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project configuration\n",
    "\n",
    "The file that we are going to use to set up the basic structure:\n",
    "\n",
    "GitHubScraper/JupyterNotebook/Structure/githubProjectStructure.sql\n",
    "\n",
    "from the github repository.\n",
    "\n",
    "### Step 1, import the structure\n",
    "In workbench: \n",
    "\"Server\" -> \"Data Import\", and then you should select the direction of \"githubProjectStructure.sql\" using the 3 dots and import from self-contained file like in the following image:\n",
    "\n",
    "![Javatpoint](./Images/Project_Setup/data_import.png)  \n",
    "\n",
    "Then scroll down, and create a new Schema by clicking on the \"New...\" button. Name it: \"githubProject\".\n",
    "\n",
    "![Javatpoint](./Images/Project_Setup/create_new_schema.png)  \n",
    "\n",
    "Scroll to the bottom, and in the bottom right, click on \"Start import\". If there are no errors, it will transitionate to \"Import progress\" and indicate success.\n",
    "\n",
    "Refresh the scehmas:\n",
    "\n",
    "![Javatpoint](./Images/Project_Setup/refresh.png)  \n",
    "\n",
    "### Step 2, importing the data from the csv files\n",
    "\n",
    "Download the csv files [here](https://drive.google.com/drive/folders/1NWhfFss0_M9V_clkcE9TH-Fy3oIEndWV?usp=sharing).\n",
    "\n",
    "Selecting the \"githubProject\" scheme, right click and \"Table Data Import Wizard\", with this, we are going to import each csv file to the matching table.\n",
    "\n",
    "You have to select the .csv you want to import, for example in this case, the trendVisits.csv\n",
    "\n",
    "![Javatpoint](./Images/Project_Setup/file_import.png) \n",
    "\n",
    "Then, since you already have the structure, you have to use the existing table that matches with the correct csv file:\n",
    "\n",
    "![Javatpoint](./Images/Project_Setup/use_existing.png) \n",
    "\n",
    "After that, check that the variables type are correct and import it. Do this for each csv file.\n",
    "\n",
    "You can also verify the data by doing this:\n",
    "\n",
    "![Javatpoint](./Images/Project_Setup/verify.png) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "TODO index or sections indicating what are we doing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.geeksforgeeks.org/connect-to-mysql-using-pymysql-in-python/\n",
    "# Connect to MySQL database\n",
    "def tryToConnectMySQL(db_name, db_host, db_port, db_username, db_password):\n",
    "    try:\n",
    "        conn = pymysql.connect(host=db_host,\n",
    "                               port=db_port,\n",
    "                               user=db_username,\n",
    "                               password=db_password,\n",
    "                               db=db_name)\n",
    "        if conn:\n",
    "            print(\"Connection successful\")\n",
    "        else:\n",
    "            print(\"Error\")\n",
    "        \n",
    "        return conn\n",
    "        \n",
    "    except pymysql.Error as e:        \n",
    "        warnings.warn(\"Error connecting to MySQL:\", e)\n",
    "        return None\n",
    "\n",
    "# Pass arguments from outside\n",
    "db_name = \"githubProject\"\n",
    "db_host = \"localhost\"\n",
    "db_port = 3306\n",
    "db_username = \"root\"\n",
    "db_password = input(\"Input your password\")\n",
    "\n",
    "dataBaseConnection = tryToConnectMySQL(db_name, db_host, db_port, db_username, db_password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute query and return data in pandas dataframe \n",
    "def execute_select_query(cursor, query):\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        output = cursor.fetchall()\n",
    "        # Fetch column names from cursor's description\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "        \n",
    "        # Convert output to pandas DataFrame\n",
    "        if output:\n",
    "            df = pd.DataFrame(output, columns=columns)\n",
    "            print(\"Query executed successfully!\")\n",
    "        return output, df\n",
    "            \n",
    "    except pymysql.Error as e:\n",
    "        print(\"Error executing query:\")\n",
    "        warnings.warn(str(e))\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the cursos in order to interact with the DataBase\n",
    "cursor = dataBaseConnection.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repositoris \"Open Source\" & dades per a la pàgina web\n",
    "Quant a l'objectiu de destacar repositoris open-source (i no només repositoris populars, com fa GitHub), volem exportar dades d'aquests repositoris en format `.json` per a que puguin ser usades per la pàgina web.\n",
    "\n",
    "Considerem que repositoris són projectes open-source si tenen més de 5 contribuïdors i més de 50 issues en total - un criteri simple però efectiu.\n",
    "\n",
    "Recollir les dades necessàries involucra fer JOINs amb múltiples taules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_source_projects_query = \"\"\"\n",
    "SELECT r.owner, r.name, r.description, r.mainLanguage,\n",
    "MAX(stars) as total_stars, MAX(contributors) as total_contributors, MAX(openIssues + closedIssues) as total_issues,\n",
    "o.avatar_url, GROUP_CONCAT(DISTINCT t.topic) as topics,\n",
    "MAX(watchers) as total_watchers FROM Repositories r\n",
    "-- Join with RepositoryVisits to get metrics like contributors, stars, issues amount\n",
    "JOIN RepositoryVisits v\n",
    "ON r.owner = v.owner AND r.name = v.name\n",
    "-- Join with Owners to get the avatar URL\n",
    "JOIN Owners o\n",
    "ON r.owner = o.username\n",
    "-- Join with RepositoryTopics to get a list of the tags/topics of the repos\n",
    "JOIN RepositoryTopics t\n",
    "ON t.repo = r.name AND t.owner = r.owner\n",
    "GROUP BY r.owner, r.name, r.mainLanguage, r.license -- Must group by mainLanguage and license as well as they're non-aggregated\n",
    "HAVING total_contributors > 5 AND total_issues > 50\n",
    "ORDER BY total_contributors DESC; -- Show repos with most contributors first\n",
    "\"\"\"\n",
    "\n",
    "output_repo, df_repo = execute_select_query(cursor, open_source_projects_query)\n",
    "print(len(df_repo), \"repositories matching criteria\")\n",
    "df_repo.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les dades d'aquests repositoris s'exporten a .json perque puguin ser usades per la pàgina web.\n",
    "\n",
    "En aquest procés també categoritzem els repositoris segons les seves temàtiques generals:\n",
    "- Desenvolupament web\n",
    "- Data science\n",
    "- Aplicacions\n",
    "- Eines de desenvolupament\n",
    "- Repositoris de recursos (ex. una col·lecció d'algorismes)\n",
    "\n",
    "La categorització es fa segons els \"topics\" (tags) que tenen els repositoris. Els repositoris també es categoritzen pel seu llenguatge de programació principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "used_languages = set()\n",
    "used_tags = set()\n",
    "\n",
    "# Renames or discards languages.\n",
    "# Used to group up frameworks, variants and transpilers.\n",
    "LANGUAGE_REMAP = {\n",
    "    \"TypeScript\": \"JavaScript\",\n",
    "    \"Vue\": \"JavaScript\",\n",
    "    \"HTML\": \"JavaScript\", # Bruh\n",
    "    \"Jupyter Notebook\": \"Python\",\n",
    "    \"Kotlin\": \"Java & Kotlin\",\n",
    "    \"Java\": \"Java & Kotlin\",\n",
    "    \"C\": \"C/C++\",\n",
    "    \"C++\": \"C/C++\",\n",
    "    \"Ruby\": \"Others\",\n",
    "    \"Go\": \"Others\",\n",
    "    \"Swift\": \"Others\",\n",
    "    \"Clojure\": \"Others\",\n",
    "    \"Haskell\": \"Others\",\n",
    "    \"Dart\": \"Others\",\n",
    "    \"Shell\": \"Others\",\n",
    "    \"PowerShell\": \"Others\",\n",
    "    \"Scala\": \"Others\",\n",
    "    \"Svelte\": \"Others\",\n",
    "    \"Vim Script\": \"\",\n",
    "    \"CSS\": \"\",\n",
    "    \"SCSS\": \"\",\n",
    "    \"MDX\": \"\",\n",
    "}\n",
    "\n",
    "# Remaps GitHub topics to the tags that are used in the website.\n",
    "TAG_MAP = {}\n",
    "# Maps tags used by the website to a list of Github \"topics\" that will be considered as the same tag.\n",
    "# Ex. repos with topics \"react\", \"vue\" become tagged as \"Web\"\n",
    "TAG_ALIASES = {\n",
    "    \"Web\": [\"react\", \"vue\", \"web\", \"reactjs\", \"css\", \"chrome-extension\", \"react-grid\", \"react-table\", \"php\", \"http\", \"nodejs\", \"typescript\", \"electron\", \"search-engine\", \"webgl\", \"rest\", \"rest-api\", \"swagger\", \"static-site-generator\", \"blog-engine\", \"router\", \"webview\", \"jquery\", \"http-client\", \"website\", \"reactive-templates\", \"nuxt\", \"nat\", \"javascript\", \"http2\", \"nginx\", \"apache\", \"aws\", \"api-gateway\", \"jekyll\", \"bootstrap\"],\n",
    "    \"Modding\": [\"mod\", \"minecraft\", \"emulation\", \"emulator\", \"forge\", \"minecraft-launcher\", \"modrinth\", \"minecraft-api\", \"minecraft-server\", \"bepinex\", \"unity3d\", \"unreal\", \"unity-mono\", \"craftbukkit\", \"valheim\", \"minecraft-mod\", \"gta5\", \"fabric\", \"gamedev\", \"retroarch\", \"game\"],\n",
    "    \"Data Science\": [\"math\", \"numpy\", \"data-science\", \"graphql\", \"data-visualization\", \"jupyter-notebook\", \"big-data\"],\n",
    "    \"Machine Learning\": [\"ml\", \"pytorch\", \"deep-learning\", \"machine-learning\", \"deep-neural-networks\", \"tensorflow\", \"neural-network\", \"tensor\", \"computer-vision\", \"reinforcement-learning\", \"hyperparameter-tuning\", \"ai\", \"artificial-intelligence\", \"llama\", \"llms\", \"llm\", \"openai\"],\n",
    "    \"Tool\": [\"containers\", \"zsh\", \"docker\", \"github\", \"cli\", \"searchengine\", \"postgrest\", \"devtool\", \"cloudstorage\", \"git\", \"npm\", \"database\", \"postgresql\", \"backend\", \"shell-scripting\", \"websocket\", \"collaboration\", \"developer-tools\", \"promise\", \"api\", \"testing\", \"translation\", \"i18n\", \"language\", \"golang-library\", \"algorithm\", \"firmware\", \"style-linter\", \"linting\", \"converter\", \"blockchain\", \"wordpress\", \"static-site-generator\", \"blog-engine\", \"material\", \"material-design\", \"framework\", \"argument-parser\", \"command-line-parser\", \"readme-generator\", \"ssh\", \"backup\", \"reverse-engineering\", \"animation\", \"sdk\", \"devops\", \"jenkins\", \"documentation\", \"terminal\", \"encryption\", \"scrapers\", \"3d-printing\", \"reactive-templates\", \"image-optimization\", \"file-server\", \"nat\", \"proxy\", \"shell\", \"linters\", \"git-client\", \"raspberry-pi\", \"blogging\", \"npm-cli\", \"aws\", \"api-gateway\", \"decompiler\", \"kubernetes\", \"tools\"],\n",
    "    \"App\": [\"note-taking\", \"productivity\", \"prest\", \"download\", \"latex\", \"text-editor\", \"curl\", \"ftp\", \"bot\", \"synchronization\", \"sqlite\", \"mattermost\", \"messaging\", \"conferencing\", \"remote-desktop\", \"emacs\", \"color-picker\", \"cli-app\", \"subtitle-downloader\", \"decompiler\", \"mobile-app\"],\n",
    "\n",
    "    \"Resource\": [\"learn-to-code\", \"freecodecamp\", \"curriculum\", \"certification\", \"learnopengl\", \"lists\", \"resources\", \"resource\", \"dataset\", \"public-api\", \"public-apis\", \"practice\", \"interview\", \"styleguide\", \"list\", \"interview-questions\", \"awesome-list\", \"principles\", \"design-patterns\"],\n",
    "}\n",
    "# Tags manually added to some repositories (which otherwise lack descriptive ones)\n",
    "MANUAL_TAGS = {\n",
    "    \"minio/minio\": [\"Machine Learning\"],\n",
    "    \"Aliucord/Aliucord\": [\"Modding\"],\n",
    "    \"cli-guidelines/cli-guidelines\": [\"Resource\"],\n",
    "    \"yjs/yjs\": [\"Tool\"],\n",
    "    \"TigerVNC/tigervnc\": [\"Tool\"],\n",
    "    \"ollama/ollama\": [\"App\"],\n",
    "    \"micropython/micropython\": [\"Tool\"], # Python implementation\n",
    "    \"raspberrypi/linux\": [\"App\"],\n",
    "    \"rust-lang/rust\": [\"Tool\"],\n",
    "    \"home-assistant/core\": [\"Tool\"],\n",
    "    \"vuejs/vue-cli\": [\"Tool\", \"Web\"],\n",
    "    \"remix-run/remix\": [\"Tool\", \"Web\"],\n",
    "}\n",
    "# Same thing as above, but mapping tag to list of repos with it instead, as I realized at the end this would've been more convenient.\n",
    "MANUAL_TAGS2 = {\n",
    "    \"Tool\": [\"pytorch/tutorials\", \"vuejs/core\", \"google/googletest\", \"google/guava\", \"ReactiveX/RxJava\"],\n",
    "    \"Web\": [\"vuejs/core\"],\n",
    "    \"App\": [\"square/retrofit\"],\n",
    "}\n",
    "for tag,aliases in TAG_ALIASES.items():\n",
    "    for alias in aliases:\n",
    "        TAG_MAP[alias] = tag\n",
    "for tag,repos in MANUAL_TAGS2.items():\n",
    "    for repo in repos:\n",
    "        if repo in MANUAL_TAGS:\n",
    "            MANUAL_TAGS[repo].append(tag)\n",
    "        else:\n",
    "            MANUAL_TAGS[repo] = [tag]\n",
    "\n",
    "IGNORED_TAGS = defaultdict(int)\n",
    "\n",
    "oss_repos = {}\n",
    "for index, row in df_repo.iterrows():\n",
    "    key = row[\"owner\"] + \"/\" + row[\"name\"]\n",
    "    main_language = row[\"mainLanguage\"]\n",
    "    if main_language in LANGUAGE_REMAP:\n",
    "        main_language = LANGUAGE_REMAP[main_language]\n",
    "    repo = {\n",
    "        \"topics\": set(),\n",
    "        \"languages\": set([main_language] if main_language != \"\" else []),\n",
    "        \"stars\": row[\"total_stars\"],\n",
    "        \"contributors\": row[\"total_contributors\"],\n",
    "        \"icon\": row[\"avatar_url\"],\n",
    "        \"description\": row[\"description\"],\n",
    "    }\n",
    "    for topic in str.split(row[\"topics\"], \",\"):\n",
    "        if topic in TAG_MAP:\n",
    "            repo[\"topics\"].add(TAG_MAP[topic])\n",
    "        else:\n",
    "            IGNORED_TAGS[tag] += 1\n",
    "        if key in MANUAL_TAGS:\n",
    "            for tag in MANUAL_TAGS[key]:\n",
    "                repo[\"topics\"].add(tag)\n",
    "\n",
    "    oss_repos[key] = repo\n",
    "    used_tags = used_tags.union(repo[\"topics\"])\n",
    "    used_languages.add(main_language)\n",
    "\n",
    "# Exclude mirrors and other projects that are not contributable projects or unsuitable\n",
    "BLACKLISTED_REPOS = [\n",
    "    \"gitlabhq/gitlabhq\", # Read-only mirror.\n",
    "    \"qemu/qemu\", # Read-only mirror.\n",
    "    \"xasset/xasset\",# Not english.\n",
    "    \"jynew/jynew\", # Unity RPG game framework, documentation in chinese-only though.\n",
    "    \"doocs/advanced-java\", # Chinese-only Java interview questions.\n",
    "    \"CyC2018/CS-Notes\", # Chinese computer science course resources.\n",
    "    \"apache/kafka\", # Read-only mirror.\n",
    "]\n",
    "for blacklisted_repo in BLACKLISTED_REPOS:\n",
    "    del oss_repos[blacklisted_repo]\n",
    "\n",
    "# Convert sets to lists for json serialization, and add other\n",
    "# keys the site expects\n",
    "for key,repo in oss_repos.items():\n",
    "    repo[\"owner\"] = key.split(\"/\")[0]\n",
    "    repo[\"repo\"] = key.split(\"/\")[1]\n",
    "    repo[\"topics\"] = list(repo[\"topics\"])\n",
    "    repo[\"languages\"] = list(repo[\"languages\"])\n",
    "\n",
    "# Export the .json\n",
    "with open(\"repositories_output.json\", \"w\") as f:\n",
    "    json.dump(oss_repos, f, indent=2)\n",
    "\n",
    "print(\"Valid repositories:\", len(oss_repos))\n",
    "print(\"Languages used:\", used_languages)\n",
    "print(\"Tags used:\", used_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com els repositoris en la base de dades foren principalment trobades per scraping de les pàgines \"trending\" de GitHub, podem concloure que un 50% dels repositoris que GitHub destaca són només repositoris populars de projectes individuals o d'equips petits i no projectes contribuïbles; només uns 1000 repositoris dels 2000 en la base de dades compleixen els requisits que hem imposat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tables\n",
    "Shows the table for the different entities of the Data Base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM Repositories;\"\n",
    "\n",
    "output_repo, df_repo = execute_select_query(cursor, query)\n",
    "df_repo.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM RepositoryVisits;\"\n",
    "\n",
    "output_repo_visists, df_repo_visists = execute_select_query(cursor, query)\n",
    "df_repo_visists.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM RepositoryTopics;\"\n",
    "\n",
    "output_repo_topics, df_repo_topics = execute_select_query(cursor, query)\n",
    "df_repo_topics.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Owners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM Owners;\"\n",
    "# Use _ to ignore\n",
    "_, df_owners = execute_select_query(cursor, query)\n",
    "df_owners.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM OwnerVisits;\"\n",
    "\n",
    "output_owner_visits, df_owner_visits = execute_select_query(cursor, query)\n",
    "df_owner_visits.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM Commits;\"\n",
    "\n",
    "output_commits, df_commits = execute_select_query(cursor, query)\n",
    "df_commits.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM Topics;\"\n",
    "\n",
    "output_topics, df_topics = execute_select_query(cursor, query)\n",
    "df_topics.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM TopicVisits;\"\n",
    "\n",
    "_, df_topic_visits = execute_select_query(cursor, query)\n",
    "df_topic_visits.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM TrendVisits;\"\n",
    "\n",
    "_, df_trend_visits = execute_select_query(cursor, query)\n",
    "df_trend_visits.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closes the connection\n",
    "dataBaseConnection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "### Basic statistics\n",
    "* Distribution of main topics (bar or pie chart). Aka, in what proportion of our studied repos are they treated.\n",
    "* Distribution of languages (bar or pie chart). Like above.\n",
    "* Time evolution of interest in topics: repositories per topic, followers per topic.\n",
    "\n",
    "### Dimensional reduction for an overview on repositories\n",
    "What we mean here is to perform a PCA (principal components analysis) in order to able to have an insight on the structre of the whole dataset. We would then color the data points in it depending on the language used, for instance, to see if these groups have similar characteristics and lie close in the dataframe or not.\n",
    "* Create a dataframe containing all RepositoryVisits data for a certain date for all the studied repositories.\n",
    "* Perform a standarization on the data (i.e., to prevent some variables such as commits to be far more important than others such as forks).\n",
    "* Perform a PCA into 2 components on it.\n",
    "* Plot the results while clustering the points depending on different criteria:\n",
    "    + mainLanguage\n",
    "    + topic\n",
    "* Supervised machine learning using stars.\n",
    "### Open questions\n",
    "* How to use stars and trends?\n",
    "* How to use contributions by owners?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commits analisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import path\n",
    "from PIL import Image\n",
    "\"\"\"\n",
    "If \n",
    "!pip install wordcloud \n",
    "doesn't work:\n",
    "import sys\n",
    "print(sys.executable) # use the path\n",
    "\n",
    "path -m pip install wordcloud\n",
    "\"\"\"\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "# https://www.datacamp.com/tutorial/wordcloud-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_cloud(word_cloud, text, save_image = False, image_name = \"none\"):\n",
    "    # Create and generate a word cloud image:\n",
    "    word_cloud_output = word_cloud.generate(text)\n",
    "    \n",
    "    # Display the generated image:\n",
    "    plt.imshow(word_cloud_output, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    if save_image:\n",
    "        word_cloud_output.to_file(f\"./Images/wordclouds/{image_name}_word_cloud.png\")\n",
    "        \n",
    "def transform_format(val):\n",
    "    if val == 0:\n",
    "        return 255\n",
    "    else:\n",
    "        return val\n",
    "        \n",
    "def obtain_mask(mask):    \n",
    "    trans_mask = np.ndarray((mask.shape[0],mask.shape[1]), np.int32)\n",
    "    for i in range(len(mask)):\n",
    "        trans_mask[i] = list(map(transform_format, mask[i]))\n",
    "    return trans_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "# stopwords.update([\"a\"]) # manually add stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standarize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# Els matches d'aquest patró seran substituïts per un espai; serveix principalment per separar signes de puntuació de paraules al voltant\n",
    "STANDARDIZE_SUBSTITUTION = re.compile(r\"[!\\\"$#%&()*+,\\-./:;<=>?[\\]^_`{|}~]\")\n",
    "\n",
    "# Eliminarem espais consecutius\n",
    "CONSECUTIVE_SPACE_SUBSTITUTION = re.compile(r\"  +\")\n",
    "\n",
    "def remove_accents(input_str):\n",
    "    \"\"\"\n",
    "    Converteix caràcters accentuats als caràcters base.\n",
    "    Font: https://stackoverflow.com/a/1207479\n",
    "    \"\"\"\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str).encode(\"ascii\", \"ignore\")\n",
    "    return bytes.decode(nfkd_form)\n",
    "\n",
    "# Aplicarem lemmatization usant NLTK;\n",
    "# Substituirà per exemple paraules plurals per la seva forma singular.\n",
    "lemmatizer = nltk.wordnet.WordNetLemmatizer()\n",
    "LEMMATIZATION_BLACKLIST = set([\n",
    "    \"was\",  # No apliquem lemmatization a aquesta paraula, produeix confusió\n",
    "    \"as\",\n",
    "])\n",
    "\n",
    "WORD_REPLACEMENTS = {\n",
    "    \"read-me\": \"readme\",\n",
    "    \"read.me\": \"readme\",\n",
    "    \"readme.md\": \"readme\"\n",
    "}\n",
    "\n",
    "def standardize(word):\n",
    "    \"\"\"\n",
    "    :param word: paraula a estandaritzar\n",
    "    :return : paraula estandaritzada\n",
    "    \"\"\"            \n",
    "    word = word.lower()  # Convertim a lowercase\n",
    "    word = re.sub(STANDARDIZE_SUBSTITUTION, \" \", word)  # Eliminem simbols\n",
    "    word = re.sub(\"read-me\", \"readme\", word)  # Manually standarize readme.\n",
    "    word = re.sub(\"read.me\", \"readme\", word)\n",
    "    word = re.sub(\"readme.md\", \"readme\", word)\n",
    "    word = remove_accents(word)  # Eliminem accents, altres combinacions de glyphs addicionals i caràcters que no tenen representació ASCII\n",
    "\n",
    "    # Eliminem possesius\n",
    "    word = word.replace(\"'s\", \"\")\n",
    "\n",
    "    # Eliminem hashtags\n",
    "    word = word.replace(\"#\", \"\")\n",
    "\n",
    "    # Eliminem espais consecutius\n",
    "    word = re.sub(CONSECUTIVE_SPACE_SUBSTITUTION, \" \", word)\n",
    "    word = word.strip()\n",
    "\n",
    "    # Per cada paraula, apliquem lemmatization\n",
    "    words = word.split(\" \")\n",
    "    words = [lemmatizer.lemmatize(word) if word not in LEMMATIZATION_BLACKLIST else word for word in words]\n",
    "\n",
    "    # Reconstruim la frase\n",
    "    return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardize(\"thIs a TesT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardize(\"readme.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_standardize_test_words = [\n",
    "    \"Testing @here as#Dasd 333232 tests testings gItHUbs testing\",  # tests -> test per lemmatization\n",
    "    \"Test_ing !!, (asd,as d).\",  # Considerem que underscores separen paraules, però realment no affecta molts tweets\n",
    "    \"Test łłłł ñaññañ áaaa\",  # Eliminació d'accents i caràcters no estandards\n",
    "    \"テスト #asdasd arabian, wolves\",\n",
    "]\n",
    "\n",
    "for word in _standardize_test_words:\n",
    "    print(standardize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove commits made by bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the commits authored by a bot.\n",
    "from IPython.display import display\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "cond_a = df_commits[df_commits[\"author\"].str.contains(\"\\[bot]|-bot\")]\n",
    "cond_c = df_commits[df_commits[\"message\"].str.contains(\"dependabot\")]\n",
    "cond_d = df_commits[df_commits[\"message\"].str.contains(\"renovatebot\")]\n",
    "\n",
    "temp = pd.concat([cond_a])\n",
    "\n",
    "unique_auth = pd.unique(temp.author)\n",
    "unique_msg = pd.unique(cond_c.message)\n",
    "\n",
    "dict = {'bots' : unique_auth}\n",
    "df_bots = pd.DataFrame(dict)\n",
    "\n",
    "# displaying the bots names\n",
    "display(df_bots)\n",
    "\n",
    "dict = {'bots_message' : unique_msg}\n",
    "df_bots_msg = pd.DataFrame(dict)\n",
    "\n",
    "display(df_bots_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the bots\n",
    "print(\"With bots\", len(df_commits))\n",
    "df_commits = df_commits[~df_commits[\"author\"].isin(df_bots[\"bots\"])]\n",
    "print(\"Without bots\", len(df_commits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_commits_small = df_commits.tail() # only last 5\n",
    "print(df_commits_small.tail().message, \"\\n\")\n",
    "small_text_sample = \" \".join(commit for commit in df_commits_small.message) # concatenate them\n",
    "print(\"Concatenated text:\", small_text_sample)\n",
    "image_name = \"first_word_cloud\"\n",
    "word_cloud = WordCloud(stopwords=stopwords, max_font_size=50, max_words=100, background_color=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_hyper_small = df_commits[\"message\"][0]\n",
    "text_hyper_small = standardize(text_hyper_small)\n",
    "\n",
    "image_name = \"first_word_cloud\"\n",
    "word_cloud = WordCloud(stopwords=stopwords, max_font_size=50, max_words=100, background_color=\"black\")\n",
    "\n",
    "plot_word_cloud(word_cloud, text_hyper_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = \" \".join(commit for commit in df_commits.message) # concatenate them\n",
    "word_cloud = WordCloud(stopwords=stopwords, max_font_size=30, max_words=200, background_color=\"black\")\n",
    "plot_word_cloud(word_cloud, all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a mask and a color map\n",
    "        \n",
    "# <a href='https://dryicons.com/icon/square-github-icon-8312'> Icon by Dryicons </a>\n",
    "github_image = np.array(Image.open(\"./Images/wordclouds/github_square.png\"))\n",
    "\n",
    "word_cloud_mask = obtain_mask(github_image)\n",
    "# Word cloud repeats words since he needs fill the gaps using the correct size.\n",
    "# print(len(all_text.split(' ')))\n",
    "# print(len(pd.unique(all_text.split(' '))))\n",
    "\n",
    "word_cloud = WordCloud(stopwords=stopwords, colormap='rainbow', mask=word_cloud_mask, max_font_size=30, max_words=10000, background_color=\"black\")\n",
    "plot_word_cloud(word_cloud, all_text, True, \"git_mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a mask        \n",
    "git_image = np.array(Image.open(\"./Images/wordclouds/git.png\"))\n",
    "\n",
    "word_cloud_mask = obtain_mask(git_image)\n",
    "\n",
    "word_cloud = WordCloud(stopwords=stopwords, mask=word_cloud_mask,colormap='hot', max_font_size=20, max_words=10000, background_color=\"#474747\")\n",
    "plot_word_cloud(word_cloud, all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_hyper_small = df_commits[\"message\"][0]\n",
    "text_hyper_small = standardize(text_hyper_small)\n",
    "\n",
    "image_name = \"first_word_cloud\"\n",
    "word_cloud = WordCloud(\n",
    "    stopwords=stopwords,\n",
    "    max_font_size=50,\n",
    "    max_words=100,\n",
    "    background_color=\"black\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    :param df: DataFrame with the messages and associated information\n",
    "    :return: Dictionary with the format {word: {n_ocur: value, n_messages: value}, ...}\n",
    "    \"\"\"\n",
    "    # Using defaultdict for convenience; we won't have to add keys explicitly\n",
    "    dicc = defaultdict(lambda: {\"n_ocur\": 0, \"n_messages\": 0})\n",
    "\n",
    "    total_rows = len(df)\n",
    "    processed_rows_count = 0\n",
    "    for row in df.iterrows():\n",
    "        text = row[1][0]\n",
    "        text = standardize(text)  # Apply standardization\n",
    "        if any(char.isdigit() for char in text):\n",
    "            continue\n",
    "        # Count the words\n",
    "        words = text.split(\" \")\n",
    "        unique_words = set(words)\n",
    "        # Times that a word appears and times that appears in different messages.\n",
    "        for word in words:\n",
    "            dicc[word][\"n_ocur\"] += 1\n",
    "        for word in unique_words:\n",
    "            dicc[word][\"n_messages\"] += 1\n",
    "\n",
    "        processed_rows_count += 1\n",
    "        # print(f\"Word {processed_rows_count}/{total_rows} done; {processed_rows_count / total_rows * 100:.2f}%\")\n",
    "    \n",
    "    return dicc\n",
    "\n",
    "frame = {'Messages': df_commits.message}\n",
    "\n",
    "result = pd.DataFrame(frame)\n",
    "\n",
    "ocurrencesDict = count_words(result)\n",
    "\n",
    "# Sort the words by occurrences and occurrences in messages.\n",
    "def obtain_top_n_words(dictionary, N, filter=\"n_ocur\", desc=True):\n",
    "    # Get all the words and their frequency\n",
    "    filtered_words = [(word, freq[filter]) for word, freq in dictionary.items() if word not in stopwords]\n",
    "    \n",
    "    # Sort from most to least frequent\n",
    "    sorted_words = sorted(filtered_words, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Take the N most frequent\n",
    "    return [(word, freq) for word, freq in sorted_words[:N]]\n",
    "\n",
    "top_words_list_ocur = obtain_top_n_words(ocurrencesDict, 30, \"n_ocur\", True)\n",
    "top_words_list_msg = obtain_top_n_words(ocurrencesDict, 30, \"n_messages\", True)\n",
    "\n",
    "print(\"Ocurrences Top\", top_words_list_ocur)\n",
    "print(\"Ocurrences per message Top\", top_words_list_msg)\n",
    "\n",
    "# Function to plot the top words\n",
    "def plot_top_words(topList, yLabel, ax):\n",
    "    x = [word for word, freq in topList]\n",
    "    y = [freq for word, freq in topList]\n",
    "    \n",
    "    # Making the bar chart on the data\n",
    "    ax.bar(x, y)\n",
    "    \n",
    "    # Giving title to the plot\n",
    "    ax.set_title(\"Top used words\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(x, rotation=90, ha='right')\n",
    "    # Giving X and Y labels\n",
    "    ax.set_xlabel(\"Word\")\n",
    "    ax.set_ylabel(yLabel)\n",
    "     \n",
    "    # We do not call plt.show() here, as we want to show both plots together\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "# Plot the top words by occurrences\n",
    "plot_top_words(top_words_list_ocur, \"Occurrences of word\", axs[0])\n",
    "\n",
    "# Plot the top words by occurrences in different messages\n",
    "plot_top_words(top_words_list_msg, \"Occurrences of word in different messages\", axs[1])\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n",
    "\n",
    "# We can see that there is no significant difference, since normally, in a commit message, we do not repeat the same word twice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use freqüent items to seek for the different word groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO, comment more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = {'Messages': df_commits.message}\n",
    "\n",
    "result = pd.DataFrame(frame)\n",
    "print(len(result))\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/apriori/#apriori-frequent-itemsets-via-the-apriori-algorithm\n",
    "def generate_data_set(df: pd.DataFrame, rmStopWords = True, useStandard = True):\n",
    "    \"\"\"\n",
    "    :param df: DataFrame with the messages and associated information\n",
    "    :return dataSet: The message of the commit is the transaction and the words the items.\n",
    "    \"\"\"\n",
    "    data_set = []\n",
    "    total_rows = len(df)\n",
    "    processed_rows_count = 0\n",
    "    for row in df.iterrows():\n",
    "        text = row[1][0]\n",
    "        if (useStandard):\n",
    "            text = standardize(text)  # Apply standardization\n",
    "       \n",
    "        # Split the words, but don't hold digits.\n",
    "        words = [word for word in text.split() if not any(char.isdigit() for char in word)]\n",
    "        if (rmStopWords):\n",
    "            unique_words = list(set(words) - stopwords)\n",
    "        else:\n",
    "            unique_words = list(set(words))\n",
    "        data_set.append(unique_words)\n",
    "\n",
    "        processed_rows_count += 1\n",
    "        # print(f\"Word {processed_rows_count}/{total_rows} done; {processed_rows_count / total_rows * 100:.2f}%\")\n",
    "    return data_set\n",
    "data_set = generate_data_set(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "import math\n",
    "#!pip install mlxtend\n",
    "\n",
    "def get_frequent_itemsets(data, min_supp = 0.01):\n",
    "    \"\"\"\n",
    "    To save memory, you may want to represent your transaction data in the sparse format. \n",
    "    This is especially useful if you have lots of products and small transactions.\n",
    "    \"\"\"\n",
    "    te = TransactionEncoder()\n",
    "    oht_ary = te.fit(data).transform(data, sparse=True)\n",
    "    sparse_df = pd.DataFrame.sparse.from_spmatrix(oht_ary, columns=te.columns_)\n",
    "    \n",
    "    # total message * support = number of messages which the bundle appears.\n",
    "    print(\"We consider that at least the bundle appears in:\", math.ceil(len(data) * min_supp), \" messages\")\n",
    "    \n",
    "    # Calculate it using apriori algorithm\n",
    "    frequent_itemsets = apriori(sparse_df, min_support=min_supp, use_colnames=True, verbose=1)\n",
    "    frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "    return frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets = get_frequent_itemsets(data_set)\n",
    "# Sort them from more length and then support\n",
    "frequent_itemsets_sorted = frequent_itemsets.sort_values(by=['length', 'support'], ascending=[False, False])\n",
    "print(frequent_itemsets_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_itemsets(data, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(data['itemsets'].astype(str), data['support'] * 100, color='skyblue')\n",
    "    plt.xlabel('Itemsets')\n",
    "    plt.ylabel('Support %')\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_len_one = frequent_itemsets[(frequent_itemsets['length'] == 1)]\n",
    "data_len_one['itemsets'] = data_len_one['itemsets'].apply(lambda x: str(sorted(list(x))))\n",
    "data_len_one = data_len_one.sort_values(by=['support'], ascending=False)\n",
    "\n",
    "plot_itemsets(data_len_one, 'Support of Frequent Itemsets of Length 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_group = frequent_itemsets[(frequent_itemsets['length'] >= 2)]\n",
    "data_group['itemsets'] = data_group['itemsets'].apply(lambda x: str(sorted(list(x))))\n",
    "data_group = data_group.sort_values(by=['length'], ascending=False)\n",
    "\n",
    "plot_itemsets(data_group, 'Support of Frequent Itemsets from larger to smaller')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO, maybe lower the minsupport even more to see strange connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequent items, but we want to also see the stopwords usage or the verbal tense used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_no_standard = generate_data_set(result, rmStopWords = False, useStandard = False)\n",
    "frequent_itemsets = get_frequent_itemsets(data_set_no_standard)\n",
    "# Sort them from more length and then support\n",
    "frequent_itemsets_sorted = frequent_itemsets.sort_values(by=['length', 'support'], ascending=[False, False])\n",
    "print(frequent_itemsets_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO, maybe lower the minsupport even more to see strange connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_no_standard = generate_data_set(result, rmStopWords = False, useStandard = False)\n",
    "frequent_itemsets = get_frequent_itemsets(data_set_no_standard, min_supp = 0.005) # Warning, if you go too low, it will crash, your pc can't handle it...\n",
    "# Sort them from more length and then support\n",
    "frequent_itemsets_sorted = frequent_itemsets.sort_values(by=['length', 'support'], ascending=[False, False])\n",
    "print(frequent_itemsets_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-python3-kernel",
   "language": "python",
   "name": "my-python3-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
